[
  {
    "filename": "array_api.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\array_api.rst.txt",
    "id": "array_api.rst.txt_chunk_0",
    "header": ".. _array_api:",
    "text": ".. _array_api:\n\n================================"
  },
  {
    "filename": "array_api.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\array_api.rst.txt",
    "id": "array_api.rst.txt_chunk_1",
    "header": "Array API support (experimental)",
    "text": "Array API support (experimental)\n================================\n\n.. currentmodule:: sklearn\n\nThe `Array API <https://data-apis.org/array-api/latest/>`_ specification defines\na standard API for all array manipulation libraries with a NumPy-like API.\nScikit-learn vendors pinned copies of\n`array-api-compat <https://github.com/data-apis/array-api-compat>`__\nand `array-api-extra <https://github.com/data-apis/array-api-extra>`__.\n\nScikit-learn's support for the array API standard requires the environment variable\n`SCIPY_ARRAY_API` to be set to `1` before importing `scipy` and `scikit-learn`:\n\n.. prompt:: bash $\n\n   export SCIPY_ARRAY_API=1\n\nPlease note that this environment variable is intended for temporary use.\nFor more details, refer to SciPy's `Array API documentation\n<https://docs.scipy.org/doc/scipy/dev/api-dev/array_api.html#using-array-api-standard-support>`_.\n\nSome scikit-learn estimators that primarily rely on NumPy (as opposed to using\nCython) to implement the algorithmic logic of their `fit`, `predict` or\n`transform` methods can be configured to accept any Array API compatible input\ndata structures and automatically dispatch operations to the underlying namespace\ninstead of relying on NumPy.\n\nAt this stage, this support is **considered experimental** and must be enabled\nexplicitly by the `array_api_dispatch` configuration. See below for details.\n\n.. note::\n    Currently, only `array-api-strict`, `cupy`, and `PyTorch` are known to work\n    with scikit-learn's estimators.\n\nThe following video provides an overview of the standard's design principles\nand how it facilitates interoperability between array libraries:\n\n- `Scikit-learn on GPUs with Array API <https://www.youtube.com/watch?v=c_s8tr1AizA>`_\n  by :user:`Thomas Fan <thomasjpfan>` at PyData NYC 2023."
  },
  {
    "filename": "array_api.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\array_api.rst.txt",
    "id": "array_api.rst.txt_chunk_2",
    "header": "Example usage",
    "text": "Example usage\n=============\n\nThe configuration `array_api_dispatch=True` needs to be set to `True` to enable array\nAPI support. We recommend setting this configuration globally to ensure consistent\nbehaviour and prevent accidental mixing of array namespaces.\nNote that we set it with :func:`config_context` below to avoid having to call\n:func:`set_config(array_api_dispatch=False)` at the end of every code snippet\nthat uses the array API.\nThe example code snippet below demonstrates how to use `CuPy\n<https://cupy.dev/>`_ to run\n:class:`~discriminant_analysis.LinearDiscriminantAnalysis` on a GPU::\n\n    >>> from sklearn.datasets import make_classification\n    >>> from sklearn import config_context\n    >>> from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n    >>> import cupy\n\n    >>> X_np, y_np = make_classification(random_state=0)\n    >>> X_cu = cupy.asarray(X_np)\n    >>> y_cu = cupy.asarray(y_np)\n    >>> X_cu.device\n    <CUDA Device 0>\n\n    >>> with config_context(array_api_dispatch=True):\n    ...     lda = LinearDiscriminantAnalysis()\n    ...     X_trans = lda.fit_transform(X_cu, y_cu)\n    >>> X_trans.device\n    <CUDA Device 0>\n\nAfter the model is trained, fitted attributes that are arrays will also be\nfrom the same Array API namespace as the training data. For example, if CuPy's\nArray API namespace was used for training, then fitted attributes will be on the\nGPU. We provide an experimental `_estimator_with_converted_arrays` utility that\ntransfers an estimator attributes from Array API to a ndarray::\n\n    >>> from sklearn.utils._array_api import _estimator_with_converted_arrays\n    >>> cupy_to_ndarray = lambda array : array.get()\n    >>> lda_np = _estimator_with_converted_arrays(lda, cupy_to_ndarray)\n    >>> X_trans = lda_np.transform(X_np)\n    >>> type(X_trans)\n    <class 'numpy.ndarray'>"
  },
  {
    "filename": "array_api.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\array_api.rst.txt",
    "id": "array_api.rst.txt_chunk_3",
    "header": "PyTorch Support",
    "text": "PyTorch Support\n---------------\n\nPyTorch Tensors can also be passed directly::\n\n    >>> import torch\n    >>> X_torch = torch.asarray(X_np, device=\"cuda\", dtype=torch.float32)\n    >>> y_torch = torch.asarray(y_np, device=\"cuda\", dtype=torch.float32)\n\n    >>> with config_context(array_api_dispatch=True):\n    ...     lda = LinearDiscriminantAnalysis()\n    ...     X_trans = lda.fit_transform(X_torch, y_torch)\n    >>> type(X_trans)\n    <class 'torch.Tensor'>\n    >>> X_trans.device.type\n    'cuda'\n\n.. _array_api_supported:"
  },
  {
    "filename": "array_api.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\array_api.rst.txt",
    "id": "array_api.rst.txt_chunk_4",
    "header": "Support for `Array API`-compatible inputs",
    "text": "Support for `Array API`-compatible inputs\n=========================================\n\nEstimators and other tools in scikit-learn that support Array API compatible inputs."
  },
  {
    "filename": "array_api.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\array_api.rst.txt",
    "id": "array_api.rst.txt_chunk_5",
    "header": "Estimators",
    "text": "Estimators\n----------\n\n- :class:`decomposition.PCA` (with `svd_solver=\"full\"`,\n  `svd_solver=\"randomized\"` and `power_iteration_normalizer=\"QR\"`)\n- :class:`linear_model.Ridge` (with `solver=\"svd\"`)\n- :class:`discriminant_analysis.LinearDiscriminantAnalysis` (with `solver=\"svd\"`)\n- :class:`preprocessing.Binarizer`\n- :class:`preprocessing.KernelCenterer`\n- :class:`preprocessing.LabelEncoder`\n- :class:`preprocessing.MaxAbsScaler`\n- :class:`preprocessing.MinMaxScaler`\n- :class:`preprocessing.Normalizer`"
  },
  {
    "filename": "array_api.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\array_api.rst.txt",
    "id": "array_api.rst.txt_chunk_6",
    "header": "Meta-estimators",
    "text": "Meta-estimators\n---------------\n\nMeta-estimators that accept Array API inputs conditioned on the fact that the\nbase estimator also does:\n\n- :class:`model_selection.GridSearchCV`\n- :class:`model_selection.RandomizedSearchCV`\n- :class:`model_selection.HalvingGridSearchCV`\n- :class:`model_selection.HalvingRandomSearchCV`"
  },
  {
    "filename": "array_api.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\array_api.rst.txt",
    "id": "array_api.rst.txt_chunk_7",
    "header": "Metrics",
    "text": "Metrics\n-------\n\n- :func:`sklearn.metrics.cluster.entropy`\n- :func:`sklearn.metrics.accuracy_score`\n- :func:`sklearn.metrics.d2_tweedie_score`\n- :func:`sklearn.metrics.explained_variance_score`\n- :func:`sklearn.metrics.f1_score`\n- :func:`sklearn.metrics.fbeta_score`\n- :func:`sklearn.metrics.hamming_loss`\n- :func:`sklearn.metrics.jaccard_score`\n- :func:`sklearn.metrics.max_error`\n- :func:`sklearn.metrics.mean_absolute_error`\n- :func:`sklearn.metrics.mean_absolute_percentage_error`\n- :func:`sklearn.metrics.mean_gamma_deviance`\n- :func:`sklearn.metrics.mean_pinball_loss`\n- :func:`sklearn.metrics.mean_poisson_deviance` (requires `enabling array API support for SciPy <https://docs.scipy.org/doc/scipy/dev/api-dev/array_api.html#using-array-api-standard-support>`_)\n- :func:`sklearn.metrics.mean_squared_error`\n- :func:`sklearn.metrics.mean_squared_log_error`\n- :func:`sklearn.metrics.mean_tweedie_deviance`\n- :func:`sklearn.metrics.multilabel_confusion_matrix`\n- :func:`sklearn.metrics.pairwise.additive_chi2_kernel`\n- :func:`sklearn.metrics.pairwise.chi2_kernel`\n- :func:`sklearn.metrics.pairwise.cosine_similarity`\n- :func:`sklearn.metrics.pairwise.cosine_distances`\n- :func:`sklearn.metrics.pairwise.euclidean_distances` (see :ref:`device_support_for_float64`)\n- :func:`sklearn.metrics.pairwise.linear_kernel`\n- :func:`sklearn.metrics.pairwise.paired_cosine_distances`\n- :func:`sklearn.metrics.pairwise.paired_euclidean_distances`\n- :func:`sklearn.metrics.pairwise.polynomial_kernel`\n- :func:`sklearn.metrics.pairwise.rbf_kernel` (see :ref:`device_support_for_float64`)\n- :func:`sklearn.metrics.pairwise.sigmoid_kernel`\n- :func:`sklearn.metrics.precision_score`\n- :func:`sklearn.metrics.precision_recall_fscore_support`\n- :func:`sklearn.metrics.r2_score`\n- :func:`sklearn.metrics.recall_score`\n- :func:`sklearn.metrics.root_mean_squared_error`\n- :func:`sklearn.metrics.root_mean_squared_log_error`\n- :func:`sklearn.metrics.zero_one_loss`"
  },
  {
    "filename": "array_api.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\array_api.rst.txt",
    "id": "array_api.rst.txt_chunk_8",
    "header": "Tools",
    "text": "Tools\n-----\n\n- :func:`model_selection.train_test_split`\n- :func:`utils.check_consistent_length`\n\nCoverage is expected to grow over time. Please follow the dedicated `meta-issue on GitHub\n<https://github.com/scikit-learn/scikit-learn/issues/22352>`_ to track progress."
  },
  {
    "filename": "array_api.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\array_api.rst.txt",
    "id": "array_api.rst.txt_chunk_9",
    "header": "Input and output array type handling",
    "text": "Input and output array type handling\n====================================\n\nEstimators and scoring functions are able to accept input arrays\nfrom different array libraries and/or devices. When a mixed set of input arrays is\npassed, scikit-learn converts arrays as needed to make them all consistent.\n\nFor estimators, the rule is **\"everything follows** `X` **\"** - mixed array inputs are\nconverted so that they all match the array library and device of `X`.\nFor scoring functions the rule is **\"everything follows** `y_pred` **\"** - mixed array\ninputs are converted so that they all match the array library and device of `y_pred`.\n\nWhen a function or method has been called with array API compatible inputs, the\nconvention is to return arrays from the same array library and on the same\ndevice as the input data."
  },
  {
    "filename": "array_api.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\array_api.rst.txt",
    "id": "array_api.rst.txt_chunk_10",
    "header": "Estimators",
    "text": "Estimators\n----------\n\nWhen an estimator is fitted with an array API compatible `X`, all other\narray inputs, including constructor arguments, (e.g., `y`, `sample_weight`)\nwill be converted to match the array library and device of `X`, if they do not already.\nThis behaviour enables switching from processing on the CPU to processing\non the GPU at any point within a pipeline.\n\nThis allows estimators to accept mixed input types, enabling `X` to be moved\nto a different device within a pipeline, without explicitly moving `y`.\nNote that scikit-learn pipelines do not allow transformation of `y` (to avoid\n:ref:`leakage <data_leakage>`).\n\nTake for example a pipeline where `X` and `y` both start on CPU, and go through\nthe following three steps:\n\n* :class:`~sklearn.preprocessing.TargetEncoder`, which will transform categorial\n  `X` but also requires `y`, meaning both `X` and `y` need to be on CPU.\n* :class:`FunctionTransformer(func=partial(torch.asarray, device=\"cuda\")) <sklearn.preprocessing.FunctionTransformer>`,\n  which moves `X` to GPU, to improve performance in the next step.\n* :class:`~sklearn.linear_model.Ridge`, whose performance can be improved when\n  passed arrays on a GPU, as they can handle large matrix operations very efficiently.\n\n`X` initially contains categorical string data (thus needs to be on CPU), which is\ntarget encoded to numerical values in :class:`~sklearn.preprocessing.TargetEncoder`.\n`X` is then explicitly moved to GPU to improve the performance of\n:class:`~sklearn.linear_model.Ridge`. `y` cannot be transformed by the pipeline\n(recall scikit-learn pipelines do not allow transformation of `y`) but as\n:class:`~sklearn.linear_model.Ridge` is able to accept mixed input types,\nthis is not a problem and the pipeline is able to be run.\n\nThe fitted attributes of an estimator fitted with an array API compatible `X`, will\nbe arrays from the same library as the input and stored on the same device.\nThe `predict` and `transform` method subsequently expect\ninputs from the same array library and device as the data passed to the `fit`\nmethod."
  },
  {
    "filename": "array_api.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\array_api.rst.txt",
    "id": "array_api.rst.txt_chunk_11",
    "header": "Scoring functions",
    "text": "Scoring functions\n-----------------\n\nWhen an array API compatible `y_pred` is passed to a scoring function,\nall other array inputs (e.g., `y_true`, `sample_weight`) will be converted\nto match the array library and device of `y_pred`, if they do not already.\nThis allows scoring functions to accept mixed input types, enabling them to be\nused within a :term:`meta-estimator` (or function that accepts estimators), with a\npipeline that moves input arrays between devices (e.g., CPU to GPU).\n\nFor example, to be able to use the pipeline described above within e.g.,\n:func:`~sklearn.model_selection.cross_validate` or\n:class:`~sklearn.model_selection.GridSearchCV`, the scoring function internally\ncalled needs to be able to accept mixed input types.\n\nThe output type of scoring functions depends on the number of output values.\nWhen a scoring function returns a scalar value, it will return a Python\nscalar (typically a `float` instance) instead of an array scalar value.\nFor scoring functions that support :term:`multiclass` or :term:`multioutput`,\nan array from the same array library and device as `y_pred` will be returned when\nmultiple values need to be output."
  },
  {
    "filename": "array_api.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\array_api.rst.txt",
    "id": "array_api.rst.txt_chunk_12",
    "header": "Common estimator checks",
    "text": "Common estimator checks\n=======================\n\nAdd the `array_api_support` tag to an estimator's set of tags to indicate that\nit supports the array API. This will enable dedicated checks as part of the\ncommon tests to verify that the estimators' results are the same when using\nvanilla NumPy and array API inputs.\n\nTo run these checks you need to install\n`array-api-strict <https://data-apis.org/array-api-strict/>`_ in your\ntest environment. This allows you to run checks without having a\nGPU. To run the full set of checks you also need to install\n`PyTorch <https://pytorch.org/>`_, `CuPy <https://cupy.dev/>`_ and have\na GPU. Checks that can not be executed or have missing dependencies will be\nautomatically skipped. Therefore it's important to run the tests with the\n`-v` flag to see which checks are skipped:\n\n.. prompt:: bash $\n\n    pip install array-api-strict  # and other libraries as needed\n    pytest -k \"array_api\" -v\n\nRunning the scikit-learn tests against `array-api-strict` should help reveal\nmost code problems related to handling multiple device inputs via the use of\nsimulated non-CPU devices. This allows for fast iterative development and debugging of\narray API related code.\n\nHowever, to ensure full handling of PyTorch or CuPy inputs allocated on actual GPU\ndevices, it is necessary to run the tests against those libraries and hardware.\nThis can either be achieved by using\n`Google Colab <https://gist.github.com/EdAbati/ff3bdc06bafeb92452b3740686cc8d7c>`_\nor leveraging our CI infrastructure on pull requests (manually triggered by maintainers\nfor cost reasons).\n\n.. _mps_support:"
  },
  {
    "filename": "array_api.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\array_api.rst.txt",
    "id": "array_api.rst.txt_chunk_13",
    "header": "Note on MPS device support",
    "text": "Note on MPS device support\n--------------------------\n\nOn macOS, PyTorch can use the Metal Performance Shaders (MPS) to access\nhardware accelerators (e.g. the internal GPU component of the M1 or M2 chips).\nHowever, the MPS device support for PyTorch is incomplete at the time of\nwriting. See the following github issue for more details:\n\n- https://github.com/pytorch/pytorch/issues/77764\n\nTo enable the MPS support in PyTorch, set the environment variable\n`PYTORCH_ENABLE_MPS_FALLBACK=1` before running the tests:\n\n.. prompt:: bash $\n\n    PYTORCH_ENABLE_MPS_FALLBACK=1 pytest -k \"array_api\" -v\n\nAt the time of writing all scikit-learn tests should pass, however, the\ncomputational speed is not necessarily better than with the CPU device.\n\n.. _device_support_for_float64:"
  },
  {
    "filename": "array_api.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\array_api.rst.txt",
    "id": "array_api.rst.txt_chunk_14",
    "header": "Note on device support for ``float64``",
    "text": "Note on device support for ``float64``\n--------------------------------------\n\nCertain operations within scikit-learn will automatically perform operations\non floating-point values with `float64` precision to prevent overflows and ensure\ncorrectness (e.g., :func:`metrics.pairwise.euclidean_distances`). However,\ncertain combinations of array namespaces and devices, such as `PyTorch on MPS`\n(see :ref:`mps_support`) do not support the `float64` data type. In these cases,\nscikit-learn will revert to using the `float32` data type instead. This can result in\ndifferent behavior (typically numerically unstable results) compared to not using array\nAPI dispatching or using a device with `float64` support."
  },
  {
    "filename": "biclustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\biclustering.rst.txt",
    "id": "biclustering.rst.txt_chunk_0",
    "header": ".. _biclustering:",
    "text": ".. _biclustering:\n\n============"
  },
  {
    "filename": "biclustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\biclustering.rst.txt",
    "id": "biclustering.rst.txt_chunk_1",
    "header": "Biclustering",
    "text": "Biclustering\n============\n\nBiclustering algorithms simultaneously\ncluster rows and columns of a data matrix. These clusters of rows and\ncolumns are known as biclusters. Each determines a submatrix of the\noriginal data matrix with some desired properties.\n\nFor instance, given a matrix of shape ``(10, 10)``, one possible bicluster\nwith three rows and two columns induces a submatrix of shape ``(3, 2)``::\n\n    >>> import numpy as np\n    >>> data = np.arange(100).reshape(10, 10)\n    >>> rows = np.array([0, 2, 3])[:, np.newaxis]\n    >>> columns = np.array([1, 2])\n    >>> data[rows, columns]\n    array([[ 1,  2],\n           [21, 22],\n           [31, 32]])\n\nFor visualization purposes, given a bicluster, the rows and columns of\nthe data matrix may be rearranged to make the bicluster contiguous.\n\nAlgorithms differ in how they define biclusters. Some of the\ncommon types include:\n\n* constant values, constant rows, or constant columns\n* unusually high or low values\n* submatrices with low variance\n* correlated rows or columns\n\nAlgorithms also differ in how rows and columns may be assigned to\nbiclusters, which leads to different bicluster structures. Block\ndiagonal or checkerboard structures occur when rows and columns are\ndivided into partitions.\n\nIf each row and each column belongs to exactly one bicluster, then\nrearranging the rows and columns of the data matrix reveals the\nbiclusters on the diagonal. Here is an example of this structure\nwhere biclusters have higher average values than the other rows and\ncolumns:\n\n.. figure:: ../auto_examples/bicluster/images/sphx_glr_plot_spectral_coclustering_003.png\n   :target: ../auto_examples/bicluster/images/sphx_glr_plot_spectral_coclustering_003.png\n   :align: center\n   :scale: 50\n\n   An example of biclusters formed by partitioning rows and columns.\n\nIn the checkerboard case, each row belongs to all column clusters, and\neach column belongs to all row clusters. Here is an example of this\nstructure where the variance of the values within each bicluster is\nsmall:\n\n.. figure:: ../auto_examples/bicluster/images/sphx_glr_plot_spectral_biclustering_003.png\n   :target: ../auto_examples/bicluster/images/sphx_glr_plot_spectral_biclustering_003.png\n   :align: center\n   :scale: 50\n\n   An example of checkerboard biclusters.\n\nAfter fitting a model, row and column cluster membership can be found\nin the ``rows_`` and ``columns_`` attributes. ``rows_[i]`` is a binary vector\nwith nonzero entries corresponding to rows that belong to bicluster\n``i``. Similarly, ``columns_[i]`` indicates which columns belong to\nbicluster ``i``.\n\nSome models also have ``row_labels_`` and ``column_labels_`` attributes.\nThese models partition the rows and columns, such as in the block\ndiagonal and checkerboard bicluster structures.\n\n.. note::\n\n    Biclustering has many other names in different fields including\n    co-clustering, two-mode clustering, two-way clustering, block\n    clustering, coupled two-way clustering, etc. The names of some\n    algorithms, such as the Spectral Co-Clustering algorithm, reflect\n    these alternate names.\n\n\n.. currentmodule:: sklearn.cluster\n\n\n.. _spectral_coclustering:"
  },
  {
    "filename": "biclustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\biclustering.rst.txt",
    "id": "biclustering.rst.txt_chunk_2",
    "header": "Spectral Co-Clustering",
    "text": "Spectral Co-Clustering\n======================\n\nThe :class:`SpectralCoclustering` algorithm finds biclusters with\nvalues higher than those in the corresponding other rows and columns.\nEach row and each column belongs to exactly one bicluster, so\nrearranging the rows and columns to make partitions contiguous reveals\nthese high values along the diagonal:\n\n.. note::\n\n    The algorithm treats the input data matrix as a bipartite graph: the\n    rows and columns of the matrix correspond to the two sets of vertices,\n    and each entry corresponds to an edge between a row and a column. The\n    algorithm approximates the normalized cut of this graph to find heavy\n    subgraphs."
  },
  {
    "filename": "biclustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\biclustering.rst.txt",
    "id": "biclustering.rst.txt_chunk_3",
    "header": "Mathematical formulation",
    "text": "Mathematical formulation\n------------------------\n\nAn approximate solution to the optimal normalized cut may be found via\nthe generalized eigenvalue decomposition of the Laplacian of the\ngraph. Usually this would mean working directly with the Laplacian\nmatrix. If the original data matrix :math:`A` has shape :math:`m\n\\times n`, the Laplacian matrix for the corresponding bipartite graph\nhas shape :math:`(m + n) \\times (m + n)`. However, in this case it is\npossible to work directly with :math:`A`, which is smaller and more\nefficient.\n\nThe input matrix :math:`A` is preprocessed as follows:\n\n.. math::\n    A_n = R^{-1/2} A C^{-1/2}\n\nWhere :math:`R` is the diagonal matrix with entry :math:`i` equal to\n:math:`\\sum_{j} A_{ij}` and :math:`C` is the diagonal matrix with\nentry :math:`j` equal to :math:`\\sum_{i} A_{ij}`.\n\nThe singular value decomposition, :math:`A_n = U \\Sigma V^\\top`,\nprovides the partitions of the rows and columns of :math:`A`. A subset\nof the left singular vectors gives the row partitions, and a subset\nof the right singular vectors gives the column partitions.\n\nThe :math:`\\ell = \\lceil \\log_2 k \\rceil` singular vectors, starting\nfrom the second, provide the desired partitioning information. They\nare used to form the matrix :math:`Z`:\n\n.. math::\n    Z = \\begin{bmatrix} R^{-1/2} U \\\\\\\\\n                        C^{-1/2} V\n          \\end{bmatrix}\n\nwhere the columns of :math:`U` are :math:`u_2, \\dots, u_{\\ell +\n1}`, and similarly for :math:`V`.\n\nThen the rows of :math:`Z` are clustered using :ref:`k-means\n<k_means>`. The first ``n_rows`` labels provide the row partitioning,\nand the remaining ``n_columns`` labels provide the column partitioning.\n\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_bicluster_plot_spectral_coclustering.py`: A simple example\n  showing how to generate a data matrix with biclusters and apply\n  this method to it.\n\n* :ref:`sphx_glr_auto_examples_bicluster_plot_bicluster_newsgroups.py`: An example of finding\n  biclusters in the twenty newsgroup dataset.\n\n\n.. rubric:: References\n\n* Dhillon, Inderjit S, 2001. :doi:`Co-clustering documents and words using\n  bipartite spectral graph partitioning\n  <10.1145/502512.502550>`\n\n\n.. _spectral_biclustering:"
  },
  {
    "filename": "biclustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\biclustering.rst.txt",
    "id": "biclustering.rst.txt_chunk_4",
    "header": "Spectral Biclustering",
    "text": "Spectral Biclustering\n=====================\n\nThe :class:`SpectralBiclustering` algorithm assumes that the input\ndata matrix has a hidden checkerboard structure. The rows and columns\nof a matrix with this structure may be partitioned so that the entries\nof any bicluster in the Cartesian product of row clusters and column\nclusters are approximately constant. For instance, if there are two\nrow partitions and three column partitions, each row will belong to\nthree biclusters, and each column will belong to two biclusters.\n\nThe algorithm partitions the rows and columns of a matrix so that a\ncorresponding blockwise-constant checkerboard matrix provides a good\napproximation to the original matrix."
  },
  {
    "filename": "biclustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\biclustering.rst.txt",
    "id": "biclustering.rst.txt_chunk_5",
    "header": "Mathematical formulation",
    "text": "Mathematical formulation\n------------------------\n\nThe input matrix :math:`A` is first normalized to make the\ncheckerboard pattern more obvious. There are three possible methods:\n\n1. *Independent row and column normalization*, as in Spectral\n   Co-Clustering. This method makes the rows sum to a constant and the\n   columns sum to a different constant.\n\n2. **Bistochastization**: repeated row and column normalization until\n   convergence. This method makes both rows and columns sum to the\n   same constant.\n\n3. **Log normalization**: the log of the data matrix is computed: :math:`L =\n   \\log A`. Then the column mean :math:`\\overline{L_{i \\cdot}}`, row mean\n   :math:`\\overline{L_{\\cdot j}}`, and overall mean :math:`\\overline{L_{\\cdot\n   \\cdot}}` of :math:`L` are computed. The final matrix is computed\n   according to the formula\n\n.. math::\n    K_{ij} = L_{ij} - \\overline{L_{i \\cdot}} - \\overline{L_{\\cdot\n    j}} + \\overline{L_{\\cdot \\cdot}}\n\nAfter normalizing, the first few singular vectors are computed, just\nas in the Spectral Co-Clustering algorithm.\n\nIf log normalization was used, all the singular vectors are\nmeaningful. However, if independent normalization or bistochastization\nwere used, the first singular vectors, :math:`u_1` and :math:`v_1`.\nare discarded. From now on, the \"first\" singular vectors refers to\n:math:`u_2 \\dots u_{p+1}` and :math:`v_2 \\dots v_{p+1}` except in the\ncase of log normalization.\n\nGiven these singular vectors, they are ranked according to which can\nbe best approximated by a piecewise-constant vector. The\napproximations for each vector are found using one-dimensional k-means\nand scored using the Euclidean distance. Some subset of the best left\nand right singular vectors are selected. Next, the data is projected to\nthis best subset of singular vectors and clustered.\n\nFor instance, if :math:`p` singular vectors were calculated, the\n:math:`q` best are found as described, where :math:`q<p`. Let\n:math:`U` be the matrix with columns the :math:`q` best left singular\nvectors, and similarly :math:`V` for the right. To partition the rows,\nthe rows of :math:`A` are projected to a :math:`q` dimensional space:\n:math:`A * V`. Treating the :math:`m` rows of this :math:`m \\times q`\nmatrix as samples and clustering using k-means yields the row labels.\nSimilarly, projecting the columns to :math:`A^{\\top} * U` and\nclustering this :math:`n \\times q` matrix yields the column labels.\n\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_bicluster_plot_spectral_biclustering.py`: a simple example\n  showing how to generate a checkerboard matrix and bicluster it.\n\n\n.. rubric:: References\n\n* Kluger, Yuval, et. al., 2003. :doi:`Spectral biclustering of microarray\n  data: coclustering genes and conditions\n  <10.1101/gr.648603>`\n\n\n.. _biclustering_evaluation:\n\n.. currentmodule:: sklearn.metrics"
  },
  {
    "filename": "biclustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\biclustering.rst.txt",
    "id": "biclustering.rst.txt_chunk_6",
    "header": "Biclustering evaluation",
    "text": "Biclustering evaluation\n=======================\n\nThere are two ways of evaluating a biclustering result: internal and\nexternal. Internal measures, such as cluster stability, rely only on\nthe data and the result themselves. Currently there are no internal\nbicluster measures in scikit-learn. External measures refer to an\nexternal source of information, such as the true solution. When\nworking with real data the true solution is usually unknown, but\nbiclustering artificial data may be useful for evaluating algorithms\nprecisely because the true solution is known.\n\nTo compare a set of found biclusters to the set of true biclusters,\ntwo similarity measures are needed: a similarity measure for\nindividual biclusters, and a way to combine these individual\nsimilarities into an overall score.\n\nTo compare individual biclusters, several measures have been used. For\nnow, only the Jaccard index is implemented:\n\n.. math::\n    J(A, B) = \\frac{|A \\cap B|}{|A| + |B| - |A \\cap B|}\n\nwhere :math:`A` and :math:`B` are biclusters, :math:`|A \\cap B|` is\nthe number of elements in their intersection. The Jaccard index\nachieves its minimum of 0 when the biclusters do not overlap at all\nand its maximum of 1 when they are identical.\n\nSeveral methods have been developed to compare two sets of biclusters.\nFor now, only :func:`consensus_score` (Hochreiter et. al., 2010) is\navailable:\n\n1. Compute bicluster similarities for pairs of biclusters, one in each\n   set, using the Jaccard index or a similar measure.\n\n2. Assign biclusters from one set to another in a one-to-one fashion\n   to maximize the sum of their similarities. This step is performed\n   using :func:`scipy.optimize.linear_sum_assignment`, which uses a\n   modified Jonker-Volgenant algorithm.\n\n3. The final sum of similarities is divided by the size of the larger\n   set.\n\nThe minimum consensus score, 0, occurs when all pairs of biclusters\nare totally dissimilar. The maximum score, 1, occurs when both sets\nare identical.\n\n\n.. rubric:: References\n\n* Hochreiter, Bodenhofer, et. al., 2010. `FABIA: factor analysis\n  for bicluster acquisition\n  <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2881408/>`__."
  },
  {
    "filename": "calibration.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\calibration.rst.txt",
    "id": "calibration.rst.txt_chunk_0",
    "header": ".. _calibration:",
    "text": ".. _calibration:\n\n======================="
  },
  {
    "filename": "calibration.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\calibration.rst.txt",
    "id": "calibration.rst.txt_chunk_1",
    "header": "Probability calibration",
    "text": "Probability calibration\n=======================\n\n.. currentmodule:: sklearn.calibration\n\n\nWhen performing classification you often want not only to predict the class\nlabel, but also obtain a probability of the respective label. This probability\ngives you some kind of confidence on the prediction. Some models can give you\npoor estimates of the class probabilities and some even do not support\nprobability prediction (e.g., some instances of\n:class:`~sklearn.linear_model.SGDClassifier`).\nThe calibration module allows you to better calibrate\nthe probabilities of a given model, or to add support for probability\nprediction.\n\nWell calibrated classifiers are probabilistic classifiers for which the output\nof the :term:`predict_proba` method can be directly interpreted as a confidence\nlevel.\nFor instance, a well calibrated (binary) classifier should classify the samples such\nthat among the samples to which it gave a :term:`predict_proba` value close to, say,\n0.8, approximately 80% actually belong to the positive class.\n\nBefore we show how to re-calibrate a classifier, we first need a way to detect how\ngood a classifier is calibrated.\n\n.. note::\n    Strictly proper scoring rules for probabilistic predictions like\n    :func:`sklearn.metrics.brier_score_loss` and\n    :func:`sklearn.metrics.log_loss` assess calibration (reliability) and\n    discriminative power (resolution) of a model, as well as the randomness of the data\n    (uncertainty) at the same time. This follows from the well-known Brier score\n    decomposition of Murphy [1]_. As it is not clear which term dominates, the score is\n    of limited use for assessing calibration alone (unless one computes each term of\n    the decomposition). A lower Brier loss, for instance, does not necessarily\n    mean a better calibrated model, it could also mean a worse calibrated model with much\n    more discriminatory power, e.g. using many more features.\n\n.. _calibration_curve:"
  },
  {
    "filename": "calibration.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\calibration.rst.txt",
    "id": "calibration.rst.txt_chunk_2",
    "header": "Calibration curves",
    "text": "Calibration curves\n------------------\n\nCalibration curves, also referred to as *reliability diagrams* (Wilks 1995 [2]_),\ncompare how well the probabilistic predictions of a binary classifier are calibrated.\nIt plots the frequency of the positive label (to be more precise, an estimation of the\n*conditional event probability* :math:`P(Y=1|\\text{predict_proba})`) on the y-axis\nagainst the predicted probability :term:`predict_proba` of a model on the x-axis.\nThe tricky part is to get values for the y-axis.\nIn scikit-learn, this is accomplished by binning the predictions such that the x-axis\nrepresents the average predicted probability in each bin.\nThe y-axis is then the *fraction of positives* given the predictions of that bin, i.e.\nthe proportion of samples whose class is the positive class (in each bin).\n\nThe top calibration curve plot is created with\n:func:`CalibrationDisplay.from_estimator`, which uses :func:`calibration_curve` to\ncalculate the per bin average predicted probabilities and fraction of positives.\n:func:`CalibrationDisplay.from_estimator`\ntakes as input a fitted classifier, which is used to calculate the predicted\nprobabilities. The classifier thus must have :term:`predict_proba` method. For\nthe few classifiers that do not have a :term:`predict_proba` method, it is\npossible to use :class:`CalibratedClassifierCV` to calibrate the classifier\noutputs to probabilities.\n\nThe bottom histogram gives some insight into the behavior of each classifier\nby showing the number of samples in each predicted probability bin.\n\n.. figure:: ../auto_examples/calibration/images/sphx_glr_plot_compare_calibration_001.png\n   :target: ../auto_examples/calibration/plot_compare_calibration.html\n   :align: center\n\n.. currentmodule:: sklearn.linear_model\n\n:class:`LogisticRegression` is more likely to return well calibrated predictions by itself as it has a\ncanonical link function for its loss, i.e. the logit-link for the :ref:`log_loss`.\nIn the unpenalized case, this leads to the so-called **balance property**, see [8]_ and :ref:`Logistic_regression`.\nIn the plot above, data is generated according to a linear mechanism, which is\nconsistent with the :class:`LogisticRegression` model (the model is 'well specified'),\nand the value of the regularization parameter `C` is tuned to be\nappropriate (neither too strong nor too low). As a consequence, this model returns\naccurate predictions from its `predict_proba` method.\nIn contrast to that, the other shown models return biased probabilities; with\ndifferent biases per model.\n\n.. currentmodule:: sklearn.naive_bayes\n\n:class:`GaussianNB` (Naive Bayes) tends to push probabilities to 0 or 1 (note the counts\nin the histograms). This is mainly because it makes the assumption that\nfeatures are conditionally independent given the class, which is not the\ncase in this dataset which contains 2 redundant features.\n\n.. currentmodule:: sklearn.ensemble\n\n:class:`RandomForestClassifier` shows the opposite behavior: the histograms\nshow peaks at probabilities approximately 0.2 and 0.9, while probabilities\nclose to 0 or 1 are very rare. An explanation for this is given by\nNiculescu-Mizil and Caruana [3]_: \"Methods such as bagging and random\nforests that average predictions from a base set of models can have\ndifficulty making predictions near 0 and 1 because variance in the\nunderlying base models will bias predictions that should be near zero or one\naway from these values. Because predictions are restricted to the interval\n[0,1], errors caused by variance tend to be one-sided near zero and one. For\nexample, if a model should predict :math:`p = 0` for a case, the only way bagging\ncan achieve this is if all bagged trees predict zero. If we add noise to the\ntrees that bagging is averaging over, this noise will cause some trees to\npredict values larger than 0 for this case, thus moving the average\nprediction of the bagged ensemble away from 0. We observe this effect most\nstrongly with random forests because the base-level trees trained with\nrandom forests have relatively high variance due to feature subsetting.\" As\na result, the calibration curve shows a characteristic sigmoid shape, indicating that\nthe classifier could trust its \"intuition\" more and return probabilities closer\nto 0 or 1 typically.\n\n.. currentmodule:: sklearn.svm\n\n:class:`LinearSVC` (SVC) shows an even more sigmoid curve than the random forest, which\nis typical for maximum-margin methods (compare Niculescu-Mizil and Caruana [3]_), which\nfocus on difficult to classify samples that are close to the decision boundary (the\nsupport vectors)."
  },
  {
    "filename": "calibration.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\calibration.rst.txt",
    "id": "calibration.rst.txt_chunk_3",
    "header": "Calibrating a classifier",
    "text": "Calibrating a classifier\n------------------------\n\n.. currentmodule:: sklearn.calibration\n\nCalibrating a classifier consists of fitting a regressor (called a\n*calibrator*) that maps the output of the classifier (as given by\n:term:`decision_function` or :term:`predict_proba`) to a calibrated probability\nin [0, 1]. Denoting the output of the classifier for a given sample by :math:`f_i`,\nthe calibrator tries to predict the conditional event probability\n:math:`P(y_i = 1 | f_i)`.\n\nIdeally, the calibrator is fit on a dataset independent of the training data used to\nfit the classifier in the first place.\nThis is because performance of the classifier on its training data would be\nbetter than for novel data. Using the classifier output of training data\nto fit the calibrator would thus result in a biased calibrator that maps to\nprobabilities closer to 0 and 1 than it should."
  },
  {
    "filename": "calibration.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\calibration.rst.txt",
    "id": "calibration.rst.txt_chunk_4",
    "header": "Usage",
    "text": "Usage\n-----\n\nThe :class:`CalibratedClassifierCV` class is used to calibrate a classifier.\n\n:class:`CalibratedClassifierCV` uses a cross-validation approach to ensure\nunbiased data is always used to fit the calibrator. The data is split into :math:`k`\n`(train_set, test_set)` couples (as determined by `cv`). When `ensemble=True`\n(default), the following procedure is repeated independently for each\ncross-validation split:\n\n1. a clone of `base_estimator` is trained on the train subset\n2. the trained `base_estimator` makes predictions on the test subset\n3. the predictions are used to fit a calibrator (either a sigmoid or isotonic\n   regressor) (when the data is multiclass, a calibrator is fit for every class)\n\nThis results in an\nensemble of :math:`k` `(classifier, calibrator)` couples where each calibrator maps\nthe output of its corresponding classifier into [0, 1]. Each couple is exposed\nin the `calibrated_classifiers_` attribute, where each entry is a calibrated\nclassifier with a :term:`predict_proba` method that outputs calibrated\nprobabilities. The output of :term:`predict_proba` for the main\n:class:`CalibratedClassifierCV` instance corresponds to the average of the\npredicted probabilities of the :math:`k` estimators in the `calibrated_classifiers_`\nlist. The output of :term:`predict` is the class that has the highest\nprobability.\n\nIt is important to choose `cv` carefully when using `ensemble=True`.\nAll classes should be present in both train and test subsets for every split.\nWhen a class is absent in the train subset, the predicted probability for that\nclass will default to 0 for the `(classifier, calibrator)` couple of that split.\nThis skews the :term:`predict_proba` as it averages across all couples.\nWhen a class is absent in the test subset, the calibrator for that class\n(within the `(classifier, calibrator)` couple of that split) is\nfit on data with no positive class. This results in ineffective calibration.\n\nWhen `ensemble=False`, cross-validation is used to obtain 'unbiased'\npredictions for all the data, via\n:func:`~sklearn.model_selection.cross_val_predict`.\nThese unbiased predictions are then used to train the calibrator. The attribute\n`calibrated_classifiers_` consists of only one `(classifier, calibrator)`\ncouple where the classifier is the `base_estimator` trained on all the data.\nIn this case the output of :term:`predict_proba` for\n:class:`CalibratedClassifierCV` is the predicted probabilities obtained\nfrom the single `(classifier, calibrator)` couple.\n\nThe main advantage of `ensemble=True` is to benefit from the traditional\nensembling effect (similar to :ref:`bagging`). The resulting ensemble should\nboth be well calibrated and slightly more accurate than with `ensemble=False`.\nThe main advantage of using `ensemble=False` is computational: it reduces the\noverall fit time by training only a single base classifier and calibrator\npair, decreases the final model size and increases prediction speed.\n\nAlternatively an already fitted classifier can be calibrated by using a\n:class:`~sklearn.frozen.FrozenEstimator` as\n``CalibratedClassifierCV(estimator=FrozenEstimator(estimator))``.\nIt is up to the user to make sure that the data used for fitting the classifier\nis disjoint from the data used for fitting the regressor.\n\n:class:`CalibratedClassifierCV` supports the use of two regression techniques\nfor calibration via the `method` parameter: `\"sigmoid\"` and `\"isotonic\"`.\n\n.. _sigmoid_regressor:"
  },
  {
    "filename": "calibration.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\calibration.rst.txt",
    "id": "calibration.rst.txt_chunk_5",
    "header": "Sigmoid",
    "text": "Sigmoid\n^^^^^^^\n\nThe sigmoid regressor, `method=\"sigmoid\"` is based on Platt's logistic model [4]_:\n\n.. math::\n       p(y_i = 1 | f_i) = \\frac{1}{1 + \\exp(A f_i + B)} \\,,\n\nwhere :math:`y_i` is the true label of sample :math:`i` and :math:`f_i`\nis the output of the un-calibrated classifier for sample :math:`i`. :math:`A`\nand :math:`B` are real numbers to be determined when fitting the regressor via\nmaximum likelihood.\n\nThe sigmoid method assumes the :ref:`calibration curve <calibration_curve>`\ncan be corrected by applying a sigmoid function to the raw predictions. This\nassumption has been empirically justified in the case of :ref:`svm` with\ncommon kernel functions on various benchmark datasets in section 2.1 of Platt\n1999 [4]_ but does not necessarily hold in general. Additionally, the\nlogistic model works best if the calibration error is symmetrical, meaning\nthe classifier output for each binary class is normally distributed with\nthe same variance [7]_. This can be a problem for highly imbalanced\nclassification problems, where outputs do not have equal variance.\n\nIn general this method is most effective for small sample sizes or when the\nun-calibrated model is under-confident and has similar calibration errors for both\nhigh and low outputs."
  },
  {
    "filename": "calibration.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\calibration.rst.txt",
    "id": "calibration.rst.txt_chunk_6",
    "header": "Isotonic",
    "text": "Isotonic\n^^^^^^^^\n\nThe `method=\"isotonic\"` fits a non-parametric isotonic regressor, which outputs\na step-wise non-decreasing function, see :mod:`sklearn.isotonic`. It minimizes:\n\n.. math::\n       \\sum_{i=1}^{n} (y_i - \\hat{f}_i)^2\n\nsubject to :math:`\\hat{f}_i \\geq \\hat{f}_j` whenever\n:math:`f_i \\geq f_j`. :math:`y_i` is the true\nlabel of sample :math:`i` and :math:`\\hat{f}_i` is the output of the\ncalibrated classifier for sample :math:`i` (i.e., the calibrated probability).\nThis method is more general when compared to `'sigmoid'` as the only restriction\nis that the mapping function is monotonically increasing. It is thus more\npowerful as it can correct any monotonic distortion of the un-calibrated model.\nHowever, it is more prone to overfitting, especially on small datasets [6]_.\n\nOverall, `'isotonic'` will perform as well as or better than `'sigmoid'` when\nthere is enough data (greater than ~ 1000 samples) to avoid overfitting [3]_.\n\n.. note:: Impact on ranking metrics like AUC\n\n    It is generally expected that calibration does not affect ranking metrics such as\n    ROC-AUC. However, these metrics might differ after calibration when using\n    `method=\"isotonic\"` since isotonic regression introduces ties in the predicted\n    probabilities. This can be seen as within the uncertainty of the model predictions.\n    In case, you strictly want to keep the ranking and thus AUC scores, use\n    `method=\"sigmoid\"` which is a strictly monotonic transformation and thus keeps\n    the ranking."
  },
  {
    "filename": "calibration.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\calibration.rst.txt",
    "id": "calibration.rst.txt_chunk_7",
    "header": "Multiclass support",
    "text": "Multiclass support\n^^^^^^^^^^^^^^^^^^\n\nBoth isotonic and sigmoid regressors only\nsupport 1-dimensional data (e.g., binary classification output) but are\nextended for multiclass classification if the `base_estimator` supports\nmulticlass predictions. For multiclass predictions,\n:class:`CalibratedClassifierCV` calibrates for\neach class separately in a :ref:`ovr_classification` fashion [5]_. When\npredicting\nprobabilities, the calibrated probabilities for each class\nare predicted separately. As those probabilities do not necessarily sum to\none, a postprocessing is performed to normalize them.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_calibration_plot_calibration_curve.py`\n* :ref:`sphx_glr_auto_examples_calibration_plot_calibration_multiclass.py`\n* :ref:`sphx_glr_auto_examples_calibration_plot_calibration.py`\n* :ref:`sphx_glr_auto_examples_calibration_plot_compare_calibration.py`\n\n.. rubric:: References\n\n.. [1] Allan H. Murphy (1973).\n       :doi:`\"A New Vector Partition of the Probability Score\"\n       <10.1175/1520-0450(1973)012%3C0595:ANVPOT%3E2.0.CO;2>`\n       Journal of Applied Meteorology and Climatology\n\n.. [2] `On the combination of forecast probabilities for\n       consecutive precipitation periods.\n       <https://doi.org/10.1175/1520-0434(1990)005%3C0640:OTCOFP%3E2.0.CO;2>`_\n       Wea. Forecasting, 5, 640\u2013650., Wilks, D. S., 1990a\n\n.. [3] `Predicting Good Probabilities with Supervised Learning\n       <https://www.cs.cornell.edu/~alexn/papers/calibration.icml05.crc.rev3.pdf>`_,\n       A. Niculescu-Mizil & R. Caruana, ICML 2005\n\n\n.. [4] `Probabilistic Outputs for Support Vector Machines and Comparisons\n       to Regularized Likelihood Methods.\n       <https://www.cs.colorado.edu/~mozer/Teaching/syllabi/6622/papers/Platt1999.pdf>`_\n       J. Platt, (1999)\n\n.. [5] `Transforming Classifier Scores into Accurate Multiclass\n       Probability Estimates.\n       <https://dl.acm.org/doi/pdf/10.1145/775047.775151>`_\n       B. Zadrozny & C. Elkan, (KDD 2002)\n\n.. [6] `Predicting accurate probabilities with a ranking loss.\n       <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4180410/>`_\n       Menon AK, Jiang XJ, Vembu S, Elkan C, Ohno-Machado L.\n       Proc Int Conf Mach Learn. 2012;2012:703-710\n\n.. [7] `Beyond sigmoids: How to obtain well-calibrated probabilities from\n       binary classifiers with beta calibration\n       <https://projecteuclid.org/euclid.ejs/1513306867>`_\n       Kull, M., Silva Filho, T. M., & Flach, P. (2017).\n\n.. [8] Mario V. W\u00fcthrich, Michael Merz (2023).\n       :doi:`\"Statistical Foundations of Actuarial Learning and its Applications\"\n       <10.1007/978-3-031-12409-9>`\n       Springer Actuarial"
  },
  {
    "filename": "classification_threshold.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\classification_threshold.rst.txt",
    "id": "classification_threshold.rst.txt_chunk_0",
    "header": ".. currentmodule:: sklearn.model_selection",
    "text": ".. currentmodule:: sklearn.model_selection\n\n.. _TunedThresholdClassifierCV:\n\n=================================================="
  },
  {
    "filename": "classification_threshold.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\classification_threshold.rst.txt",
    "id": "classification_threshold.rst.txt_chunk_1",
    "header": "Tuning the decision threshold for class prediction",
    "text": "Tuning the decision threshold for class prediction\n==================================================\n\nClassification is best divided into two parts:\n\n* the statistical problem of learning a model to predict, ideally, class probabilities;\n* the decision problem to take concrete action based on those probability predictions.\n\nLet's take a straightforward example related to weather forecasting: the first point is\nrelated to answering \"what is the chance that it will rain tomorrow?\" while the second\npoint is related to answering \"should I take an umbrella tomorrow?\".\n\nWhen it comes to the scikit-learn API, the first point is addressed by providing scores\nusing :term:`predict_proba` or :term:`decision_function`. The former returns conditional\nprobability estimates :math:`P(y|X)` for each class, while the latter returns a decision\nscore for each class.\n\nThe decision corresponding to the labels is obtained with :term:`predict`. In binary\nclassification, a decision rule or action is then defined by thresholding the scores,\nleading to the prediction of a single class label for each sample. For binary\nclassification in scikit-learn, class labels predictions are obtained by hard-coded\ncut-off rules: a positive class is predicted when the conditional probability\n:math:`P(y|X)` is greater than 0.5 (obtained with :term:`predict_proba`) or if the\ndecision score is greater than 0 (obtained with :term:`decision_function`).\n\nHere, we show an example that illustrates the relatonship between conditional\nprobability estimates :math:`P(y|X)` and class labels::\n\n    >>> from sklearn.datasets import make_classification\n    >>> from sklearn.tree import DecisionTreeClassifier\n    >>> X, y = make_classification(random_state=0)\n    >>> classifier = DecisionTreeClassifier(max_depth=2, random_state=0).fit(X, y)\n    >>> classifier.predict_proba(X[:4])\n    array([[0.94     , 0.06     ],\n           [0.94     , 0.06     ],\n           [0.0416, 0.9583],\n           [0.0416, 0.9583]])\n    >>> classifier.predict(X[:4])\n    array([0, 0, 1, 1])\n\nWhile these hard-coded rules might at first seem reasonable as default behavior, they\nare most certainly not ideal for most use cases. Let's illustrate with an example.\n\nConsider a scenario where a predictive model is being deployed to assist\nphysicians in detecting tumors. In this setting, physicians will most likely be\ninterested in identifying all patients with cancer and not missing anyone with cancer so\nthat they can provide them with the right treatment. In other words, physicians\nprioritize achieving a high recall rate. This emphasis on recall comes, of course, with\nthe trade-off of potentially more false-positive predictions, reducing the precision of\nthe model. That is a risk physicians are willing to take because the cost of a missed\ncancer is much higher than the cost of further diagnostic tests. Consequently, when it\ncomes to deciding whether to classify a patient as having cancer or not, it may be more\nbeneficial to classify them as positive for cancer when the conditional probability\nestimate is much lower than 0.5."
  },
  {
    "filename": "classification_threshold.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\classification_threshold.rst.txt",
    "id": "classification_threshold.rst.txt_chunk_2",
    "header": "Post-tuning the decision threshold",
    "text": "Post-tuning the decision threshold\n==================================\n\nOne solution to address the problem stated in the introduction is to tune the decision\nthreshold of the classifier once the model has been trained. The\n:class:`~sklearn.model_selection.TunedThresholdClassifierCV` tunes this threshold using\nan internal cross-validation. The optimum threshold is chosen to maximize a given\nmetric.\n\nThe following image illustrates the tuning of the decision threshold for a gradient\nboosting classifier. While the vanilla and tuned classifiers provide the same\n:term:`predict_proba` outputs and thus the same Receiver Operating Characteristic (ROC)\nand Precision-Recall curves, the class label predictions differ because of the tuned\ndecision threshold. The vanilla classifier predicts the class of interest for a\nconditional probability greater than 0.5 while the tuned classifier predicts the class\nof interest for a very low probability (around 0.02). This decision threshold optimizes\na utility metric defined by the business (in this case an insurance company).\n\n.. figure:: ../auto_examples/model_selection/images/sphx_glr_plot_cost_sensitive_learning_002.png\n   :target: ../auto_examples/model_selection/plot_cost_sensitive_learning.html\n   :align: center"
  },
  {
    "filename": "classification_threshold.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\classification_threshold.rst.txt",
    "id": "classification_threshold.rst.txt_chunk_3",
    "header": "Options to tune the decision threshold",
    "text": "Options to tune the decision threshold\n--------------------------------------\n\nThe decision threshold can be tuned through different strategies controlled by the\nparameter `scoring`.\n\nOne way to tune the threshold is by maximizing a pre-defined scikit-learn metric. These\nmetrics can be found by calling the function :func:`~sklearn.metrics.get_scorer_names`.\nBy default, the balanced accuracy is the metric used but be aware that one should choose\na meaningful metric for their use case.\n\n.. note::\n\n    It is important to notice that these metrics come with default parameters, notably\n    the label of the class of interest (i.e. `pos_label`). Thus, if this label is not\n    the right one for your application, you need to define a scorer and pass the right\n    `pos_label` (and additional parameters) using the\n    :func:`~sklearn.metrics.make_scorer`. Refer to :ref:`scoring_callable` to get\n    information to define your own scoring function. For instance, we show how to pass\n    the information to the scorer that the label of interest is `0` when maximizing the\n    :func:`~sklearn.metrics.f1_score`::\n\n        >>> from sklearn.linear_model import LogisticRegression\n        >>> from sklearn.model_selection import TunedThresholdClassifierCV\n        >>> from sklearn.metrics import make_scorer, f1_score\n        >>> X, y = make_classification(\n        ...   n_samples=1_000, weights=[0.1, 0.9], random_state=0)\n        >>> pos_label = 0\n        >>> scorer = make_scorer(f1_score, pos_label=pos_label)\n        >>> base_model = LogisticRegression()\n        >>> model = TunedThresholdClassifierCV(base_model, scoring=scorer)\n        >>> scorer(model.fit(X, y), X, y)\n        0.88\n        >>> # compare it with the internal score found by cross-validation\n        >>> model.best_score_\n        np.float64(0.86)"
  },
  {
    "filename": "classification_threshold.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\classification_threshold.rst.txt",
    "id": "classification_threshold.rst.txt_chunk_4",
    "header": "Important notes regarding the internal cross-validation",
    "text": "Important notes regarding the internal cross-validation\n-------------------------------------------------------\n\nBy default :class:`~sklearn.model_selection.TunedThresholdClassifierCV` uses a 5-fold\nstratified cross-validation to tune the decision threshold. The parameter `cv` allows to\ncontrol the cross-validation strategy. It is possible to bypass cross-validation by\nsetting `cv=\"prefit\"` and providing a fitted classifier. In this case, the decision\nthreshold is tuned on the data provided to the `fit` method.\n\nHowever, you should be extremely careful when using this option. You should never use\nthe same data for training the classifier and tuning the decision threshold due to the\nrisk of overfitting. Refer to the following example section for more details (cf.\n:ref:`TunedThresholdClassifierCV_no_cv`). If you have limited resources, consider using\na float number for `cv` to limit to an internal single train-test split.\n\nThe option `cv=\"prefit\"` should only be used when the provided classifier was already\ntrained, and you just want to find the best decision threshold using a new validation\nset.\n\n.. _FixedThresholdClassifier:"
  },
  {
    "filename": "classification_threshold.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\classification_threshold.rst.txt",
    "id": "classification_threshold.rst.txt_chunk_5",
    "header": "Manually setting the decision threshold",
    "text": "Manually setting the decision threshold\n---------------------------------------\n\nThe previous sections discussed strategies to find an optimal decision threshold. It is\nalso possible to manually set the decision threshold using the class\n:class:`~sklearn.model_selection.FixedThresholdClassifier`. In case that you don't want\nto refit the model when calling `fit`, wrap your sub-estimator with a\n:class:`~sklearn.frozen.FrozenEstimator` and do\n``FixedThresholdClassifier(FrozenEstimator(estimator), ...)``."
  },
  {
    "filename": "classification_threshold.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\classification_threshold.rst.txt",
    "id": "classification_threshold.rst.txt_chunk_6",
    "header": "Examples",
    "text": "Examples\n--------\n\n- See the example entitled\n  :ref:`sphx_glr_auto_examples_model_selection_plot_tuned_decision_threshold.py`,\n  to get insights on the post-tuning of the decision threshold.\n- See the example entitled\n  :ref:`sphx_glr_auto_examples_model_selection_plot_cost_sensitive_learning.py`,\n  to learn about cost-sensitive learning and decision threshold tuning."
  },
  {
    "filename": "clustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\clustering.rst.txt",
    "id": "clustering.rst.txt_chunk_0",
    "header": ".. _clustering:",
    "text": ".. _clustering:\n\n=========="
  },
  {
    "filename": "clustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\clustering.rst.txt",
    "id": "clustering.rst.txt_chunk_1",
    "header": "Clustering",
    "text": "Clustering\n==========\n\n`Clustering <https://en.wikipedia.org/wiki/Cluster_analysis>`__ of\nunlabeled data can be performed with the module :mod:`sklearn.cluster`.\n\nEach clustering algorithm comes in two variants: a class, that implements\nthe ``fit`` method to learn the clusters on train data, and a function,\nthat, given train data, returns an array of integer labels corresponding\nto the different clusters. For the class, the labels over the training\ndata can be found in the ``labels_`` attribute.\n\n.. currentmodule:: sklearn.cluster\n\n.. topic:: Input data\n\n    One important thing to note is that the algorithms implemented in\n    this module can take different kinds of matrix as input. All the\n    methods accept standard data matrices of shape ``(n_samples, n_features)``.\n    These can be obtained from the classes in the :mod:`sklearn.feature_extraction`\n    module. For :class:`AffinityPropagation`, :class:`SpectralClustering`\n    and :class:`DBSCAN` one can also input similarity matrices of shape\n    ``(n_samples, n_samples)``. These can be obtained from the functions\n    in the :mod:`sklearn.metrics.pairwise` module."
  },
  {
    "filename": "clustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\clustering.rst.txt",
    "id": "clustering.rst.txt_chunk_2",
    "header": "Overview of clustering methods",
    "text": "Overview of clustering methods\n===============================\n\n.. figure:: ../auto_examples/cluster/images/sphx_glr_plot_cluster_comparison_001.png\n   :target: ../auto_examples/cluster/plot_cluster_comparison.html\n   :align: center\n   :scale: 50\n\n   A comparison of the clustering algorithms in scikit-learn\n\n\n.. list-table::\n   :header-rows: 1\n   :widths: 14 15 19 25 20\n\n   * - Method name\n     - Parameters\n     - Scalability\n     - Usecase\n     - Geometry (metric used)\n\n   * - :ref:`K-Means <k_means>`\n     - number of clusters\n     - Very large ``n_samples``, medium ``n_clusters`` with\n       :ref:`MiniBatch code <mini_batch_kmeans>`\n     - General-purpose, even cluster size, flat geometry,\n       not too many clusters, inductive\n     - Distances between points\n\n   * - :ref:`Affinity propagation <affinity_propagation>`\n     - damping, sample preference\n     - Not scalable with n_samples\n     - Many clusters, uneven cluster size, non-flat geometry, inductive\n     - Graph distance (e.g. nearest-neighbor graph)\n\n   * - :ref:`Mean-shift <mean_shift>`\n     - bandwidth\n     - Not scalable with ``n_samples``\n     - Many clusters, uneven cluster size, non-flat geometry, inductive\n     - Distances between points\n\n   * - :ref:`Spectral clustering <spectral_clustering>`\n     - number of clusters\n     - Medium ``n_samples``, small ``n_clusters``\n     - Few clusters, even cluster size, non-flat geometry, transductive\n     - Graph distance (e.g. nearest-neighbor graph)\n\n   * - :ref:`Ward hierarchical clustering <hierarchical_clustering>`\n     - number of clusters or distance threshold\n     - Large ``n_samples`` and ``n_clusters``\n     - Many clusters, possibly connectivity constraints, transductive\n     - Distances between points\n\n   * - :ref:`Agglomerative clustering <hierarchical_clustering>`\n     - number of clusters or distance threshold, linkage type, distance\n     - Large ``n_samples`` and ``n_clusters``\n     - Many clusters, possibly connectivity constraints, non Euclidean\n       distances, transductive\n     - Any pairwise distance\n\n   * - :ref:`DBSCAN <dbscan>`\n     - neighborhood size\n     - Very large ``n_samples``, medium ``n_clusters``\n     - Non-flat geometry, uneven cluster sizes, outlier removal,\n       transductive\n     - Distances between nearest points\n\n   * - :ref:`HDBSCAN <hdbscan>`\n     - minimum cluster membership, minimum point neighbors\n     - large ``n_samples``, medium ``n_clusters``\n     - Non-flat geometry, uneven cluster sizes, outlier removal,\n       transductive, hierarchical, variable cluster density\n     - Distances between nearest points\n\n   * - :ref:`OPTICS <optics>`\n     - minimum cluster membership\n     - Very large ``n_samples``, large ``n_clusters``\n     - Non-flat geometry, uneven cluster sizes, variable cluster density,\n       outlier removal, transductive\n     - Distances between points\n\n   * - :ref:`Gaussian mixtures <mixture>`\n     - many\n     - Not scalable\n     - Flat geometry, good for density estimation, inductive\n     - Mahalanobis distances to  centers\n\n   * - :ref:`BIRCH <birch>`\n     - branching factor, threshold, optional global clusterer.\n     - Large ``n_clusters`` and ``n_samples``\n     - Large dataset, outlier removal, data reduction, inductive\n     - Euclidean distance between points\n\n   * - :ref:`Bisecting K-Means <bisect_k_means>`\n     - number of clusters\n     - Very large ``n_samples``, medium ``n_clusters``\n     - General-purpose, even cluster size, flat geometry,\n       no empty clusters, inductive, hierarchical\n     - Distances between points\n\nNon-flat geometry clustering is useful when the clusters have a specific\nshape, i.e. a non-flat manifold, and the standard euclidean distance is\nnot the right metric. This case arises in the two top rows of the figure\nabove.\n\nGaussian mixture models, useful for clustering, are described in\n:ref:`another chapter of the documentation <mixture>` dedicated to\nmixture models. KMeans can be seen as a special case of Gaussian mixture\nmodel with equal covariance per component.\n\n:term:`Transductive <transductive>` clustering methods (in contrast to\n:term:`inductive` clustering methods) are not designed to be applied to new,\nunseen data.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_cluster_plot_inductive_clustering.py`: An example\n  of an inductive clustering model for handling new data.\n\n.. _k_means:"
  },
  {
    "filename": "clustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\clustering.rst.txt",
    "id": "clustering.rst.txt_chunk_3",
    "header": "K-means",
    "text": "K-means\n=======\n\nThe :class:`KMeans` algorithm clusters data by trying to separate samples in n\ngroups of equal variance, minimizing a criterion known as the *inertia* or\nwithin-cluster sum-of-squares (see below). This algorithm requires the number\nof clusters to be specified. It scales well to large numbers of samples and has\nbeen used across a large range of application areas in many different fields.\n\nThe k-means algorithm divides a set of :math:`N` samples :math:`X` into\n:math:`K` disjoint clusters :math:`C`, each described by the mean :math:`\\mu_j`\nof the samples in the cluster. The means are commonly called the cluster\n\"centroids\"; note that they are not, in general, points from :math:`X`,\nalthough they live in the same space.\n\nThe K-means algorithm aims to choose centroids that minimise the **inertia**,\nor **within-cluster sum-of-squares criterion**:\n\n.. math:: \\sum_{i=0}^{n}\\min_{\\mu_j \\in C}(||x_i - \\mu_j||^2)\n\nInertia can be recognized as a measure of how internally coherent clusters are.\nIt suffers from various drawbacks:\n\n- Inertia makes the assumption that clusters are convex and isotropic,\n  which is not always the case. It responds poorly to elongated clusters,\n  or manifolds with irregular shapes.\n\n- Inertia is not a normalized metric: we just know that lower values are\n  better and zero is optimal. But in very high-dimensional spaces, Euclidean\n  distances tend to become inflated\n  (this is an instance of the so-called \"curse of dimensionality\").\n  Running a dimensionality reduction algorithm such as :ref:`PCA` prior to\n  k-means clustering can alleviate this problem and speed up the\n  computations.\n\n.. image:: ../auto_examples/cluster/images/sphx_glr_plot_kmeans_assumptions_002.png\n   :target: ../auto_examples/cluster/plot_kmeans_assumptions.html\n   :align: center\n   :scale: 50\n\nFor more detailed descriptions of the issues shown above and how to address them,\nrefer to the examples :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_assumptions.py`\nand :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_silhouette_analysis.py`.\n\nK-means is often referred to as Lloyd's algorithm. In basic terms, the\nalgorithm has three steps. The first step chooses the initial centroids, with\nthe most basic method being to choose :math:`k` samples from the dataset\n:math:`X`. After initialization, K-means consists of looping between the\ntwo other steps. The first step assigns each sample to its nearest centroid.\nThe second step creates new centroids by taking the mean value of all of the\nsamples assigned to each previous centroid. The difference between the old\nand the new centroids are computed and the algorithm repeats these last two\nsteps until this value is less than a threshold. In other words, it repeats\nuntil the centroids do not move significantly.\n\n.. image:: ../auto_examples/cluster/images/sphx_glr_plot_kmeans_digits_001.png\n   :target: ../auto_examples/cluster/plot_kmeans_digits.html\n   :align: right\n   :scale: 35\n\nK-means is equivalent to the expectation-maximization algorithm\nwith a small, all-equal, diagonal covariance matrix.\n\nThe algorithm can also be understood through the concept of `Voronoi diagrams\n<https://en.wikipedia.org/wiki/Voronoi_diagram>`_. First the Voronoi diagram of\nthe points is calculated using the current centroids. Each segment in the\nVoronoi diagram becomes a separate cluster. Secondly, the centroids are updated\nto the mean of each segment. The algorithm then repeats this until a stopping\ncriterion is fulfilled. Usually, the algorithm stops when the relative decrease\nin the objective function between iterations is less than the given tolerance\nvalue. This is not the case in this implementation: iteration stops when\ncentroids move less than the tolerance.\n\nGiven enough time, K-means will always converge, however this may be to a local\nminimum. This is highly dependent on the initialization of the centroids.\nAs a result, the computation is often done several times, with different\ninitializations of the centroids. One method to help address this issue is the\nk-means++ initialization scheme, which has been implemented in scikit-learn\n(use the ``init='k-means++'`` parameter). This initializes the centroids to be\n(generally) distant from each other, leading to probably better results than\nrandom initialization, as shown in the reference. For detailed examples of\ncomparing different initialization schemes, refer to\n:ref:`sphx_glr_auto_examples_cluster_plot_kmeans_digits.py` and\n:ref:`sphx_glr_auto_examples_cluster_plot_kmeans_stability_low_dim_dense.py`.\n\nK-means++ can also be called independently to select seeds for other\nclustering algorithms, see :func:`sklearn.cluster.kmeans_plusplus` for details\nand example usage.\n\nThe algorithm supports sample weights, which can be given by a parameter\n``sample_weight``. This allows to assign more weight to some samples when\ncomputing cluster centers and values of inertia. For example, assigning a\nweight of 2 to a sample is equivalent to adding a duplicate of that sample\nto the dataset :math:`X`.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`: Document clustering\n  using :class:`KMeans` and :class:`MiniBatchKMeans` based on sparse data\n\n* :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_plusplus.py`: Using K-means++\n  to select seeds for other clustering algorithms."
  },
  {
    "filename": "clustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\clustering.rst.txt",
    "id": "clustering.rst.txt_chunk_4",
    "header": "Low-level parallelism",
    "text": "Low-level parallelism\n---------------------\n\n:class:`KMeans` benefits from OpenMP based parallelism through Cython. Small\nchunks of data (256 samples) are processed in parallel, which in addition\nyields a low memory footprint. For more details on how to control the number of\nthreads, please refer to our :ref:`parallelism` notes.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_assumptions.py`: Demonstrating when\n  k-means performs intuitively and when it does not\n* :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_digits.py`: Clustering handwritten digits\n\n.. dropdown:: References\n\n  * `\"k-means++: The advantages of careful seeding\"\n    <http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf>`_\n    Arthur, David, and Sergei Vassilvitskii,\n    *Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete\n    algorithms*, Society for Industrial and Applied Mathematics (2007)\n\n\n.. _mini_batch_kmeans:"
  },
  {
    "filename": "clustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\clustering.rst.txt",
    "id": "clustering.rst.txt_chunk_5",
    "header": "Mini Batch K-Means",
    "text": "Mini Batch K-Means\n------------------\n\nThe :class:`MiniBatchKMeans` is a variant of the :class:`KMeans` algorithm\nwhich uses mini-batches to reduce the computation time, while still attempting\nto optimise the same objective function. Mini-batches are subsets of the input\ndata, randomly sampled in each training iteration. These mini-batches\ndrastically reduce the amount of computation required to converge to a local\nsolution. In contrast to other algorithms that reduce the convergence time of\nk-means, mini-batch k-means produces results that are generally only slightly\nworse than the standard algorithm.\n\nThe algorithm iterates between two major steps, similar to vanilla k-means.\nIn the first step, :math:`b` samples are drawn randomly from the dataset, to form\na mini-batch. These are then assigned to the nearest centroid. In the second\nstep, the centroids are updated. In contrast to k-means, this is done on a\nper-sample basis. For each sample in the mini-batch, the assigned centroid\nis updated by taking the streaming average of the sample and all previous\nsamples assigned to that centroid. This has the effect of decreasing the\nrate of change for a centroid over time. These steps are performed until\nconvergence or a predetermined number of iterations is reached.\n\n:class:`MiniBatchKMeans` converges faster than :class:`KMeans`, but the quality\nof the results is reduced. In practice this difference in quality can be quite\nsmall, as shown in the example and cited reference.\n\n.. figure:: ../auto_examples/cluster/images/sphx_glr_plot_mini_batch_kmeans_001.png\n   :target: ../auto_examples/cluster/plot_mini_batch_kmeans.html\n   :align: center\n   :scale: 100\n\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_cluster_plot_mini_batch_kmeans.py`: Comparison of\n  :class:`KMeans` and :class:`MiniBatchKMeans`\n\n* :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`: Document clustering\n  using :class:`KMeans` and :class:`MiniBatchKMeans` based on sparse data\n\n* :ref:`sphx_glr_auto_examples_cluster_plot_dict_face_patches.py`\n\n.. dropdown:: References\n\n  * `\"Web Scale K-Means clustering\"\n    <https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf>`_\n    D. Sculley, *Proceedings of the 19th international conference on World\n    wide web* (2010)\n\n.. _affinity_propagation:"
  },
  {
    "filename": "clustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\clustering.rst.txt",
    "id": "clustering.rst.txt_chunk_6",
    "header": "Affinity Propagation",
    "text": "Affinity Propagation\n====================\n\n:class:`AffinityPropagation` creates clusters by sending messages between\npairs of samples until convergence. A dataset is then described using a small\nnumber of exemplars, which are identified as those most representative of other\nsamples. The messages sent between pairs represent the suitability for one\nsample to be the exemplar of the other, which is updated in response to the\nvalues from other pairs. This updating happens iteratively until convergence,\nat which point the final exemplars are chosen, and hence the final clustering\nis given.\n\n.. figure:: ../auto_examples/cluster/images/sphx_glr_plot_affinity_propagation_001.png\n   :target: ../auto_examples/cluster/plot_affinity_propagation.html\n   :align: center\n   :scale: 50\n\n\nAffinity Propagation can be interesting as it chooses the number of\nclusters based on the data provided. For this purpose, the two important\nparameters are the *preference*, which controls how many exemplars are\nused, and the *damping factor* which damps the responsibility and\navailability messages to avoid numerical oscillations when updating these\nmessages.\n\nThe main drawback of Affinity Propagation is its complexity. The\nalgorithm has a time complexity of the order :math:`O(N^2 T)`, where :math:`N`\nis the number of samples and :math:`T` is the number of iterations until\nconvergence. Further, the memory complexity is of the order\n:math:`O(N^2)` if a dense similarity matrix is used, but reducible if a\nsparse similarity matrix is used. This makes Affinity Propagation most\nappropriate for small to medium sized datasets.\n\n.. dropdown:: Algorithm description\n\n  The messages sent between points belong to one of two categories. The first is\n  the responsibility :math:`r(i, k)`, which is the accumulated evidence that\n  sample :math:`k` should be the exemplar for sample :math:`i`. The second is the\n  availability :math:`a(i, k)` which is the accumulated evidence that sample\n  :math:`i` should choose sample :math:`k` to be its exemplar, and considers the\n  values for all other samples that :math:`k` should be an exemplar. In this way,\n  exemplars are chosen by samples if they are (1) similar enough to many samples\n  and (2) chosen by many samples to be representative of themselves.\n\n  More formally, the responsibility of a sample :math:`k` to be the exemplar of\n  sample :math:`i` is given by:\n\n  .. math::\n\n      r(i, k) \\leftarrow s(i, k) - max [ a(i, k') + s(i, k') \\forall k' \\neq k ]\n\n  Where :math:`s(i, k)` is the similarity between samples :math:`i` and :math:`k`.\n  The availability of sample :math:`k` to be the exemplar of sample :math:`i` is\n  given by:\n\n  .. math::\n\n      a(i, k) \\leftarrow min [0, r(k, k) + \\sum_{i'~s.t.~i' \\notin \\{i, k\\}}{r(i',\n      k)}]\n\n  To begin with, all values for :math:`r` and :math:`a` are set to zero, and the\n  calculation of each iterates until convergence. As discussed above, in order to\n  avoid numerical oscillations when updating the messages, the damping factor\n  :math:`\\lambda` is introduced to iteration process:\n\n  .. math:: r_{t+1}(i, k) = \\lambda\\cdot r_{t}(i, k) + (1-\\lambda)\\cdot r_{t+1}(i, k)\n  .. math:: a_{t+1}(i, k) = \\lambda\\cdot a_{t}(i, k) + (1-\\lambda)\\cdot a_{t+1}(i, k)\n\n  where :math:`t` indicates the iteration times.\n\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_cluster_plot_affinity_propagation.py`: Affinity\n  Propagation on a synthetic 2D datasets with 3 classes\n* :ref:`sphx_glr_auto_examples_applications_plot_stock_market.py` Affinity Propagation\n  on financial time series to find groups of companies\n\n\n.. _mean_shift:"
  },
  {
    "filename": "clustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\clustering.rst.txt",
    "id": "clustering.rst.txt_chunk_7",
    "header": "Mean Shift",
    "text": "Mean Shift\n==========\n:class:`MeanShift` clustering aims to discover *blobs* in a smooth density of\nsamples. It is a centroid based algorithm, which works by updating candidates\nfor centroids to be the mean of the points within a given region. These\ncandidates are then filtered in a post-processing stage to eliminate\nnear-duplicates to form the final set of centroids.\n\n.. dropdown:: Mathematical details\n\n  The position of centroid candidates is iteratively adjusted using a technique\n  called hill climbing, which finds local maxima of the estimated probability\n  density. Given a candidate centroid :math:`x` for iteration :math:`t`, the\n  candidate is updated according to the following equation:\n\n  .. math::\n\n      x^{t+1} = x^t + m(x^t)\n\n  Where :math:`m` is the *mean shift* vector that is computed for each centroid\n  that points towards a region of the maximum increase in the density of points.\n  To compute :math:`m` we define :math:`N(x)` as the neighborhood of samples\n  within a given distance around :math:`x`. Then :math:`m` is computed using the\n  following equation, effectively updating a centroid to be the mean of the\n  samples within its neighborhood:\n\n  .. math::\n\n      m(x) = \\frac{1}{|N(x)|} \\sum_{x_j \\in N(x)}x_j - x\n\n  In general, the equation for :math:`m` depends on a kernel used for density\n  estimation. The generic formula is:\n\n  .. math::\n\n      m(x) = \\frac{\\sum_{x_j \\in N(x)}K(x_j - x)x_j}{\\sum_{x_j \\in N(x)}K(x_j -\n      x)} - x\n\n  In our implementation, :math:`K(x)` is equal to 1 if :math:`x` is small enough\n  and is equal to 0 otherwise. Effectively :math:`K(y - x)` indicates whether\n  :math:`y` is in the neighborhood of :math:`x`.\n\n\nThe algorithm automatically sets the number of clusters, instead of relying on a\nparameter ``bandwidth``, which dictates the size of the region to search through.\nThis parameter can be set manually, but can be estimated using the provided\n``estimate_bandwidth`` function, which is called if the bandwidth is not set.\n\nThe algorithm is not highly scalable, as it requires multiple nearest neighbor\nsearches during the execution of the algorithm. The algorithm is guaranteed to\nconverge, however the algorithm will stop iterating when the change in centroids\nis small.\n\nLabelling a new sample is performed by finding the nearest centroid for a\ngiven sample.\n\n\n.. figure:: ../auto_examples/cluster/images/sphx_glr_plot_mean_shift_001.png\n   :target: ../auto_examples/cluster/plot_mean_shift.html\n   :align: center\n   :scale: 50\n\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_cluster_plot_mean_shift.py`: Mean Shift clustering\n  on a synthetic 2D datasets with 3 classes.\n\n.. dropdown:: References\n\n  * :doi:`\"Mean shift: A robust approach toward feature space analysis\"\n    <10.1109/34.1000236>` D. Comaniciu and P. Meer, *IEEE Transactions on Pattern\n    Analysis and Machine Intelligence* (2002)\n\n\n.. _spectral_clustering:"
  },
  {
    "filename": "clustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\clustering.rst.txt",
    "id": "clustering.rst.txt_chunk_8",
    "header": "Spectral clustering",
    "text": "Spectral clustering\n===================\n\n:class:`SpectralClustering` performs a low-dimension embedding of the\naffinity matrix between samples, followed by clustering, e.g., by KMeans,\nof the components of the eigenvectors in the low dimensional space.\nIt is especially computationally efficient if the affinity matrix is sparse\nand the `amg` solver is used for the eigenvalue problem (Note, the `amg` solver\nrequires that the `pyamg <https://github.com/pyamg/pyamg>`_ module is installed.)\n\nThe present version of SpectralClustering requires the number of clusters\nto be specified in advance. It works well for a small number of clusters,\nbut is not advised for many clusters.\n\nFor two clusters, SpectralClustering solves a convex relaxation of the\n`normalized cuts <https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf>`_\nproblem on the similarity graph: cutting the graph in two so that the weight of\nthe edges cut is small compared to the weights of the edges inside each\ncluster. This criteria is especially interesting when working on images, where\ngraph vertices are pixels, and weights of the edges of the similarity graph are\ncomputed using a function of a gradient of the image.\n\n\n.. |noisy_img| image:: ../auto_examples/cluster/images/sphx_glr_plot_segmentation_toy_001.png\n    :target: ../auto_examples/cluster/plot_segmentation_toy.html\n    :scale: 50\n\n.. |segmented_img| image:: ../auto_examples/cluster/images/sphx_glr_plot_segmentation_toy_002.png\n    :target: ../auto_examples/cluster/plot_segmentation_toy.html\n    :scale: 50\n\n.. centered:: |noisy_img| |segmented_img|\n\n.. warning:: Transforming distance to well-behaved similarities\n\n    Note that if the values of your similarity matrix are not well\n    distributed, e.g. with negative values or with a distance matrix\n    rather than a similarity, the spectral problem will be singular and\n    the problem not solvable. In which case it is advised to apply a\n    transformation to the entries of the matrix. For instance, in the\n    case of a signed distance matrix, is common to apply a heat kernel::\n\n        similarity = np.exp(-beta * distance / distance.std())\n\n    See the examples for such an application.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_cluster_plot_segmentation_toy.py`: Segmenting objects\n  from a noisy background using spectral clustering.\n* :ref:`sphx_glr_auto_examples_cluster_plot_coin_segmentation.py`: Spectral clustering\n  to split the image of coins in regions.\n\n\n.. |coin_kmeans| image:: ../auto_examples/cluster/images/sphx_glr_plot_coin_segmentation_001.png\n  :target: ../auto_examples/cluster/plot_coin_segmentation.html\n  :scale: 35\n\n.. |coin_discretize| image:: ../auto_examples/cluster/images/sphx_glr_plot_coin_segmentation_002.png\n  :target: ../auto_examples/cluster/plot_coin_segmentation.html\n  :scale: 35\n\n.. |coin_cluster_qr| image:: ../auto_examples/cluster/images/sphx_glr_plot_coin_segmentation_003.png\n  :target: ../auto_examples/cluster/plot_coin_segmentation.html\n  :scale: 35"
  },
  {
    "filename": "clustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\clustering.rst.txt",
    "id": "clustering.rst.txt_chunk_9",
    "header": "Different label assignment strategies",
    "text": "Different label assignment strategies\n-------------------------------------\n\nDifferent label assignment strategies can be used, corresponding to the\n``assign_labels`` parameter of :class:`SpectralClustering`.\n``\"kmeans\"`` strategy can match finer details, but can be unstable.\nIn particular, unless you control the ``random_state``, it may not be\nreproducible from run-to-run, as it depends on random initialization.\nThe alternative ``\"discretize\"`` strategy is 100% reproducible, but tends\nto create parcels of fairly even and geometrical shape.\nThe recently added ``\"cluster_qr\"`` option is a deterministic alternative that\ntends to create the visually best partitioning on the example application\nbelow.\n\n================================  ================================  ================================\n ``assign_labels=\"kmeans\"``        ``assign_labels=\"discretize\"``    ``assign_labels=\"cluster_qr\"``\n================================  ================================  ================================\n|coin_kmeans|                          |coin_discretize|                  |coin_cluster_qr|\n================================  ================================  ================================\n\n.. dropdown:: References\n\n  * `\"Multiclass spectral clustering\"\n    <https://people.eecs.berkeley.edu/~jordan/courses/281B-spring04/readings/yu-shi.pdf>`_\n    Stella X. Yu, Jianbo Shi, 2003\n\n  * :doi:`\"Simple, direct, and efficient multi-way spectral clustering\"<10.1093/imaiai/iay008>`\n    Anil Damle, Victor Minden, Lexing Ying, 2019\n\n\n.. _spectral_clustering_graph:"
  },
  {
    "filename": "clustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\clustering.rst.txt",
    "id": "clustering.rst.txt_chunk_10",
    "header": "Spectral Clustering Graphs",
    "text": "Spectral Clustering Graphs\n--------------------------\n\nSpectral Clustering can also be used to partition graphs via their spectral\nembeddings.  In this case, the affinity matrix is the adjacency matrix of the\ngraph, and SpectralClustering is initialized with `affinity='precomputed'`::\n\n    >>> from sklearn.cluster import SpectralClustering\n    >>> sc = SpectralClustering(3, affinity='precomputed', n_init=100,\n    ...                         assign_labels='discretize')\n    >>> sc.fit_predict(adjacency_matrix)  # doctest: +SKIP\n\n.. dropdown:: References\n\n  * :doi:`\"A Tutorial on Spectral Clustering\" <10.1007/s11222-007-9033-z>` Ulrike\n    von Luxburg, 2007\n\n  * :doi:`\"Normalized cuts and image segmentation\" <10.1109/34.868688>` Jianbo\n    Shi, Jitendra Malik, 2000\n\n  * `\"A Random Walks View of Spectral Segmentation\"\n    <https://citeseerx.ist.psu.edu/doc_view/pid/84a86a69315e994cfd1e0c7debb86d62d7bd1f44>`_\n    Marina Meila, Jianbo Shi, 2001\n\n  * `\"On Spectral Clustering: Analysis and an algorithm\"\n    <https://citeseerx.ist.psu.edu/doc_view/pid/796c5d6336fc52aa84db575fb821c78918b65f58>`_\n    Andrew Y. Ng, Michael I. Jordan, Yair Weiss, 2001\n\n  * :arxiv:`\"Preconditioned Spectral Clustering for Stochastic Block Partition\n    Streaming Graph Challenge\" <1708.07481>` David Zhuzhunashvili, Andrew Knyazev\n\n\n.. _hierarchical_clustering:"
  },
  {
    "filename": "clustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\clustering.rst.txt",
    "id": "clustering.rst.txt_chunk_11",
    "header": "Hierarchical clustering",
    "text": "Hierarchical clustering\n=======================\n\nHierarchical clustering is a general family of clustering algorithms that\nbuild nested clusters by merging or splitting them successively. This\nhierarchy of clusters is represented as a tree (or dendrogram). The root of the\ntree is the unique cluster that gathers all the samples, the leaves being the\nclusters with only one sample. See the `Wikipedia page\n<https://en.wikipedia.org/wiki/Hierarchical_clustering>`_ for more details.\n\nThe :class:`AgglomerativeClustering` object performs a hierarchical clustering\nusing a bottom up approach: each observation starts in its own cluster, and\nclusters are successively merged together. The linkage criteria determines the\nmetric used for the merge strategy:\n\n- **Ward** minimizes the sum of squared differences within all clusters. It is a\n  variance-minimizing approach and in this sense is similar to the k-means\n  objective function but tackled with an agglomerative hierarchical\n  approach.\n- **Maximum** or **complete linkage** minimizes the maximum distance between\n  observations of pairs of clusters.\n- **Average linkage** minimizes the average of the distances between all\n  observations of pairs of clusters.\n- **Single linkage** minimizes the distance between the closest\n  observations of pairs of clusters.\n\n:class:`AgglomerativeClustering` can also scale to large number of samples\nwhen it is used jointly with a connectivity matrix, but is computationally\nexpensive when no connectivity constraints are added between samples: it\nconsiders at each step all the possible merges.\n\n.. topic:: :class:`FeatureAgglomeration`\n\n   The :class:`FeatureAgglomeration` uses agglomerative clustering to\n   group together features that look very similar, thus decreasing the\n   number of features. It is a dimensionality reduction tool, see\n   :ref:`data_reduction`."
  },
  {
    "filename": "clustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\clustering.rst.txt",
    "id": "clustering.rst.txt_chunk_12",
    "header": "Different linkage type: Ward, complete, average, and single linkage",
    "text": "Different linkage type: Ward, complete, average, and single linkage\n-------------------------------------------------------------------\n\n:class:`AgglomerativeClustering` supports Ward, single, average, and complete\nlinkage strategies.\n\n.. image:: ../auto_examples/cluster/images/sphx_glr_plot_linkage_comparison_001.png\n    :target: ../auto_examples/cluster/plot_linkage_comparison.html\n    :scale: 43\n\nAgglomerative cluster has a \"rich get richer\" behavior that leads to\nuneven cluster sizes. In this regard, single linkage is the worst\nstrategy, and Ward gives the most regular sizes. However, the affinity\n(or distance used in clustering) cannot be varied with Ward, thus for non\nEuclidean metrics, average linkage is a good alternative. Single linkage,\nwhile not robust to noisy data, can be computed very efficiently and can\ntherefore be useful to provide hierarchical clustering of larger datasets.\nSingle linkage can also perform well on non-globular data.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_cluster_plot_digits_linkage.py`: exploration of the\n  different linkage strategies in a real dataset.\n\n  * :ref:`sphx_glr_auto_examples_cluster_plot_linkage_comparison.py`: exploration of\n    the different linkage strategies in toy datasets."
  },
  {
    "filename": "clustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\clustering.rst.txt",
    "id": "clustering.rst.txt_chunk_13",
    "header": "Visualization of cluster hierarchy",
    "text": "Visualization of cluster hierarchy\n----------------------------------\n\nIt's possible to visualize the tree representing the hierarchical merging of clusters\nas a dendrogram. Visual inspection can often be useful for understanding the structure\nof the data, though more so in the case of small sample sizes.\n\n.. image:: ../auto_examples/cluster/images/sphx_glr_plot_agglomerative_dendrogram_001.png\n    :target: ../auto_examples/cluster/plot_agglomerative_dendrogram.html\n    :scale: 42\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_cluster_plot_agglomerative_dendrogram.py`"
  },
  {
    "filename": "clustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\clustering.rst.txt",
    "id": "clustering.rst.txt_chunk_14",
    "header": "Adding connectivity constraints",
    "text": "Adding connectivity constraints\n-------------------------------\n\nAn interesting aspect of :class:`AgglomerativeClustering` is that\nconnectivity constraints can be added to this algorithm (only adjacent\nclusters can be merged together), through a connectivity matrix that defines\nfor each sample the neighboring samples following a given structure of the\ndata. For instance, in the swiss-roll example below, the connectivity\nconstraints forbid the merging of points that are not adjacent on the swiss\nroll, and thus avoid forming clusters that extend across overlapping folds of\nthe roll.\n\n.. |unstructured| image:: ../auto_examples/cluster/images/sphx_glr_plot_ward_structured_vs_unstructured_001.png\n        :target: ../auto_examples/cluster/plot_ward_structured_vs_unstructured.html\n        :scale: 49\n\n.. |structured| image:: ../auto_examples/cluster/images/sphx_glr_plot_ward_structured_vs_unstructured_002.png\n        :target: ../auto_examples/cluster/plot_ward_structured_vs_unstructured.html\n        :scale: 49\n\n.. centered:: |unstructured| |structured|\n\nThese constraint are useful to impose a certain local structure, but they\nalso make the algorithm faster, especially when the number of the samples\nis high.\n\nThe connectivity constraints are imposed via an connectivity matrix: a\nscipy sparse matrix that has elements only at the intersection of a row\nand a column with indices of the dataset that should be connected. This\nmatrix can be constructed from a-priori information: for instance, you\nmay wish to cluster web pages by only merging pages with a link pointing\nfrom one to another. It can also be learned from the data, for instance\nusing :func:`sklearn.neighbors.kneighbors_graph` to restrict\nmerging to nearest neighbors as in :ref:`this example\n<sphx_glr_auto_examples_cluster_plot_agglomerative_clustering.py>`, or\nusing :func:`sklearn.feature_extraction.image.grid_to_graph` to\nenable only merging of neighboring pixels on an image, as in the\n:ref:`coin <sphx_glr_auto_examples_cluster_plot_coin_ward_segmentation.py>` example.\n\n.. warning:: **Connectivity constraints with single, average and complete linkage**\n\n    Connectivity constraints and single, complete or average linkage can enhance\n    the 'rich getting richer' aspect of agglomerative clustering,\n    particularly so if they are built with\n    :func:`sklearn.neighbors.kneighbors_graph`. In the limit of a small\n    number of clusters, they tend to give a few macroscopically occupied\n    clusters and almost empty ones. (see the discussion in\n    :ref:`sphx_glr_auto_examples_cluster_plot_agglomerative_clustering.py`).\n    Single linkage is the most brittle linkage option with regard to this issue.\n\n.. image:: ../auto_examples/cluster/images/sphx_glr_plot_agglomerative_clustering_001.png\n    :target: ../auto_examples/cluster/plot_agglomerative_clustering.html\n    :scale: 38\n\n.. image:: ../auto_examples/cluster/images/sphx_glr_plot_agglomerative_clustering_002.png\n    :target: ../auto_examples/cluster/plot_agglomerative_clustering.html\n    :scale: 38\n\n.. image:: ../auto_examples/cluster/images/sphx_glr_plot_agglomerative_clustering_003.png\n    :target: ../auto_examples/cluster/plot_agglomerative_clustering.html\n    :scale: 38\n\n.. image:: ../auto_examples/cluster/images/sphx_glr_plot_agglomerative_clustering_004.png\n    :target: ../auto_examples/cluster/plot_agglomerative_clustering.html\n    :scale: 38\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_cluster_plot_coin_ward_segmentation.py`: Ward\n  clustering to split the image of coins in regions.\n\n* :ref:`sphx_glr_auto_examples_cluster_plot_ward_structured_vs_unstructured.py`: Example\n  of Ward algorithm on a swiss-roll, comparison of structured approaches\n  versus unstructured approaches.\n\n* :ref:`sphx_glr_auto_examples_cluster_plot_feature_agglomeration_vs_univariate_selection.py`: Example\n  of dimensionality reduction with feature agglomeration based on Ward\n  hierarchical clustering.\n\n* :ref:`sphx_glr_auto_examples_cluster_plot_agglomerative_clustering.py`"
  },
  {
    "filename": "clustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\clustering.rst.txt",
    "id": "clustering.rst.txt_chunk_15",
    "header": "Varying the metric",
    "text": "Varying the metric\n-------------------\n\nSingle, average and complete linkage can be used with a variety of distances (or\naffinities), in particular Euclidean distance (*l2*), Manhattan distance\n(or Cityblock, or *l1*), cosine distance, or any precomputed affinity\nmatrix.\n\n* *l1* distance is often good for sparse features, or sparse noise: i.e.\n  many of the features are zero, as in text mining using occurrences of\n  rare words.\n\n* *cosine* distance is interesting because it is invariant to global\n  scalings of the signal.\n\nThe guidelines for choosing a metric is to use one that maximizes the\ndistance between samples in different classes, and minimizes that within\neach class.\n\n.. image:: ../auto_examples/cluster/images/sphx_glr_plot_agglomerative_clustering_metrics_005.png\n    :target: ../auto_examples/cluster/plot_agglomerative_clustering_metrics.html\n    :scale: 32\n\n.. image:: ../auto_examples/cluster/images/sphx_glr_plot_agglomerative_clustering_metrics_006.png\n    :target: ../auto_examples/cluster/plot_agglomerative_clustering_metrics.html\n    :scale: 32\n\n.. image:: ../auto_examples/cluster/images/sphx_glr_plot_agglomerative_clustering_metrics_007.png\n    :target: ../auto_examples/cluster/plot_agglomerative_clustering_metrics.html\n    :scale: 32\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_cluster_plot_agglomerative_clustering_metrics.py`"
  },
  {
    "filename": "clustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\clustering.rst.txt",
    "id": "clustering.rst.txt_chunk_16",
    "header": "Bisecting K-Means",
    "text": "Bisecting K-Means\n-----------------\n\n.. _bisect_k_means:\n\nThe :class:`BisectingKMeans` is an iterative variant of :class:`KMeans`, using\ndivisive hierarchical clustering. Instead of creating all centroids at once, centroids\nare picked progressively based on a previous clustering: a cluster is split into two\nnew clusters repeatedly until the target number of clusters is reached.\n\n:class:`BisectingKMeans` is more efficient than :class:`KMeans` when the number of\nclusters is large since it only works on a subset of the data at each bisection\nwhile :class:`KMeans` always works on the entire dataset.\n\nAlthough :class:`BisectingKMeans` can't benefit from the advantages of the `\"k-means++\"`\ninitialization by design, it will still produce comparable results than\n`KMeans(init=\"k-means++\")` in terms of inertia at cheaper computational costs, and will\nlikely produce better results than `KMeans` with a random initialization.\n\nThis variant is more efficient to agglomerative clustering if the number of clusters is\nsmall compared to the number of data points.\n\nThis variant also does not produce empty clusters.\n\nThere exist two strategies for selecting the cluster to split:\n - ``bisecting_strategy=\"largest_cluster\"`` selects the cluster having the most points\n - ``bisecting_strategy=\"biggest_inertia\"`` selects the cluster with biggest inertia\n   (cluster with biggest Sum of Squared Errors within)\n\nPicking by largest amount of data points in most cases produces result as\naccurate as picking by inertia and is faster (especially for larger amount of data\npoints, where calculating error may be costly).\n\nPicking by largest amount of data points will also likely produce clusters of similar\nsizes while `KMeans` is known to produce clusters of different sizes.\n\nDifference between Bisecting K-Means and regular K-Means can be seen on example\n:ref:`sphx_glr_auto_examples_cluster_plot_bisect_kmeans.py`.\nWhile the regular K-Means algorithm tends to create non-related clusters,\nclusters from Bisecting K-Means are well ordered and create quite a visible hierarchy.\n\n.. dropdown:: References\n\n  * `\"A Comparison of Document Clustering Techniques\"\n    <http://www.philippe-fournier-viger.com/spmf/bisectingkmeans.pdf>`_ Michael\n    Steinbach, George Karypis and Vipin Kumar, Department of Computer Science and\n    Egineering, University of Minnesota (June 2000)\n  * `\"Performance Analysis of K-Means and Bisecting K-Means Algorithms in Weblog\n    Data\"\n    <https://ijeter.everscience.org/Manuscripts/Volume-4/Issue-8/Vol-4-issue-8-M-23.pdf>`_\n    K.Abirami and Dr.P.Mayilvahanan, International Journal of Emerging\n    Technologies in Engineering Research (IJETER) Volume 4, Issue 8, (August 2016)\n  * `\"Bisecting K-means Algorithm Based on K-valued Self-determining and\n    Clustering Center Optimization\"\n    <http://www.jcomputers.us/vol13/jcp1306-01.pdf>`_ Jian Di, Xinyue Gou School\n    of Control and Computer Engineering,North China Electric Power University,\n    Baoding, Hebei, China (August 2017)\n\n\n.. _dbscan:"
  },
  {
    "filename": "clustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\clustering.rst.txt",
    "id": "clustering.rst.txt_chunk_17",
    "header": "DBSCAN",
    "text": "DBSCAN\n======\n\nThe :class:`DBSCAN` algorithm views clusters as areas of high density\nseparated by areas of low density. Due to this rather generic view, clusters\nfound by DBSCAN can be any shape, as opposed to k-means which assumes that\nclusters are convex shaped. The central component to the DBSCAN is the concept\nof *core samples*, which are samples that are in areas of high density. A\ncluster is therefore a set of core samples, each close to each other\n(measured by some distance measure)\nand a set of non-core samples that are close to a core sample (but are not\nthemselves core samples). There are two parameters to the algorithm,\n``min_samples`` and ``eps``,\nwhich define formally what we mean when we say *dense*.\nHigher ``min_samples`` or lower ``eps``\nindicate higher density necessary to form a cluster.\n\nMore formally, we define a core sample as being a sample in the dataset such\nthat there exist ``min_samples`` other samples within a distance of\n``eps``, which are defined as *neighbors* of the core sample. This tells\nus that the core sample is in a dense area of the vector space. A cluster\nis a set of core samples that can be built by recursively taking a core\nsample, finding all of its neighbors that are core samples, finding all of\n*their* neighbors that are core samples, and so on. A cluster also has a\nset of non-core samples, which are samples that are neighbors of a core sample\nin the cluster but are not themselves core samples. Intuitively, these samples\nare on the fringes of a cluster.\n\nAny core sample is part of a cluster, by definition. Any sample that is not a\ncore sample, and is at least ``eps`` in distance from any core sample, is\nconsidered an outlier by the algorithm.\n\nWhile the parameter ``min_samples`` primarily controls how tolerant the\nalgorithm is towards noise (on noisy and large data sets it may be desirable\nto increase this parameter), the parameter ``eps`` is *crucial to choose\nappropriately* for the data set and distance function and usually cannot be\nleft at the default value. It controls the local neighborhood of the points.\nWhen chosen too small, most data will not be clustered at all (and labeled\nas ``-1`` for \"noise\"). When chosen too large, it causes close clusters to\nbe merged into one cluster, and eventually the entire data set to be returned\nas a single cluster. Some heuristics for choosing this parameter have been\ndiscussed in the literature, for example based on a knee in the nearest neighbor\ndistances plot (as discussed in the references below).\n\nIn the figure below, the color indicates cluster membership, with large circles\nindicating core samples found by the algorithm. Smaller circles are non-core\nsamples that are still part of a cluster. Moreover, the outliers are indicated\nby black points below.\n\n.. |dbscan_results| image:: ../auto_examples/cluster/images/sphx_glr_plot_dbscan_002.png\n    :target: ../auto_examples/cluster/plot_dbscan.html\n    :scale: 50\n\n.. centered:: |dbscan_results|\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_cluster_plot_dbscan.py`\n\n.. dropdown:: Implementation\n\n  The DBSCAN algorithm is deterministic, always generating the same clusters when\n  given the same data in the same order.  However, the results can differ when\n  data is provided in a different order. First, even though the core samples will\n  always be assigned to the same clusters, the labels of those clusters will\n  depend on the order in which those samples are encountered in the data. Second\n  and more importantly, the clusters to which non-core samples are assigned can\n  differ depending on the data order.  This would happen when a non-core sample\n  has a distance lower than ``eps`` to two core samples in different clusters. By\n  the triangular inequality, those two core samples must be more distant than\n  ``eps`` from each other, or they would be in the same cluster. The non-core\n  sample is assigned to whichever cluster is generated first in a pass through the\n  data, and so the results will depend on the data ordering.\n\n  The current implementation uses ball trees and kd-trees to determine the\n  neighborhood of points, which avoids calculating the full distance matrix (as\n  was done in scikit-learn versions before 0.14). The possibility to use custom\n  metrics is retained; for details, see :class:`NearestNeighbors`.\n\n.. dropdown:: Memory consumption for large sample sizes\n\n  This implementation is by default not memory efficient because it constructs a\n  full pairwise similarity matrix in the case where kd-trees or ball-trees cannot\n  be used (e.g., with sparse matrices). This matrix will consume :math:`n^2`\n  floats. A couple of mechanisms for getting around this are:\n\n  - Use :ref:`OPTICS <optics>` clustering in conjunction with the `extract_dbscan`\n    method. OPTICS clustering also calculates the full pairwise matrix, but only\n    keeps one row in memory at a time (memory complexity n).\n\n  - A sparse radius neighborhood graph (where missing entries are presumed to be\n    out of eps) can be precomputed in a memory-efficient way and dbscan can be run\n    over this with ``metric='precomputed'``.  See\n    :meth:`sklearn.neighbors.NearestNeighbors.radius_neighbors_graph`.\n\n  - The dataset can be compressed, either by removing exact duplicates if these\n    occur in your data, or by using BIRCH. Then you only have a relatively small\n    number of representatives for a large number of points. You can then provide a\n    ``sample_weight`` when fitting DBSCAN.\n\n.. dropdown:: References\n\n* `A Density-Based Algorithm for Discovering Clusters in Large Spatial\n  Databases with Noise <https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf>`_\n  Ester, M., H. P. Kriegel, J. Sander, and X. Xu, In Proceedings of the 2nd\n  International Conference on Knowledge Discovery and Data Mining, Portland, OR,\n  AAAI Press, pp. 226-231. 1996\n\n* :doi:`DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.\n  <10.1145/3068335>` Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu,\n  X. (2017). In ACM Transactions on Database Systems (TODS), 42(3), 19.\n\n\n.. _hdbscan:"
  },
  {
    "filename": "clustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\clustering.rst.txt",
    "id": "clustering.rst.txt_chunk_18",
    "header": "HDBSCAN",
    "text": "HDBSCAN\n=======\n\nThe :class:`HDBSCAN` algorithm can be seen as an extension of :class:`DBSCAN`\nand :class:`OPTICS`. Specifically, :class:`DBSCAN` assumes that the clustering\ncriterion (i.e. density requirement) is *globally homogeneous*.\nIn other words, :class:`DBSCAN` may struggle to successfully capture clusters\nwith different densities.\n:class:`HDBSCAN` alleviates this assumption and explores all possible density\nscales by building an alternative representation of the clustering problem.\n\n.. note::\n\n  This implementation is adapted from the original implementation of HDBSCAN,\n  `scikit-learn-contrib/hdbscan <https://github.com/scikit-learn-contrib/hdbscan>`_ based on [LJ2017]_.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_cluster_plot_hdbscan.py`"
  },
  {
    "filename": "clustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\clustering.rst.txt",
    "id": "clustering.rst.txt_chunk_19",
    "header": "Mutual Reachability Graph",
    "text": "Mutual Reachability Graph\n-------------------------\n\nHDBSCAN first defines :math:`d_c(x_p)`, the *core distance* of a sample :math:`x_p`, as the\ndistance to its `min_samples` th-nearest neighbor, counting itself. For example,\nif `min_samples=5` and :math:`x_*` is the 5th-nearest neighbor of :math:`x_p`\nthen the core distance is:\n\n.. math:: d_c(x_p)=d(x_p, x_*).\n\nNext it defines :math:`d_m(x_p, x_q)`, the *mutual reachability distance* of two points\n:math:`x_p, x_q`, as:\n\n.. math:: d_m(x_p, x_q) = \\max\\{d_c(x_p), d_c(x_q), d(x_p, x_q)\\}\n\nThese two notions allow us to construct the *mutual reachability graph*\n:math:`G_{ms}` defined for a fixed choice of `min_samples` by associating each\nsample :math:`x_p` with a vertex of the graph, and thus edges between points\n:math:`x_p, x_q` are the mutual reachability distance :math:`d_m(x_p, x_q)`\nbetween them. We may build subsets of this graph, denoted as\n:math:`G_{ms,\\varepsilon}`, by removing any edges with value greater than :math:`\\varepsilon`:\nfrom the original graph. Any points whose core distance is less than :math:`\\varepsilon`:\nare at this staged marked as noise. The remaining points are then clustered by\nfinding the connected components of this trimmed graph.\n\n.. note::\n\n  Taking the connected components of a trimmed graph :math:`G_{ms,\\varepsilon}` is\n  equivalent to running DBSCAN* with `min_samples` and :math:`\\varepsilon`. DBSCAN* is a\n  slightly modified version of DBSCAN mentioned in [CM2013]_."
  },
  {
    "filename": "clustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\clustering.rst.txt",
    "id": "clustering.rst.txt_chunk_20",
    "header": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\n-----------------------\nHDBSCAN can be seen as an algorithm which performs DBSCAN* clustering across all\nvalues of :math:`\\varepsilon`. As mentioned prior, this is equivalent to finding the connected\ncomponents of the mutual reachability graphs for all values of :math:`\\varepsilon`. To do this\nefficiently, HDBSCAN first extracts a minimum spanning tree (MST) from the fully\n-connected mutual reachability graph, then greedily cuts the edges with highest\nweight. An outline of the HDBSCAN algorithm is as follows:\n\n1. Extract the MST of :math:`G_{ms}`.\n2. Extend the MST by adding a \"self edge\" for each vertex, with weight equal\n   to the core distance of the underlying sample.\n3. Initialize a single cluster and label for the MST.\n4. Remove the edge with the greatest weight from the MST (ties are\n   removed simultaneously).\n5. Assign cluster labels to the connected components which contain the\n   end points of the now-removed edge. If the component does not have at least\n   one edge it is instead assigned a \"null\" label marking it as noise.\n6. Repeat 4-5 until there are no more connected components.\n\nHDBSCAN is therefore able to obtain all possible partitions achievable by\nDBSCAN* for a fixed choice of `min_samples` in a hierarchical fashion.\nIndeed, this allows HDBSCAN to perform clustering across multiple densities\nand as such it no longer needs :math:`\\varepsilon` to be given as a hyperparameter. Instead\nit relies solely on the choice of `min_samples`, which tends to be a more robust\nhyperparameter.\n\n.. |hdbscan_ground_truth| image:: ../auto_examples/cluster/images/sphx_glr_plot_hdbscan_005.png\n    :target: ../auto_examples/cluster/plot_hdbscan.html\n    :scale: 75\n.. |hdbscan_results| image:: ../auto_examples/cluster/images/sphx_glr_plot_hdbscan_007.png\n    :target: ../auto_examples/cluster/plot_hdbscan.html\n    :scale: 75\n\n.. centered:: |hdbscan_ground_truth|\n.. centered:: |hdbscan_results|\n\nHDBSCAN can be smoothed with an additional hyperparameter `min_cluster_size`\nwhich specifies that during the hierarchical clustering, components with fewer\nthan `minimum_cluster_size` many samples are considered noise. In practice, one\ncan set `minimum_cluster_size = min_samples` to couple the parameters and\nsimplify the hyperparameter space.\n\n.. rubric:: References\n\n.. [CM2013] Campello, R.J.G.B., Moulavi, D., Sander, J. (2013). Density-Based\n  Clustering Based on Hierarchical Density Estimates. In: Pei, J., Tseng, V.S.,\n  Cao, L., Motoda, H., Xu, G. (eds) Advances in Knowledge Discovery and Data\n  Mining. PAKDD 2013. Lecture Notes in Computer Science(), vol 7819. Springer,\n  Berlin, Heidelberg. :doi:`Density-Based Clustering Based on Hierarchical\n  Density Estimates <10.1007/978-3-642-37456-2_14>`\n\n.. [LJ2017] L. McInnes and J. Healy, (2017). Accelerated Hierarchical Density\n  Based Clustering. In: IEEE International Conference on Data Mining Workshops\n  (ICDMW), 2017, pp. 33-42. :doi:`Accelerated Hierarchical Density Based\n  Clustering <10.1109/ICDMW.2017.12>`\n\n.. _optics:"
  },
  {
    "filename": "clustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\clustering.rst.txt",
    "id": "clustering.rst.txt_chunk_21",
    "header": "OPTICS",
    "text": "OPTICS\n======\n\nThe :class:`OPTICS` algorithm shares many similarities with the :class:`DBSCAN`\nalgorithm, and can be considered a generalization of DBSCAN that relaxes the\n``eps`` requirement from a single value to a value range. The key difference\nbetween DBSCAN and OPTICS is that the OPTICS algorithm builds a *reachability*\ngraph, which assigns each sample both a ``reachability_`` distance, and a spot\nwithin the cluster ``ordering_`` attribute; these two attributes are assigned\nwhen the model is fitted, and are used to determine cluster membership. If\nOPTICS is run with the default value of *inf* set for ``max_eps``, then DBSCAN\nstyle cluster extraction can be performed repeatedly in linear time for any\ngiven ``eps`` value using the ``cluster_optics_dbscan`` method. Setting\n``max_eps`` to a lower value will result in shorter run times, and can be\nthought of as the maximum neighborhood radius from each point to find other\npotential reachable points.\n\n.. |optics_results| image:: ../auto_examples/cluster/images/sphx_glr_plot_optics_001.png\n        :target: ../auto_examples/cluster/plot_optics.html\n        :scale: 50\n\n.. centered:: |optics_results|\n\nThe *reachability* distances generated by OPTICS allow for variable density\nextraction of clusters within a single data set. As shown in the above plot,\ncombining *reachability* distances and data set ``ordering_`` produces a\n*reachability plot*, where point density is represented on the Y-axis, and\npoints are ordered such that nearby points are adjacent. 'Cutting' the\nreachability plot at a single value produces DBSCAN like results; all points\nabove the 'cut' are classified as noise, and each time that there is a break\nwhen reading from left to right signifies a new cluster. The default cluster\nextraction with OPTICS looks at the steep slopes within the graph to find\nclusters, and the user can define what counts as a steep slope using the\nparameter ``xi``. There are also other possibilities for analysis on the graph\nitself, such as generating hierarchical representations of the data through\nreachability-plot dendrograms, and the hierarchy of clusters detected by the\nalgorithm can be accessed through the ``cluster_hierarchy_`` parameter. The\nplot above has been color-coded so that cluster colors in planar space match\nthe linear segment clusters of the reachability plot. Note that the blue and\nred clusters are adjacent in the reachability plot, and can be hierarchically\nrepresented as children of a larger parent cluster.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_cluster_plot_optics.py`\n\n\n.. dropdown:: Comparison with DBSCAN\n\n  The results from OPTICS ``cluster_optics_dbscan`` method and DBSCAN are very\n  similar, but not always identical; specifically, labeling of periphery and noise\n  points. This is in part because the first samples of each dense area processed\n  by OPTICS have a large reachability value while being close to other points in\n  their area, and will thus sometimes be marked as noise rather than periphery.\n  This affects adjacent points when they are considered as candidates for being\n  marked as either periphery or noise.\n\n  Note that for any single value of ``eps``, DBSCAN will tend to have a shorter\n  run time than OPTICS; however, for repeated runs at varying ``eps`` values, a\n  single run of OPTICS may require less cumulative runtime than DBSCAN. It is also\n  important to note that OPTICS' output is close to DBSCAN's only if ``eps`` and\n  ``max_eps`` are close.\n\n.. dropdown:: Computational Complexity\n\n  Spatial indexing trees are used to avoid calculating the full distance matrix,\n  and allow for efficient memory usage on large sets of samples. Different\n  distance metrics can be supplied via the ``metric`` keyword.\n\n  For large datasets, similar (but not identical) results can be obtained via\n  :class:`HDBSCAN`. The HDBSCAN implementation is multithreaded, and has better\n  algorithmic runtime complexity than OPTICS, at the cost of worse memory scaling.\n  For extremely large datasets that exhaust system memory using HDBSCAN, OPTICS\n  will maintain :math:`n` (as opposed to :math:`n^2`) memory scaling; however,\n  tuning of the ``max_eps`` parameter will likely need to be used to give a\n  solution in a reasonable amount of wall time.\n\n\n.. dropdown:: References\n\n  * \"OPTICS: ordering points to identify the clustering structure.\" Ankerst,\n    Mihael, Markus M. Breunig, Hans-Peter Kriegel, and J\u00f6rg Sander. In ACM Sigmod\n    Record, vol. 28, no. 2, pp. 49-60. ACM, 1999.\n\n\n.. _birch:"
  },
  {
    "filename": "clustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\clustering.rst.txt",
    "id": "clustering.rst.txt_chunk_22",
    "header": "BIRCH",
    "text": "BIRCH\n=====\n\nThe :class:`Birch` builds a tree called the Clustering Feature Tree (CFT)\nfor the given data. The data is essentially lossy compressed to a set of\nClustering Feature nodes (CF Nodes). The CF Nodes have a number of\nsubclusters called Clustering Feature subclusters (CF Subclusters)\nand these CF Subclusters located in the non-terminal CF Nodes\ncan have CF Nodes as children.\n\nThe CF Subclusters hold the necessary information for clustering which prevents\nthe need to hold the entire input data in memory. This information includes:\n\n- Number of samples in a subcluster.\n- Linear Sum - An n-dimensional vector holding the sum of all samples\n- Squared Sum - Sum of the squared L2 norm of all samples.\n- Centroids - To avoid recalculation linear sum / n_samples.\n- Squared norm of the centroids.\n\nThe BIRCH algorithm has two parameters, the threshold and the branching factor.\nThe branching factor limits the number of subclusters in a node and the\nthreshold limits the distance between the entering sample and the existing\nsubclusters.\n\nThis algorithm can be viewed as an instance or data reduction method,\nsince it reduces the input data to a set of subclusters which are obtained directly\nfrom the leaves of the CFT. This reduced data can be further processed by feeding\nit into a global clusterer. This global clusterer can be set by ``n_clusters``.\nIf ``n_clusters`` is set to None, the subclusters from the leaves are directly\nread off, otherwise a global clustering step labels these subclusters into global\nclusters (labels) and the samples are mapped to the global label of the nearest subcluster.\n\n.. dropdown:: Algorithm description\n\n  - A new sample is inserted into the root of the CF Tree which is a CF Node. It\n    is then merged with the subcluster of the root, that has the smallest radius\n    after merging, constrained by the threshold and branching factor conditions.\n    If the subcluster has any child node, then this is done repeatedly till it\n    reaches a leaf. After finding the nearest subcluster in the leaf, the\n    properties of this subcluster and the parent subclusters are recursively\n    updated.\n\n  - If the radius of the subcluster obtained by merging the new sample and the\n    nearest subcluster is greater than the square of the threshold and if the\n    number of subclusters is greater than the branching factor, then a space is\n    temporarily allocated to this new sample. The two farthest subclusters are\n    taken and the subclusters are divided into two groups on the basis of the\n    distance between these subclusters.\n\n  - If this split node has a parent subcluster and there is room for a new\n    subcluster, then the parent is split into two. If there is no room, then this\n    node is again split into two and the process is continued recursively, till it\n    reaches the root.\n\n.. dropdown:: BIRCH or MiniBatchKMeans?\n\n  - BIRCH does not scale very well to high dimensional data. As a rule of thumb if\n    ``n_features`` is greater than twenty, it is generally better to use MiniBatchKMeans.\n  - If the number of instances of data needs to be reduced, or if one wants a\n    large number of subclusters either as a preprocessing step or otherwise,\n    BIRCH is more useful than MiniBatchKMeans.\n\n  .. image:: ../auto_examples/cluster/images/sphx_glr_plot_birch_vs_minibatchkmeans_001.png\n    :target: ../auto_examples/cluster/plot_birch_vs_minibatchkmeans.html\n\n.. dropdown:: How to use partial_fit?\n\n  To avoid the computation of global clustering, for every call of ``partial_fit``\n  the user is advised:\n\n  1. To set ``n_clusters=None`` initially.\n  2. Train all data by multiple calls to partial_fit.\n  3. Set ``n_clusters`` to a required value using\n     ``brc.set_params(n_clusters=n_clusters)``.\n  4. Call ``partial_fit`` finally with no arguments, i.e. ``brc.partial_fit()``\n     which performs the global clustering.\n\n.. dropdown:: References\n\n  * Tian Zhang, Raghu Ramakrishnan, Maron Livny BIRCH: An efficient data\n    clustering method for large databases.\n    https://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf\n\n  * Roberto Perdisci JBirch - Java implementation of BIRCH clustering algorithm\n    https://code.google.com/archive/p/jbirch\n\n\n\n.. _clustering_evaluation:"
  },
  {
    "filename": "clustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\clustering.rst.txt",
    "id": "clustering.rst.txt_chunk_23",
    "header": "Clustering performance evaluation",
    "text": "Clustering performance evaluation\n=================================\n\nEvaluating the performance of a clustering algorithm is not as trivial as\ncounting the number of errors or the precision and recall of a supervised\nclassification algorithm. In particular any evaluation metric should not\ntake the absolute values of the cluster labels into account but rather\nif this clustering define separations of the data similar to some ground\ntruth set of classes or satisfying some assumption such that members\nbelong to the same class are more similar than members of different\nclasses according to some similarity metric.\n\n.. currentmodule:: sklearn.metrics\n\n.. _rand_score:\n.. _adjusted_rand_score:"
  },
  {
    "filename": "clustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\clustering.rst.txt",
    "id": "clustering.rst.txt_chunk_24",
    "header": "Rand index",
    "text": "Rand index\n----------\n\nGiven the knowledge of the ground truth class assignments\n``labels_true`` and our clustering algorithm assignments of the same\nsamples ``labels_pred``, the **(adjusted or unadjusted) Rand index**\nis a function that measures the **similarity** of the two assignments,\nignoring permutations::\n\n  >>> from sklearn import metrics\n  >>> labels_true = [0, 0, 0, 1, 1, 1]\n  >>> labels_pred = [0, 0, 1, 1, 2, 2]\n  >>> metrics.rand_score(labels_true, labels_pred)\n  0.66\n\nThe Rand index does not ensure to obtain a value close to 0.0 for a\nrandom labelling. The adjusted Rand index **corrects for chance** and\nwill give such a baseline.\n\n  >>> metrics.adjusted_rand_score(labels_true, labels_pred)\n  0.24\n\nAs with all clustering metrics, one can permute 0 and 1 in the predicted\nlabels, rename 2 to 3, and get the same score::\n\n  >>> labels_pred = [1, 1, 0, 0, 3, 3]\n  >>> metrics.rand_score(labels_true, labels_pred)\n  0.66\n  >>> metrics.adjusted_rand_score(labels_true, labels_pred)\n  0.24\n\nFurthermore, both :func:`rand_score` and :func:`adjusted_rand_score` are\n**symmetric**: swapping the argument does not change the scores. They can\nthus be used as **consensus measures**::\n\n  >>> metrics.rand_score(labels_pred, labels_true)\n  0.66\n  >>> metrics.adjusted_rand_score(labels_pred, labels_true)\n  0.24\n\nPerfect labeling is scored 1.0::\n\n  >>> labels_pred = labels_true[:]\n  >>> metrics.rand_score(labels_true, labels_pred)\n  1.0\n  >>> metrics.adjusted_rand_score(labels_true, labels_pred)\n  1.0\n\nPoorly agreeing labels (e.g. independent labelings) have lower scores,\nand for the adjusted Rand index the score will be negative or close to\nzero. However, for the unadjusted Rand index the score, while lower,\nwill not necessarily be close to zero::\n\n  >>> labels_true = [0, 0, 0, 0, 0, 0, 1, 1]\n  >>> labels_pred = [0, 1, 2, 3, 4, 5, 5, 6]\n  >>> metrics.rand_score(labels_true, labels_pred)\n  0.39\n  >>> metrics.adjusted_rand_score(labels_true, labels_pred)\n  -0.072\n\n\n.. topic:: Advantages:\n\n  - **Interpretability**: The unadjusted Rand index is proportional to the\n    number of sample pairs whose labels are the same in both `labels_pred` and\n    `labels_true`, or are different in both.\n\n  - **Random (uniform) label assignments have an adjusted Rand index score close\n    to 0.0** for any value of ``n_clusters`` and ``n_samples`` (which is not the\n    case for the unadjusted Rand index or the V-measure for instance).\n\n  - **Bounded range**: Lower values indicate different labelings, similar\n    clusterings have a high (adjusted or unadjusted) Rand index, 1.0 is the\n    perfect match score. The score range is [0, 1] for the unadjusted Rand index\n    and [-0.5, 1] for the adjusted Rand index.\n\n  - **No assumption is made on the cluster structure**: The (adjusted or\n    unadjusted) Rand index can be used to compare all kinds of clustering\n    algorithms, and can be used to compare clustering algorithms such as k-means\n    which assumes isotropic blob shapes with results of spectral clustering\n    algorithms which can find cluster with \"folded\" shapes.\n\n.. topic:: Drawbacks:\n\n  - Contrary to inertia, the **(adjusted or unadjusted) Rand index requires\n    knowledge of the ground truth classes** which is almost never available in\n    practice or requires manual assignment by human annotators (as in the\n    supervised learning setting).\n\n    However (adjusted or unadjusted) Rand index can also be useful in a purely\n    unsupervised setting as a building block for a Consensus Index that can be\n    used for clustering model selection (TODO).\n\n  - The **unadjusted Rand index is often close to 1.0** even if the clusterings\n    themselves differ significantly. This can be understood when interpreting\n    the Rand index as the accuracy of element pair labeling resulting from the\n    clusterings: In practice there often is a majority of element pairs that are\n    assigned the ``different`` pair label under both the predicted and the\n    ground truth clustering resulting in a high proportion of pair labels that\n    agree, which leads subsequently to a high score.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_cluster_plot_adjusted_for_chance_measures.py`:\n  Analysis of the impact of the dataset size on the value of\n  clustering measures for random assignments.\n\n.. dropdown:: Mathematical formulation\n\n  If C is a ground truth class assignment and K the clustering, let us define\n  :math:`a` and :math:`b` as:\n\n  - :math:`a`, the number of pairs of elements that are in the same set in C and\n    in the same set in K\n\n  - :math:`b`, the number of pairs of elements that are in different sets in C and\n    in different sets in K\n\n  The unadjusted Rand index is then given by:\n\n  .. math:: \\text{RI} = \\frac{a + b}{C_2^{n_{samples}}}\n\n  where :math:`C_2^{n_{samples}}` is the total number of possible pairs in the\n  dataset. It does not matter if the calculation is performed on ordered pairs or\n  unordered pairs as long as the calculation is performed consistently.\n\n  However, the Rand index does not guarantee that random label assignments will\n  get a value close to zero (esp. if the number of clusters is in the same order\n  of magnitude as the number of samples).\n\n  To counter this effect we can discount the expected RI :math:`E[\\text{RI}]` of\n  random labelings by defining the adjusted Rand index as follows:\n\n  .. math:: \\text{ARI} = \\frac{\\text{RI} - E[\\text{RI}]}{\\max(\\text{RI}) - E[\\text{RI}]}\n\n.. dropdown:: References\n\n  * `Comparing Partitions\n    <https://link.springer.com/article/10.1007%2FBF01908075>`_ L. Hubert and P.\n    Arabie, Journal of Classification 1985\n\n  * `Properties of the Hubert-Arabie adjusted Rand index\n    <https://psycnet.apa.org/record/2004-17801-007>`_ D. Steinley, Psychological\n    Methods 2004\n\n  * `Wikipedia entry for the Rand index\n    <https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index>`_\n\n  * :doi:`Minimum adjusted Rand index for two clusterings of a given size, 2022, J. E. Chac\u00f3n and A. I. Rastrojo <10.1007/s11634-022-00491-w>`\n\n\n.. _mutual_info_score:"
  },
  {
    "filename": "clustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\clustering.rst.txt",
    "id": "clustering.rst.txt_chunk_25",
    "header": "Mutual Information based scores",
    "text": "Mutual Information based scores\n-------------------------------\n\nGiven the knowledge of the ground truth class assignments ``labels_true`` and\nour clustering algorithm assignments of the same samples ``labels_pred``, the\n**Mutual Information** is a function that measures the **agreement** of the two\nassignments, ignoring permutations.  Two different normalized versions of this\nmeasure are available, **Normalized Mutual Information (NMI)** and **Adjusted\nMutual Information (AMI)**. NMI is often used in the literature, while AMI was\nproposed more recently and is **normalized against chance**::\n\n  >>> from sklearn import metrics\n  >>> labels_true = [0, 0, 0, 1, 1, 1]\n  >>> labels_pred = [0, 0, 1, 1, 2, 2]\n\n  >>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP\n  0.22504\n\nOne can permute 0 and 1 in the predicted labels, rename 2 to 3 and get\nthe same score::\n\n  >>> labels_pred = [1, 1, 0, 0, 3, 3]\n  >>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP\n  0.22504\n\nAll, :func:`mutual_info_score`, :func:`adjusted_mutual_info_score` and\n:func:`normalized_mutual_info_score` are symmetric: swapping the argument does\nnot change the score. Thus they can be used as a **consensus measure**::\n\n  >>> metrics.adjusted_mutual_info_score(labels_pred, labels_true)  # doctest: +SKIP\n  0.22504\n\nPerfect labeling is scored 1.0::\n\n  >>> labels_pred = labels_true[:]\n  >>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP\n  1.0\n\n  >>> metrics.normalized_mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP\n  1.0\n\nThis is not true for ``mutual_info_score``, which is therefore harder to judge::\n\n  >>> metrics.mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP\n  0.69\n\nBad (e.g. independent labelings) have non-positive scores::\n\n  >>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1]\n  >>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]\n  >>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP\n  -0.10526\n\n\n.. topic:: Advantages:\n\n  - **Random (uniform) label assignments have a AMI score close to 0.0** for any\n    value of ``n_clusters`` and ``n_samples`` (which is not the case for raw\n    Mutual Information or the V-measure for instance).\n\n  - **Upper bound  of 1**:  Values close to zero indicate two label assignments\n    that are largely independent, while values close to one indicate significant\n    agreement. Further, an AMI of exactly 1 indicates that the two label\n    assignments are equal (with or without permutation).\n\n.. topic:: Drawbacks:\n\n  - Contrary to inertia, **MI-based measures require the knowledge of the ground\n    truth classes** while almost never available in practice or requires manual\n    assignment by human annotators (as in the supervised learning setting).\n\n    However MI-based measures can also be useful in purely unsupervised setting\n    as a building block for a Consensus Index that can be used for clustering\n    model selection.\n\n  - NMI and MI are not adjusted against chance.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_cluster_plot_adjusted_for_chance_measures.py`: Analysis\n  of the impact of the dataset size on the value of clustering measures for random\n  assignments. This example also includes the Adjusted Rand Index.\n\n.. dropdown:: Mathematical formulation\n\n  Assume two label assignments (of the same N objects), :math:`U` and :math:`V`.\n  Their entropy is the amount of uncertainty for a partition set, defined by:\n\n  .. math:: H(U) = - \\sum_{i=1}^{|U|}P(i)\\log(P(i))\n\n  where :math:`P(i) = |U_i| / N` is the probability that an object picked at\n  random from :math:`U` falls into class :math:`U_i`. Likewise for :math:`V`:\n\n  .. math:: H(V) = - \\sum_{j=1}^{|V|}P'(j)\\log(P'(j))\n\n  With :math:`P'(j) = |V_j| / N`. The mutual information (MI) between :math:`U`\n  and :math:`V` is calculated by:\n\n  .. math:: \\text{MI}(U, V) = \\sum_{i=1}^{|U|}\\sum_{j=1}^{|V|}P(i, j)\\log\\left(\\frac{P(i,j)}{P(i)P'(j)}\\right)\n\n  where :math:`P(i, j) = |U_i \\cap V_j| / N` is the probability that an object\n  picked at random falls into both classes :math:`U_i` and :math:`V_j`.\n\n  It also can be expressed in set cardinality formulation:\n\n  .. math:: \\text{MI}(U, V) = \\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\frac{|U_i \\cap V_j|}{N}\\log\\left(\\frac{N|U_i \\cap V_j|}{|U_i||V_j|}\\right)\n\n  The normalized mutual information is defined as\n\n  .. math:: \\text{NMI}(U, V) = \\frac{\\text{MI}(U, V)}{\\text{mean}(H(U), H(V))}\n\n  This value of the mutual information and also the normalized variant is not\n  adjusted for chance and will tend to increase as the number of different labels\n  (clusters) increases, regardless of the actual amount of \"mutual information\"\n  between the label assignments.\n\n  The expected value for the mutual information can be calculated using the\n  following equation [VEB2009]_. In this equation, :math:`a_i = |U_i|` (the number\n  of elements in :math:`U_i`) and :math:`b_j = |V_j|` (the number of elements in\n  :math:`V_j`).\n\n  .. math:: E[\\text{MI}(U,V)]=\\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\sum_{n_{ij}=(a_i+b_j-N)^+\n    }^{\\min(a_i, b_j)} \\frac{n_{ij}}{N}\\log \\left( \\frac{ N.n_{ij}}{a_i b_j}\\right)\n    \\frac{a_i!b_j!(N-a_i)!(N-b_j)!}{N!n_{ij}!(a_i-n_{ij})!(b_j-n_{ij})!\n    (N-a_i-b_j+n_{ij})!}\n\n  Using the expected value, the adjusted mutual information can then be calculated\n  using a similar form to that of the adjusted Rand index:\n\n  .. math:: \\text{AMI} = \\frac{\\text{MI} - E[\\text{MI}]}{\\text{mean}(H(U), H(V)) - E[\\text{MI}]}\n\n  For normalized mutual information and adjusted mutual information, the\n  normalizing value is typically some *generalized* mean of the entropies of each\n  clustering. Various generalized means exist, and no firm rules exist for\n  preferring one over the others.  The decision is largely a field-by-field basis;\n  for instance, in community detection, the arithmetic mean is most common. Each\n  normalizing method provides \"qualitatively similar behaviours\" [YAT2016]_. In\n  our implementation, this is controlled by the ``average_method`` parameter.\n\n  Vinh et al. (2010) named variants of NMI and AMI by their averaging method\n  [VEB2010]_. Their 'sqrt' and 'sum' averages are the geometric and arithmetic\n  means; we use these more broadly common names.\n\n  .. rubric:: References\n\n  * Strehl, Alexander, and Joydeep Ghosh (2002). \"Cluster ensembles - a\n    knowledge reuse framework for combining multiple partitions\". Journal of\n    Machine Learning Research 3: 583-617. `doi:10.1162/153244303321897735\n    <http://strehl.com/download/strehl-jmlr02.pdf>`_.\n\n  * `Wikipedia entry for the (normalized) Mutual Information\n    <https://en.wikipedia.org/wiki/Mutual_Information>`_\n\n  * `Wikipedia entry for the Adjusted Mutual Information\n    <https://en.wikipedia.org/wiki/Adjusted_Mutual_Information>`_\n\n  .. [VEB2009] Vinh, Epps, and Bailey, (2009). \"Information theoretic measures\n    for clusterings comparison\". Proceedings of the 26th Annual International\n    Conference on Machine Learning - ICML '09. `doi:10.1145/1553374.1553511\n    <https://dl.acm.org/citation.cfm?doid=1553374.1553511>`_. ISBN\n    9781605585161.\n\n  .. [VEB2010] Vinh, Epps, and Bailey, (2010). \"Information Theoretic Measures\n    for Clusterings Comparison: Variants, Properties, Normalization and\n    Correction for Chance\". JMLR\n    <https://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf>\n\n  .. [YAT2016] Yang, Algesheimer, and Tessone, (2016). \"A comparative analysis\n    of community detection algorithms on artificial networks\". Scientific\n    Reports 6: 30750. `doi:10.1038/srep30750\n    <https://www.nature.com/articles/srep30750>`_.\n\n\n.. _homogeneity_completeness:"
  },
  {
    "filename": "clustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\clustering.rst.txt",
    "id": "clustering.rst.txt_chunk_26",
    "header": "Homogeneity, completeness and V-measure",
    "text": "Homogeneity, completeness and V-measure\n---------------------------------------\n\nGiven the knowledge of the ground truth class assignments of the samples,\nit is possible to define some intuitive metric using conditional entropy\nanalysis.\n\nIn particular Rosenberg and Hirschberg (2007) define the following two\ndesirable objectives for any cluster assignment:\n\n- **homogeneity**: each cluster contains only members of a single class.\n\n- **completeness**: all members of a given class are assigned to the same\n  cluster.\n\nWe can turn those concept as scores :func:`homogeneity_score` and\n:func:`completeness_score`. Both are bounded below by 0.0 and above by\n1.0 (higher is better)::\n\n  >>> from sklearn import metrics\n  >>> labels_true = [0, 0, 0, 1, 1, 1]\n  >>> labels_pred = [0, 0, 1, 1, 2, 2]\n\n  >>> metrics.homogeneity_score(labels_true, labels_pred)\n  0.66\n\n  >>> metrics.completeness_score(labels_true, labels_pred)\n  0.42\n\nTheir harmonic mean called **V-measure** is computed by\n:func:`v_measure_score`::\n\n  >>> metrics.v_measure_score(labels_true, labels_pred)\n  0.516\n\nThis function's formula is as follows:\n\n.. math:: v = \\frac{(1 + \\beta) \\times \\text{homogeneity} \\times \\text{completeness}}{(\\beta \\times \\text{homogeneity} + \\text{completeness})}\n\n`beta` defaults to a value of 1.0, but for using a value less than 1 for beta::\n\n  >>> metrics.v_measure_score(labels_true, labels_pred, beta=0.6)\n  0.547\n\nmore weight will be attributed to homogeneity, and using a value greater than 1::\n\n  >>> metrics.v_measure_score(labels_true, labels_pred, beta=1.8)\n  0.48\n\nmore weight will be attributed to completeness.\n\nThe V-measure is actually equivalent to the mutual information (NMI)\ndiscussed above, with the aggregation function being the arithmetic mean [B2011]_.\n\nHomogeneity, completeness and V-measure can be computed at once using\n:func:`homogeneity_completeness_v_measure` as follows::\n\n  >>> metrics.homogeneity_completeness_v_measure(labels_true, labels_pred)\n  (0.67, 0.42, 0.52)\n\nThe following clustering assignment is slightly better, since it is\nhomogeneous but not complete::\n\n  >>> labels_pred = [0, 0, 0, 1, 2, 2]\n  >>> metrics.homogeneity_completeness_v_measure(labels_true, labels_pred)\n  (1.0, 0.68, 0.81)\n\n.. note::\n\n  :func:`v_measure_score` is **symmetric**: it can be used to evaluate\n  the **agreement** of two independent assignments on the same dataset.\n\n  This is not the case for :func:`completeness_score` and\n  :func:`homogeneity_score`: both are bound by the relationship::\n\n    homogeneity_score(a, b) == completeness_score(b, a)\n\n\n.. topic:: Advantages:\n\n  - **Bounded scores**: 0.0 is as bad as it can be, 1.0 is a perfect score.\n\n  - Intuitive interpretation: clustering with bad V-measure can be\n    **qualitatively analyzed in terms of homogeneity and completeness** to\n    better feel what 'kind' of mistakes is done by the assignment.\n\n  - **No assumption is made on the cluster structure**: can be used to compare\n    clustering algorithms such as k-means which assumes isotropic blob shapes\n    with results of spectral clustering algorithms which can find cluster with\n    \"folded\" shapes.\n\n.. topic:: Drawbacks:\n\n  - The previously introduced metrics are **not normalized with regards to\n    random labeling**: this means that depending on the number of samples,\n    clusters and ground truth classes, a completely random labeling will not\n    always yield the same values for homogeneity, completeness and hence\n    v-measure. In particular **random labeling won't yield zero scores\n    especially when the number of clusters is large**.\n\n    This problem can safely be ignored when the number of samples is more than a\n    thousand and the number of clusters is less than 10. **For smaller sample\n    sizes or larger number of clusters it is safer to use an adjusted index such\n    as the Adjusted Rand Index (ARI)**.\n\n  .. figure:: ../auto_examples/cluster/images/sphx_glr_plot_adjusted_for_chance_measures_001.png\n    :target: ../auto_examples/cluster/plot_adjusted_for_chance_measures.html\n    :align: center\n    :scale: 100\n\n  - These metrics **require the knowledge of the ground truth classes** while\n    almost never available in practice or requires manual assignment by human\n    annotators (as in the supervised learning setting).\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_cluster_plot_adjusted_for_chance_measures.py`: Analysis\n  of the impact of the dataset size on the value of clustering measures for\n  random assignments.\n\n.. dropdown:: Mathematical formulation\n\n  Homogeneity and completeness scores are formally given by:\n\n  .. math:: h = 1 - \\frac{H(C|K)}{H(C)}\n\n  .. math:: c = 1 - \\frac{H(K|C)}{H(K)}\n\n  where :math:`H(C|K)` is the **conditional entropy of the classes given the\n  cluster assignments** and is given by:\n\n  .. math:: H(C|K) = - \\sum_{c=1}^{|C|} \\sum_{k=1}^{|K|} \\frac{n_{c,k}}{n}\n            \\cdot \\log\\left(\\frac{n_{c,k}}{n_k}\\right)\n\n  and :math:`H(C)` is the **entropy of the classes** and is given by:\n\n  .. math:: H(C) = - \\sum_{c=1}^{|C|} \\frac{n_c}{n} \\cdot \\log\\left(\\frac{n_c}{n}\\right)\n\n  with :math:`n` the total number of samples, :math:`n_c` and :math:`n_k` the\n  number of samples respectively belonging to class :math:`c` and cluster\n  :math:`k`, and finally :math:`n_{c,k}` the number of samples from class\n  :math:`c` assigned to cluster :math:`k`.\n\n  The **conditional entropy of clusters given class** :math:`H(K|C)` and the\n  **entropy of clusters** :math:`H(K)` are defined in a symmetric manner.\n\n  Rosenberg and Hirschberg further define **V-measure** as the **harmonic mean of\n  homogeneity and completeness**:\n\n  .. math:: v = 2 \\cdot \\frac{h \\cdot c}{h + c}\n\n.. rubric:: References\n\n* `V-Measure: A conditional entropy-based external cluster evaluation measure\n  <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_ Andrew Rosenberg and Julia\n  Hirschberg, 2007\n\n.. [B2011] `Identification and Characterization of Events in Social Media\n  <http://www.cs.columbia.edu/~hila/hila-thesis-distributed.pdf>`_, Hila\n  Becker, PhD Thesis.\n\n\n.. _fowlkes_mallows_scores:"
  },
  {
    "filename": "clustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\clustering.rst.txt",
    "id": "clustering.rst.txt_chunk_27",
    "header": "Fowlkes-Mallows scores",
    "text": "Fowlkes-Mallows scores\n----------------------\n\nThe original Fowlkes-Mallows index (FMI) was intended to measure the similarity\nbetween two clustering results, which is inherently an unsupervised comparison.\nThe supervised adaptation of the Fowlkes-Mallows index\n(as implemented in :func:`sklearn.metrics.fowlkes_mallows_score`) can be used\nwhen the ground truth class assignments of the samples are known.\nThe FMI is defined as the geometric mean of the pairwise precision and recall:\n\n.. math:: \\text{FMI} = \\frac{\\text{TP}}{\\sqrt{(\\text{TP} + \\text{FP}) (\\text{TP} + \\text{FN})}}\n\nIn the above formula:\n\n* ``TP`` (**True Positive**): The number of pairs of points that are clustered together\n  both in the true labels and in the predicted labels.\n\n* ``FP`` (**False Positive**): The number of pairs of points that are clustered together\n  in the predicted labels but not in the true labels.\n\n* ``FN`` (**False Negative**): The number of pairs of points that are clustered together\n  in the true labels but not in the predicted labels.\n\nThe score ranges from 0 to 1. A high value indicates a good similarity\nbetween two clusters.\n\n  >>> from sklearn import metrics\n  >>> labels_true = [0, 0, 0, 1, 1, 1]\n  >>> labels_pred = [0, 0, 1, 1, 2, 2]\n\n  >>> metrics.fowlkes_mallows_score(labels_true, labels_pred)\n  0.47140\n\nOne can permute 0 and 1 in the predicted labels, rename 2 to 3 and get\nthe same score::\n\n  >>> labels_pred = [1, 1, 0, 0, 3, 3]\n\n  >>> metrics.fowlkes_mallows_score(labels_true, labels_pred)\n  0.47140\n\nPerfect labeling is scored 1.0::\n\n  >>> labels_pred = labels_true[:]\n  >>> metrics.fowlkes_mallows_score(labels_true, labels_pred)\n  1.0\n\nBad (e.g. independent labelings) have zero scores::\n\n  >>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1]\n  >>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]\n  >>> metrics.fowlkes_mallows_score(labels_true, labels_pred)\n  0.0\n\n.. topic:: Advantages:\n\n  - **Random (uniform) label assignments have a FMI score close to 0.0** for any\n    value of ``n_clusters`` and ``n_samples`` (which is not the case for raw\n    Mutual Information or the V-measure for instance).\n\n  - **Upper-bounded at 1**:  Values close to zero indicate two label assignments\n    that are largely independent, while values close to one indicate significant\n    agreement. Further, values of exactly 0 indicate **purely** independent\n    label assignments and a FMI of exactly 1 indicates that the two label\n    assignments are equal (with or without permutation).\n\n  - **No assumption is made on the cluster structure**: can be used to compare\n    clustering algorithms such as k-means which assumes isotropic blob shapes\n    with results of spectral clustering algorithms which can find cluster with\n    \"folded\" shapes.\n\n.. topic:: Drawbacks:\n\n  - Contrary to inertia, **FMI-based measures require the knowledge of the\n    ground truth classes** while almost never available in practice or requires\n    manual assignment by human annotators (as in the supervised learning\n    setting).\n\n.. dropdown:: References\n\n  * E. B. Fowkles and C. L. Mallows, 1983. \"A method for comparing two\n    hierarchical clusterings\". Journal of the American Statistical Association.\n    https://www.tandfonline.com/doi/abs/10.1080/01621459.1983.10478008\n\n  * `Wikipedia entry for the Fowlkes-Mallows Index\n    <https://en.wikipedia.org/wiki/Fowlkes-Mallows_index>`_\n\n\n.. _silhouette_coefficient:"
  },
  {
    "filename": "clustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\clustering.rst.txt",
    "id": "clustering.rst.txt_chunk_28",
    "header": "Silhouette Coefficient",
    "text": "Silhouette Coefficient\n----------------------\n\nIf the ground truth labels are not known, evaluation must be performed using\nthe model itself. The Silhouette Coefficient\n(:func:`sklearn.metrics.silhouette_score`)\nis an example of such an evaluation, where a\nhigher Silhouette Coefficient score relates to a model with better defined\nclusters. The Silhouette Coefficient is defined for each sample and is composed\nof two scores:\n\n- **a**: The mean distance between a sample and all other points in the same\n  class.\n\n- **b**: The mean distance between a sample and all other points in the *next\n  nearest cluster*.\n\nThe Silhouette Coefficient *s* for a single sample is then given as:\n\n.. math:: s = \\frac{b - a}{max(a, b)}\n\nThe Silhouette Coefficient for a set of samples is given as the mean of the\nSilhouette Coefficient for each sample.\n\n\n  >>> from sklearn import metrics\n  >>> from sklearn.metrics import pairwise_distances\n  >>> from sklearn import datasets\n  >>> X, y = datasets.load_iris(return_X_y=True)\n\nIn normal usage, the Silhouette Coefficient is applied to the results of a\ncluster analysis.\n\n  >>> import numpy as np\n  >>> from sklearn.cluster import KMeans\n  >>> kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)\n  >>> labels = kmeans_model.labels_\n  >>> metrics.silhouette_score(X, labels, metric='euclidean')\n  0.55\n\n.. topic:: Advantages:\n\n  - The score is bounded between -1 for incorrect clustering and +1 for highly\n    dense clustering. Scores around zero indicate overlapping clusters.\n\n  - The score is higher when clusters are dense and well separated, which\n    relates to a standard concept of a cluster.\n\n.. topic:: Drawbacks:\n\n  - The Silhouette Coefficient is generally higher for convex clusters than\n    other concepts of clusters, such as density based clusters like those\n    obtained through DBSCAN.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_silhouette_analysis.py` : In\n  this example the silhouette analysis is used to choose an optimal value for\n  n_clusters.\n\n.. dropdown:: References\n\n  * Peter J. Rousseeuw (1987). :doi:`\"Silhouettes: a Graphical Aid to the\n    Interpretation and Validation of Cluster Analysis\"<10.1016/0377-0427(87)90125-7>`.\n    Computational and Applied Mathematics 20: 53-65.\n\n\n.. _calinski_harabasz_index:"
  },
  {
    "filename": "clustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\clustering.rst.txt",
    "id": "clustering.rst.txt_chunk_29",
    "header": "Calinski-Harabasz Index",
    "text": "Calinski-Harabasz Index\n-----------------------\n\n\nIf the ground truth labels are not known, the Calinski-Harabasz index\n(:func:`sklearn.metrics.calinski_harabasz_score`) - also known as the Variance\nRatio Criterion - can be used to evaluate the model, where a higher\nCalinski-Harabasz score relates to a model with better defined clusters.\n\nThe index is the ratio of the sum of between-clusters dispersion and of\nwithin-cluster dispersion for all clusters (where dispersion is defined as the\nsum of distances squared):\n\n  >>> from sklearn import metrics\n  >>> from sklearn.metrics import pairwise_distances\n  >>> from sklearn import datasets\n  >>> X, y = datasets.load_iris(return_X_y=True)\n\nIn normal usage, the Calinski-Harabasz index is applied to the results of a\ncluster analysis:\n\n  >>> import numpy as np\n  >>> from sklearn.cluster import KMeans\n  >>> kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)\n  >>> labels = kmeans_model.labels_\n  >>> metrics.calinski_harabasz_score(X, labels)\n  561.59\n\n\n.. topic:: Advantages:\n\n  - The score is higher when clusters are dense and well separated, which\n    relates to a standard concept of a cluster.\n\n  - The score is fast to compute.\n\n.. topic:: Drawbacks:\n\n  - The Calinski-Harabasz index is generally higher for convex clusters than\n    other concepts of clusters, such as density based clusters like those\n    obtained through DBSCAN.\n\n.. dropdown:: Mathematical formulation\n\n  For a set of data :math:`E` of size :math:`n_E` which has been clustered into\n  :math:`k` clusters, the Calinski-Harabasz score :math:`s` is defined as the\n  ratio of the between-clusters dispersion mean and the within-cluster\n  dispersion:\n\n  .. math::\n    s = \\frac{\\mathrm{tr}(B_k)}{\\mathrm{tr}(W_k)} \\times \\frac{n_E - k}{k - 1}\n\n  where :math:`\\mathrm{tr}(B_k)` is trace of the between group dispersion matrix\n  and :math:`\\mathrm{tr}(W_k)` is the trace of the within-cluster dispersion\n  matrix defined by:\n\n  .. math:: W_k = \\sum_{q=1}^k \\sum_{x \\in C_q} (x - c_q) (x - c_q)^T\n\n  .. math:: B_k = \\sum_{q=1}^k n_q (c_q - c_E) (c_q - c_E)^T\n\n  with :math:`C_q` the set of points in cluster :math:`q`, :math:`c_q` the\n  center of cluster :math:`q`, :math:`c_E` the center of :math:`E`, and\n  :math:`n_q` the number of points in cluster :math:`q`.\n\n.. dropdown:: References\n\n  * Cali\u0144ski, T., & Harabasz, J. (1974). `\"A Dendrite Method for Cluster Analysis\"\n    <https://www.researchgate.net/publication/233096619_A_Dendrite_Method_for_Cluster_Analysis>`_.\n    :doi:`Communications in Statistics-theory and Methods 3: 1-27\n    <10.1080/03610927408827101>`.\n\n\n.. _davies-bouldin_index:"
  },
  {
    "filename": "clustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\clustering.rst.txt",
    "id": "clustering.rst.txt_chunk_30",
    "header": "Davies-Bouldin Index",
    "text": "Davies-Bouldin Index\n--------------------\n\nIf the ground truth labels are not known, the Davies-Bouldin index\n(:func:`sklearn.metrics.davies_bouldin_score`) can be used to evaluate the\nmodel, where a lower Davies-Bouldin index relates to a model with better\nseparation between the clusters.\n\nThis index signifies the average 'similarity' between clusters, where the\nsimilarity is a measure that compares the distance between clusters with the\nsize of the clusters themselves.\n\nZero is the lowest possible score. Values closer to zero indicate a better\npartition.\n\nIn normal usage, the Davies-Bouldin index is applied to the results of a\ncluster analysis as follows:\n\n  >>> from sklearn import datasets\n  >>> iris = datasets.load_iris()\n  >>> X = iris.data\n  >>> from sklearn.cluster import KMeans\n  >>> from sklearn.metrics import davies_bouldin_score\n  >>> kmeans = KMeans(n_clusters=3, random_state=1).fit(X)\n  >>> labels = kmeans.labels_\n  >>> davies_bouldin_score(X, labels)\n  0.666\n\n\n.. topic:: Advantages:\n\n  - The computation of Davies-Bouldin is simpler than that of Silhouette scores.\n  - The index is solely based on quantities and features inherent to the dataset\n    as its computation only uses point-wise distances.\n\n.. topic:: Drawbacks:\n\n  - The Davies-Bouldin index is generally higher for convex clusters than other\n    concepts of clusters, such as density-based clusters like those\n    obtained from DBSCAN.\n  - The usage of centroid distance limits the distance metric to Euclidean\n    space.\n\n.. dropdown:: Mathematical formulation\n\n  The index is defined as the average similarity between each cluster :math:`C_i`\n  for :math:`i=1, ..., k` and its most similar one :math:`C_j`. In the context of\n  this index, similarity is defined as a measure :math:`R_{ij}` that trades off:\n\n  - :math:`s_i`, the average distance between each point of cluster :math:`i` and\n    the centroid of that cluster -- also known as cluster diameter.\n  - :math:`d_{ij}`, the distance between cluster centroids :math:`i` and\n    :math:`j`.\n\n  A simple choice to construct :math:`R_{ij}` so that it is nonnegative and\n  symmetric is:\n\n  .. math::\n    R_{ij} = \\frac{s_i + s_j}{d_{ij}}\n\n  Then the Davies-Bouldin index is defined as:\n\n  .. math::\n    DB = \\frac{1}{k} \\sum_{i=1}^k \\max_{i \\neq j} R_{ij}\n\n.. dropdown:: References\n\n  * Davies, David L.; Bouldin, Donald W. (1979). :doi:`\"A Cluster Separation\n    Measure\" <10.1109/TPAMI.1979.4766909>` IEEE Transactions on Pattern Analysis\n    and Machine Intelligence. PAMI-1 (2): 224-227.\n\n  * Halkidi, Maria; Batistakis, Yannis; Vazirgiannis, Michalis (2001). :doi:`\"On\n    Clustering Validation Techniques\" <10.1023/A:1012801612483>` Journal of\n    Intelligent Information Systems, 17(2-3), 107-145.\n\n  * `Wikipedia entry for Davies-Bouldin index\n    <https://en.wikipedia.org/wiki/Davies-Bouldin_index>`_.\n\n\n.. _contingency_matrix:"
  },
  {
    "filename": "clustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\clustering.rst.txt",
    "id": "clustering.rst.txt_chunk_31",
    "header": "Contingency Matrix",
    "text": "Contingency Matrix\n------------------\n\nContingency matrix (:func:`sklearn.metrics.cluster.contingency_matrix`)\nreports the intersection cardinality for every true/predicted cluster pair.\nThe contingency matrix provides sufficient statistics for all clustering\nmetrics where the samples are independent and identically distributed and\none doesn't need to account for some instances not being clustered.\n\nHere is an example::\n\n   >>> from sklearn.metrics.cluster import contingency_matrix\n   >>> x = [\"a\", \"a\", \"a\", \"b\", \"b\", \"b\"]\n   >>> y = [0, 0, 1, 1, 2, 2]\n   >>> contingency_matrix(x, y)\n   array([[2, 1, 0],\n          [0, 1, 2]])\n\nThe first row of the output array indicates that there are three samples whose\ntrue cluster is \"a\". Of them, two are in predicted cluster 0, one is in 1,\nand none is in 2. And the second row indicates that there are three samples\nwhose true cluster is \"b\". Of them, none is in predicted cluster 0, one is in\n1 and two are in 2.\n\nA :ref:`confusion matrix <confusion_matrix>` for classification is a square\ncontingency matrix where the order of rows and columns correspond to a list\nof classes.\n\n\n.. topic:: Advantages:\n\n  - Allows to examine the spread of each true cluster across predicted clusters\n    and vice versa.\n\n  - The contingency table calculated is typically utilized in the calculation of\n    a similarity statistic (like the others listed in this document) between the\n    two clusterings.\n\n.. topic:: Drawbacks:\n\n  - Contingency matrix is easy to interpret for a small number of clusters, but\n    becomes very hard to interpret for a large number of clusters.\n\n  - It doesn't give a single metric to use as an objective for clustering\n    optimisation.\n\n.. dropdown:: References\n\n  * `Wikipedia entry for contingency matrix\n    <https://en.wikipedia.org/wiki/Contingency_table>`_\n\n\n.. _pair_confusion_matrix:"
  },
  {
    "filename": "clustering.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\clustering.rst.txt",
    "id": "clustering.rst.txt_chunk_32",
    "header": "Pair Confusion Matrix",
    "text": "Pair Confusion Matrix\n---------------------\n\nThe pair confusion matrix\n(:func:`sklearn.metrics.cluster.pair_confusion_matrix`) is a 2x2\nsimilarity matrix\n\n.. math::\n   C = \\left[\\begin{matrix}\n   C_{00} & C_{01} \\\\\n   C_{10} & C_{11}\n   \\end{matrix}\\right]\n\nbetween two clusterings computed by considering all pairs of samples and\ncounting pairs that are assigned into the same or into different clusters\nunder the true and predicted clusterings.\n\nIt has the following entries:\n\n:math:`C_{00}` : number of pairs with both clusterings having the samples\nnot clustered together\n\n:math:`C_{10}` : number of pairs with the true label clustering having the\nsamples clustered together but the other clustering not having the samples\nclustered together\n\n:math:`C_{01}` : number of pairs with the true label clustering not having\nthe samples clustered together but the other clustering having the samples\nclustered together\n\n:math:`C_{11}` : number of pairs with both clusterings having the samples\nclustered together\n\nConsidering a pair of samples that is clustered together a positive pair,\nthen as in binary classification the count of true negatives is\n:math:`C_{00}`, false negatives is :math:`C_{10}`, true positives is\n:math:`C_{11}` and false positives is :math:`C_{01}`.\n\nPerfectly matching labelings have all non-zero entries on the\ndiagonal regardless of actual label values::\n\n   >>> from sklearn.metrics.cluster import pair_confusion_matrix\n   >>> pair_confusion_matrix([0, 0, 1, 1], [0, 0, 1, 1])\n   array([[8, 0],\n          [0, 4]])\n\n::\n\n   >>> pair_confusion_matrix([0, 0, 1, 1], [1, 1, 0, 0])\n   array([[8, 0],\n          [0, 4]])\n\nLabelings that assign all classes members to the same clusters\nare complete but may not always be pure, hence penalized, and\nhave some off-diagonal non-zero entries::\n\n   >>> pair_confusion_matrix([0, 0, 1, 2], [0, 0, 1, 1])\n   array([[8, 2],\n          [0, 2]])\n\nThe matrix is not symmetric::\n\n   >>> pair_confusion_matrix([0, 0, 1, 1], [0, 0, 1, 2])\n   array([[8, 0],\n          [2, 2]])\n\nIf classes members are completely split across different clusters, the\nassignment is totally incomplete, hence the matrix has all zero\ndiagonal entries::\n\n   >>> pair_confusion_matrix([0, 0, 0, 0], [0, 1, 2, 3])\n   array([[ 0,  0],\n          [12,  0]])\n\n.. dropdown:: References\n\n  * :doi:`\"Comparing Partitions\" <10.1007/BF01908075>` L. Hubert and P. Arabie,\n    Journal of Classification 1985"
  },
  {
    "filename": "compose.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\compose.rst.txt",
    "id": "compose.rst.txt_chunk_0",
    "header": "",
    "text": ".. _combining_estimators:\n\n=================================="
  },
  {
    "filename": "compose.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\compose.rst.txt",
    "id": "compose.rst.txt_chunk_1",
    "header": "Pipelines and composite estimators",
    "text": "Pipelines and composite estimators\n==================================\n\nTo build a composite estimator, transformers are usually combined with other\ntransformers or with :term:`predictors` (such as classifiers or regressors).\nThe most common tool used for composing estimators is a :ref:`Pipeline\n<pipeline>`. Pipelines require all steps except the last to be a\n:term:`transformer`. The last step can be anything, a transformer, a\n:term:`predictor`, or a clustering estimator which might have or not have a\n`.predict(...)` method. A pipeline exposes all methods provided by the last\nestimator: if the last step provides a `transform` method, then the pipeline\nwould have a `transform` method and behave like a transformer. If the last step\nprovides a `predict` method, then the pipeline would expose that method, and\ngiven a data :term:`X`, use all steps except the last to transform the data,\nand then give that transformed data to the `predict` method of the last step of\nthe pipeline. The class :class:`Pipeline` is often used in combination with\n:ref:`ColumnTransformer <column_transformer>` or\n:ref:`FeatureUnion <feature_union>` which concatenate the output of transformers\ninto a composite feature space.\n:ref:`TransformedTargetRegressor <transformed_target_regressor>`\ndeals with transforming the :term:`target` (i.e. log-transform :term:`y`).\n\n.. _pipeline:"
  },
  {
    "filename": "compose.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\compose.rst.txt",
    "id": "compose.rst.txt_chunk_2",
    "header": "Pipeline: chaining estimators",
    "text": "Pipeline: chaining estimators\n=============================\n\n.. currentmodule:: sklearn.pipeline\n\n:class:`Pipeline` can be used to chain multiple estimators\ninto one. This is useful as there is often a fixed sequence\nof steps in processing the data, for example feature selection, normalization\nand classification. :class:`Pipeline` serves multiple purposes here:\n\nConvenience and encapsulation\n    You only have to call :term:`fit` and :term:`predict` once on your\n    data to fit a whole sequence of estimators.\nJoint parameter selection\n    You can :ref:`grid search <grid_search>`\n    over parameters of all estimators in the pipeline at once.\nSafety\n    Pipelines help avoid leaking statistics from your test data into the\n    trained model in cross-validation, by ensuring that the same samples are\n    used to train the transformers and predictors.\n\nAll estimators in a pipeline, except the last one, must be transformers\n(i.e. must have a :term:`transform` method).\nThe last estimator may be any type (transformer, classifier, etc.).\n\n.. note::\n\n    Calling ``fit`` on the pipeline is the same as calling ``fit`` on\n    each estimator in turn, ``transform`` the input and pass it on to the next step.\n    The pipeline has all the methods that the last estimator in the pipeline has,\n    i.e. if the last estimator is a classifier, the :class:`Pipeline` can be used\n    as a classifier. If the last estimator is a transformer, again, so is the\n    pipeline."
  },
  {
    "filename": "compose.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\compose.rst.txt",
    "id": "compose.rst.txt_chunk_3",
    "header": "Usage",
    "text": "Usage\n-----\n\nBuild a pipeline\n................\n\nThe :class:`Pipeline` is built using a list of ``(key, value)`` pairs, where\nthe ``key`` is a string containing the name you want to give this step and ``value``\nis an estimator object::\n\n    >>> from sklearn.pipeline import Pipeline\n    >>> from sklearn.svm import SVC\n    >>> from sklearn.decomposition import PCA\n    >>> estimators = [('reduce_dim', PCA()), ('clf', SVC())]\n    >>> pipe = Pipeline(estimators)\n    >>> pipe\n    Pipeline(steps=[('reduce_dim', PCA()), ('clf', SVC())])\n\n.. dropdown:: Shorthand version using :func:`make_pipeline`\n\n  The utility function :func:`make_pipeline` is a shorthand\n  for constructing pipelines;\n  it takes a variable number of estimators and returns a pipeline,\n  filling in the names automatically::\n\n      >>> from sklearn.pipeline import make_pipeline\n      >>> make_pipeline(PCA(), SVC())\n      Pipeline(steps=[('pca', PCA()), ('svc', SVC())])\n\nAccess pipeline steps\n.....................\n\nThe estimators of a pipeline are stored as a list in the ``steps`` attribute.\nA sub-pipeline can be extracted using the slicing notation commonly used\nfor Python Sequences such as lists or strings (although only a step of 1 is\npermitted). This is convenient for performing only some of the transformations\n(or their inverse):\n\n    >>> pipe[:1]\n    Pipeline(steps=[('reduce_dim', PCA())])\n    >>> pipe[-1:]\n    Pipeline(steps=[('clf', SVC())])\n\n.. dropdown:: Accessing a step by name or position\n\n  A specific step can also be accessed by index or name by indexing (with ``[idx]``) the\n  pipeline::\n\n      >>> pipe.steps[0]\n      ('reduce_dim', PCA())\n      >>> pipe[0]\n      PCA()\n      >>> pipe['reduce_dim']\n      PCA()\n\n  `Pipeline`'s `named_steps` attribute allows accessing steps by name with tab\n  completion in interactive environments::\n\n      >>> pipe.named_steps.reduce_dim is pipe['reduce_dim']\n      True\n\nTracking feature names in a pipeline\n....................................\n\nTo enable model inspection, :class:`~sklearn.pipeline.Pipeline` has a\n``get_feature_names_out()`` method, just like all transformers. You can use\npipeline slicing to get the feature names going into each step::\n\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.feature_selection import SelectKBest\n    >>> iris = load_iris()\n    >>> pipe = Pipeline(steps=[\n    ...    ('select', SelectKBest(k=2)),\n    ...    ('clf', LogisticRegression())])\n    >>> pipe.fit(iris.data, iris.target)\n    Pipeline(steps=[('select', SelectKBest(...)), ('clf', LogisticRegression(...))])\n    >>> pipe[:-1].get_feature_names_out()\n    array(['x2', 'x3'], ...)\n\n.. dropdown:: Customize feature names\n\n  You can also provide custom feature names for the input data using\n  ``get_feature_names_out``::\n\n      >>> pipe[:-1].get_feature_names_out(iris.feature_names)\n      array(['petal length (cm)', 'petal width (cm)'], ...)\n\n.. _pipeline_nested_parameters:\n\nAccess to nested parameters\n...........................\n\nIt is common to adjust the parameters of an estimator within a pipeline. This parameter\nis therefore nested because it belongs to a particular sub-step. Parameters of the\nestimators in the pipeline are accessible using the ``<estimator>__<parameter>``\nsyntax::\n\n    >>> pipe = Pipeline(steps=[(\"reduce_dim\", PCA()), (\"clf\", SVC())])\n    >>> pipe.set_params(clf__C=10)\n    Pipeline(steps=[('reduce_dim', PCA()), ('clf', SVC(C=10))])\n\n.. dropdown:: When does it matter?\n\n  This is particularly important for doing grid searches::\n\n      >>> from sklearn.model_selection import GridSearchCV\n      >>> param_grid = dict(reduce_dim__n_components=[2, 5, 10],\n      ...                   clf__C=[0.1, 10, 100])\n      >>> grid_search = GridSearchCV(pipe, param_grid=param_grid)\n\n  Individual steps may also be replaced as parameters, and non-final steps may be\n  ignored by setting them to ``'passthrough'``::\n\n      >>> param_grid = dict(reduce_dim=['passthrough', PCA(5), PCA(10)],\n      ...                   clf=[SVC(), LogisticRegression()],\n      ...                   clf__C=[0.1, 10, 100])\n      >>> grid_search = GridSearchCV(pipe, param_grid=param_grid)\n\n  .. seealso::\n\n    * :ref:`composite_grid_search`\n\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_feature_selection_plot_feature_selection_pipeline.py`\n* :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_text_feature_extraction.py`\n* :ref:`sphx_glr_auto_examples_compose_plot_digits_pipe.py`\n* :ref:`sphx_glr_auto_examples_miscellaneous_plot_kernel_approximation.py`\n* :ref:`sphx_glr_auto_examples_svm_plot_svm_anova.py`\n* :ref:`sphx_glr_auto_examples_compose_plot_compare_reduction.py`\n* :ref:`sphx_glr_auto_examples_miscellaneous_plot_pipeline_display.py`\n\n\n.. _pipeline_cache:"
  },
  {
    "filename": "compose.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\compose.rst.txt",
    "id": "compose.rst.txt_chunk_4",
    "header": "Caching transformers: avoid repeated computation",
    "text": "Caching transformers: avoid repeated computation\n-------------------------------------------------\n\n.. currentmodule:: sklearn.pipeline\n\nFitting transformers may be computationally expensive. With its\n``memory`` parameter set, :class:`Pipeline` will cache each transformer\nafter calling ``fit``.\nThis feature is used to avoid computing the fit transformers within a pipeline\nif the parameters and input data are identical. A typical example is the case of\na grid search in which the transformers can be fitted only once and reused for\neach configuration. The last step will never be cached, even if it is a transformer.\n\nThe parameter ``memory`` is needed in order to cache the transformers.\n``memory`` can be either a string containing the directory where to cache the\ntransformers or a `joblib.Memory <https://joblib.readthedocs.io/en/latest/memory.html>`_\nobject::\n\n    >>> from tempfile import mkdtemp\n    >>> from shutil import rmtree\n    >>> from sklearn.decomposition import PCA\n    >>> from sklearn.svm import SVC\n    >>> from sklearn.pipeline import Pipeline\n    >>> estimators = [('reduce_dim', PCA()), ('clf', SVC())]\n    >>> cachedir = mkdtemp()\n    >>> pipe = Pipeline(estimators, memory=cachedir)\n    >>> pipe\n    Pipeline(memory=...,\n             steps=[('reduce_dim', PCA()), ('clf', SVC())])\n    >>> # Clear the cache directory when you don't need it anymore\n    >>> rmtree(cachedir)\n\n.. dropdown:: Side effect of caching transformers\n  :color: warning\n\n  Using a :class:`Pipeline` without cache enabled, it is possible to\n  inspect the original instance such as::\n\n      >>> from sklearn.datasets import load_digits\n      >>> X_digits, y_digits = load_digits(return_X_y=True)\n      >>> pca1 = PCA(n_components=10)\n      >>> svm1 = SVC()\n      >>> pipe = Pipeline([('reduce_dim', pca1), ('clf', svm1)])\n      >>> pipe.fit(X_digits, y_digits)\n      Pipeline(steps=[('reduce_dim', PCA(n_components=10)), ('clf', SVC())])\n      >>> # The pca instance can be inspected directly\n      >>> pca1.components_.shape\n      (10, 64)\n\n  Enabling caching triggers a clone of the transformers before fitting.\n  Therefore, the transformer instance given to the pipeline cannot be\n  inspected directly.\n  In the following example, accessing the :class:`~sklearn.decomposition.PCA`\n  instance ``pca2`` will raise an ``AttributeError`` since ``pca2`` will be an\n  unfitted transformer.\n  Instead, use the attribute ``named_steps`` to inspect estimators within\n  the pipeline::\n\n      >>> cachedir = mkdtemp()\n      >>> pca2 = PCA(n_components=10)\n      >>> svm2 = SVC()\n      >>> cached_pipe = Pipeline([('reduce_dim', pca2), ('clf', svm2)],\n      ...                        memory=cachedir)\n      >>> cached_pipe.fit(X_digits, y_digits)\n      Pipeline(memory=...,\n               steps=[('reduce_dim', PCA(n_components=10)), ('clf', SVC())])\n      >>> cached_pipe.named_steps['reduce_dim'].components_.shape\n      (10, 64)\n      >>> # Remove the cache directory\n      >>> rmtree(cachedir)\n\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_compose_plot_compare_reduction.py`\n\n.. _transformed_target_regressor:"
  },
  {
    "filename": "compose.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\compose.rst.txt",
    "id": "compose.rst.txt_chunk_5",
    "header": "Transforming target in regression",
    "text": "Transforming target in regression\n=================================\n\n:class:`~sklearn.compose.TransformedTargetRegressor` transforms the\ntargets ``y`` before fitting a regression model. The predictions are mapped\nback to the original space via an inverse transform. It takes as an argument\nthe regressor that will be used for prediction, and the transformer that will\nbe applied to the target variable::\n\n  >>> import numpy as np\n  >>> from sklearn.datasets import make_regression\n  >>> from sklearn.compose import TransformedTargetRegressor\n  >>> from sklearn.preprocessing import QuantileTransformer\n  >>> from sklearn.linear_model import LinearRegression\n  >>> from sklearn.model_selection import train_test_split\n  >>> # create a synthetic dataset\n  >>> X, y = make_regression(n_samples=20640,\n  ...                        n_features=8,\n  ...                        noise=100.0,\n  ...                        random_state=0)\n  >>> y = np.exp( 1 + (y - y.min()) * (4 / (y.max() - y.min())))\n  >>> X, y = X[:2000, :], y[:2000]  # select a subset of data\n  >>> transformer = QuantileTransformer(output_distribution='normal')\n  >>> regressor = LinearRegression()\n  >>> regr = TransformedTargetRegressor(regressor=regressor,\n  ...                                   transformer=transformer)\n  >>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n  >>> regr.fit(X_train, y_train)\n  TransformedTargetRegressor(...)\n  >>> print(f\"R2 score: {regr.score(X_test, y_test):.2f}\")\n  R2 score: 0.67\n  >>> raw_target_regr = LinearRegression().fit(X_train, y_train)\n  >>> print(f\"R2 score: {raw_target_regr.score(X_test, y_test):.2f}\")\n  R2 score: 0.64\n\nFor simple transformations, instead of a Transformer object, a pair of\nfunctions can be passed, defining the transformation and its inverse mapping::\n\n  >>> def func(x):\n  ...     return np.log(x)\n  >>> def inverse_func(x):\n  ...     return np.exp(x)\n\nSubsequently, the object is created as::\n\n  >>> regr = TransformedTargetRegressor(regressor=regressor,\n  ...                                   func=func,\n  ...                                   inverse_func=inverse_func)\n  >>> regr.fit(X_train, y_train)\n  TransformedTargetRegressor(...)\n  >>> print(f\"R2 score: {regr.score(X_test, y_test):.2f}\")\n  R2 score: 0.67\n\nBy default, the provided functions are checked at each fit to be the inverse of\neach other. However, it is possible to bypass this checking by setting\n``check_inverse`` to ``False``::\n\n  >>> def inverse_func(x):\n  ...     return x\n  >>> regr = TransformedTargetRegressor(regressor=regressor,\n  ...                                   func=func,\n  ...                                   inverse_func=inverse_func,\n  ...                                   check_inverse=False)\n  >>> regr.fit(X_train, y_train)\n  TransformedTargetRegressor(...)\n  >>> print(f\"R2 score: {regr.score(X_test, y_test):.2f}\")\n  R2 score: -3.02\n\n.. note::\n\n   The transformation can be triggered by setting either ``transformer`` or the\n   pair of functions ``func`` and ``inverse_func``. However, setting both\n   options will raise an error.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_compose_plot_transformed_target.py`\n\n\n.. _feature_union:"
  },
  {
    "filename": "compose.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\compose.rst.txt",
    "id": "compose.rst.txt_chunk_6",
    "header": "FeatureUnion: composite feature spaces",
    "text": "FeatureUnion: composite feature spaces\n======================================\n\n.. currentmodule:: sklearn.pipeline\n\n:class:`FeatureUnion` combines several transformer objects into a new\ntransformer that combines their output. A :class:`FeatureUnion` takes\na list of transformer objects. During fitting, each of these\nis fit to the data independently. The transformers are applied in parallel,\nand the feature matrices they output are concatenated side-by-side into a\nlarger matrix.\n\nWhen you want to apply different transformations to each field of the data,\nsee the related class :class:`~sklearn.compose.ColumnTransformer`\n(see :ref:`user guide <column_transformer>`).\n\n:class:`FeatureUnion` serves the same purposes as :class:`Pipeline` -\nconvenience and joint parameter estimation and validation.\n\n:class:`FeatureUnion` and :class:`Pipeline` can be combined to\ncreate complex models.\n\n(A :class:`FeatureUnion` has no way of checking whether two transformers\nmight produce identical features. It only produces a union when the\nfeature sets are disjoint, and making sure they are is the caller's\nresponsibility.)"
  },
  {
    "filename": "compose.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\compose.rst.txt",
    "id": "compose.rst.txt_chunk_7",
    "header": "Usage",
    "text": "Usage\n-----\n\nA :class:`FeatureUnion` is built using a list of ``(key, value)`` pairs,\nwhere the ``key`` is the name you want to give to a given transformation\n(an arbitrary string; it only serves as an identifier)\nand ``value`` is an estimator object::\n\n    >>> from sklearn.pipeline import FeatureUnion\n    >>> from sklearn.decomposition import PCA\n    >>> from sklearn.decomposition import KernelPCA\n    >>> estimators = [('linear_pca', PCA()), ('kernel_pca', KernelPCA())]\n    >>> combined = FeatureUnion(estimators)\n    >>> combined\n    FeatureUnion(transformer_list=[('linear_pca', PCA()),\n                                   ('kernel_pca', KernelPCA())])\n\n\nLike pipelines, feature unions have a shorthand constructor called\n:func:`make_union` that does not require explicit naming of the components.\n\n\nLike ``Pipeline``, individual steps may be replaced using ``set_params``,\nand ignored by setting to ``'drop'``::\n\n    >>> combined.set_params(kernel_pca='drop')\n    FeatureUnion(transformer_list=[('linear_pca', PCA()),\n                                   ('kernel_pca', 'drop')])\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_compose_plot_feature_union.py`\n\n\n.. _column_transformer:"
  },
  {
    "filename": "compose.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\compose.rst.txt",
    "id": "compose.rst.txt_chunk_8",
    "header": "ColumnTransformer for heterogeneous data",
    "text": "ColumnTransformer for heterogeneous data\n========================================\n\nMany datasets contain features of different types, say text, floats, and dates,\nwhere each type of feature requires separate preprocessing or feature\nextraction steps.  Often it is easiest to preprocess data before applying\nscikit-learn methods, for example using `pandas <https://pandas.pydata.org/>`__.\nProcessing your data before passing it to scikit-learn might be problematic for\none of the following reasons:\n\n1. Incorporating statistics from test data into the preprocessors makes\n   cross-validation scores unreliable (known as *data leakage*),\n   for example in the case of scalers or imputing missing values.\n2. You may want to include the parameters of the preprocessors in a\n   :ref:`parameter search <grid_search>`.\n\nThe :class:`~sklearn.compose.ColumnTransformer` helps performing different\ntransformations for different columns of the data, within a\n:class:`~sklearn.pipeline.Pipeline` that is safe from data leakage and that can\nbe parametrized. :class:`~sklearn.compose.ColumnTransformer` works on\narrays, sparse matrices, and\n`pandas DataFrames <https://pandas.pydata.org/pandas-docs/stable/>`__.\n\nTo each column, a different transformation can be applied, such as\npreprocessing or a specific feature extraction method::\n\n  >>> import pandas as pd\n  >>> X = pd.DataFrame(\n  ...     {'city': ['London', 'London', 'Paris', 'Sallisaw'],\n  ...      'title': [\"His Last Bow\", \"How Watson Learned the Trick\",\n  ...                \"A Moveable Feast\", \"The Grapes of Wrath\"],\n  ...      'expert_rating': [5, 3, 4, 5],\n  ...      'user_rating': [4, 5, 4, 3]})\n\nFor this data, we might want to encode the ``'city'`` column as a categorical\nvariable using :class:`~sklearn.preprocessing.OneHotEncoder` but apply a\n:class:`~sklearn.feature_extraction.text.CountVectorizer` to the ``'title'`` column.\nAs we might use multiple feature extraction methods on the same column, we give\neach transformer a unique name, say ``'city_category'`` and ``'title_bow'``.\nBy default, the remaining rating columns are ignored (``remainder='drop'``)::\n\n  >>> from sklearn.compose import ColumnTransformer\n  >>> from sklearn.feature_extraction.text import CountVectorizer\n  >>> from sklearn.preprocessing import OneHotEncoder\n  >>> column_trans = ColumnTransformer(\n  ...     [('categories', OneHotEncoder(dtype='int'), ['city']),\n  ...      ('title_bow', CountVectorizer(), 'title')],\n  ...     remainder='drop', verbose_feature_names_out=False)\n\n  >>> column_trans.fit(X)\n  ColumnTransformer(transformers=[('categories', OneHotEncoder(dtype='int'),\n                                   ['city']),\n                                  ('title_bow', CountVectorizer(), 'title')],\n                    verbose_feature_names_out=False)\n\n  >>> column_trans.get_feature_names_out()\n  array(['city_London', 'city_Paris', 'city_Sallisaw', 'bow', 'feast',\n  'grapes', 'his', 'how', 'last', 'learned', 'moveable', 'of', 'the',\n   'trick', 'watson', 'wrath'], ...)\n\n  >>> column_trans.transform(X).toarray()\n  array([[1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n         [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0],\n         [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n         [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1]]...)\n\nIn the above example, the\n:class:`~sklearn.feature_extraction.text.CountVectorizer` expects a 1D array as\ninput and therefore the columns were specified as a string (``'title'``).\nHowever, :class:`~sklearn.preprocessing.OneHotEncoder`\nas most of other transformers expects 2D data, therefore in that case you need\nto specify the column as a list of strings (``['city']``).\n\nApart from a scalar or a single item list, the column selection can be specified\nas a list of multiple items, an integer array, a slice, a boolean mask, or\nwith a :func:`~sklearn.compose.make_column_selector`. The\n:func:`~sklearn.compose.make_column_selector` is used to select columns based\non data type or column name::\n\n  >>> from sklearn.preprocessing import StandardScaler\n  >>> from sklearn.compose import make_column_selector\n  >>> ct = ColumnTransformer([\n  ...       ('scale', StandardScaler(),\n  ...       make_column_selector(dtype_include=np.number)),\n  ...       ('onehot',\n  ...       OneHotEncoder(),\n  ...       make_column_selector(pattern='city', dtype_include=object))])\n  >>> ct.fit_transform(X)\n  array([[ 0.904,  0.      ,  1. ,  0. ,  0. ],\n         [-1.507,  1.414,  1. ,  0. ,  0. ],\n         [-0.301,  0.      ,  0. ,  1. ,  0. ],\n         [ 0.904, -1.414,  0. ,  0. ,  1. ]])\n\nStrings can reference columns if the input is a DataFrame, integers are always\ninterpreted as the positional columns.\n\nWe can keep the remaining rating columns by setting\n``remainder='passthrough'``. The values are appended to the end of the\ntransformation::\n\n  >>> column_trans = ColumnTransformer(\n  ...     [('city_category', OneHotEncoder(dtype='int'),['city']),\n  ...      ('title_bow', CountVectorizer(), 'title')],\n  ...     remainder='passthrough')\n\n  >>> column_trans.fit_transform(X)\n  array([[1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 5, 4],\n         [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 3, 5],\n         [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 4],\n         [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 5, 3]]...)\n\nThe ``remainder`` parameter can be set to an estimator to transform the\nremaining rating columns. The transformed values are appended to the end of\nthe transformation::\n\n  >>> from sklearn.preprocessing import MinMaxScaler\n  >>> column_trans = ColumnTransformer(\n  ...     [('city_category', OneHotEncoder(), ['city']),\n  ...      ('title_bow', CountVectorizer(), 'title')],\n  ...     remainder=MinMaxScaler())\n\n  >>> column_trans.fit_transform(X)[:, -2:]\n  array([[1. , 0.5],\n         [0. , 1. ],\n         [0.5, 0.5],\n         [1. , 0. ]])\n\n.. _make_column_transformer:\n\nThe :func:`~sklearn.compose.make_column_transformer` function is available\nto more easily create a :class:`~sklearn.compose.ColumnTransformer` object.\nSpecifically, the names will be given automatically. The equivalent for the\nabove example would be::\n\n  >>> from sklearn.compose import make_column_transformer\n  >>> column_trans = make_column_transformer(\n  ...     (OneHotEncoder(), ['city']),\n  ...     (CountVectorizer(), 'title'),\n  ...     remainder=MinMaxScaler())\n  >>> column_trans\n  ColumnTransformer(remainder=MinMaxScaler(),\n                    transformers=[('onehotencoder', OneHotEncoder(), ['city']),\n                                  ('countvectorizer', CountVectorizer(),\n                                   'title')])\n\nIf :class:`~sklearn.compose.ColumnTransformer` is fitted with a dataframe\nand the dataframe only has string column names, then transforming a dataframe\nwill use the column names to select the columns::\n\n\n  >>> ct = ColumnTransformer(\n  ...          [(\"scale\", StandardScaler(), [\"expert_rating\"])]).fit(X)\n  >>> X_new = pd.DataFrame({\"expert_rating\": [5, 6, 1],\n  ...                       \"ignored_new_col\": [1.2, 0.3, -0.1]})\n  >>> ct.transform(X_new)\n  array([[ 0.9],\n         [ 2.1],\n         [-3.9]])\n\n.. _visualizing_composite_estimators:"
  },
  {
    "filename": "compose.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\compose.rst.txt",
    "id": "compose.rst.txt_chunk_9",
    "header": "Visualizing Composite Estimators",
    "text": "Visualizing Composite Estimators\n================================\n\nEstimators are displayed with an HTML representation when shown in a\njupyter notebook. This is useful to diagnose or visualize a Pipeline with\nmany estimators. This visualization is activated by default::\n\n  >>> column_trans  # doctest: +SKIP\n\nIt can be deactivated by setting the `display` option in :func:`~sklearn.set_config`\nto 'text'::\n\n  >>> from sklearn import set_config\n  >>> set_config(display='text')  # doctest: +SKIP\n  >>> # displays text representation in a jupyter context\n  >>> column_trans  # doctest: +SKIP\n\nAn example of the HTML output can be seen in the\n**HTML representation of Pipeline** section of\n:ref:`sphx_glr_auto_examples_compose_plot_column_transformer_mixed_types.py`.\nAs an alternative, the HTML can be written to a file using\n:func:`~sklearn.utils.estimator_html_repr`::\n\n   >>> from sklearn.utils import estimator_html_repr\n   >>> with open('my_estimator.html', 'w') as f:  # doctest: +SKIP\n   ...     f.write(estimator_html_repr(clf))\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_compose_plot_column_transformer.py`\n* :ref:`sphx_glr_auto_examples_compose_plot_column_transformer_mixed_types.py`"
  },
  {
    "filename": "covariance.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\covariance.rst.txt",
    "id": "covariance.rst.txt_chunk_0",
    "header": ".. _covariance:",
    "text": ".. _covariance:\n\n==================================================="
  },
  {
    "filename": "covariance.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\covariance.rst.txt",
    "id": "covariance.rst.txt_chunk_1",
    "header": "Covariance estimation",
    "text": "Covariance estimation\n===================================================\n\n.. currentmodule:: sklearn.covariance\n\n\nMany statistical problems require the estimation of a\npopulation's covariance matrix, which can be seen as an estimation of\ndata set scatter plot shape. Most of the time, such an estimation has\nto be done on a sample whose properties (size, structure, homogeneity)\nhave a large influence on the estimation's quality. The\n:mod:`sklearn.covariance` package provides tools for accurately estimating\na population's covariance matrix under various settings.\n\nWe assume that the observations are independent and identically\ndistributed (i.i.d.)."
  },
  {
    "filename": "covariance.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\covariance.rst.txt",
    "id": "covariance.rst.txt_chunk_2",
    "header": "Empirical covariance",
    "text": "Empirical covariance\n====================\n\nThe covariance matrix of a data set is known to be well approximated\nby the classical *maximum likelihood estimator* (or \"empirical\ncovariance\"), provided the number of observations is large enough\ncompared to the number of features (the variables describing the\nobservations). More precisely, the Maximum Likelihood Estimator of a\nsample is an asymptotically unbiased estimator of the corresponding\npopulation's covariance matrix.\n\nThe empirical covariance matrix of a sample can be computed using the\n:func:`empirical_covariance` function of the package, or by fitting an\n:class:`EmpiricalCovariance` object to the data sample with the\n:meth:`EmpiricalCovariance.fit` method. Be careful that results depend\non whether the data are centered, so one may want to use the\n`assume_centered` parameter accurately. More precisely, if `assume_centered=True`, then\nall features in the train and test sets should have a mean of zero. If not, both should\nbe centered by the user, or `assume_centered=False` should be used.\n\n.. rubric:: Examples\n\n* See :ref:`sphx_glr_auto_examples_covariance_plot_covariance_estimation.py` for\n  an example on how to fit an :class:`EmpiricalCovariance` object to data.\n\n\n.. _shrunk_covariance:"
  },
  {
    "filename": "covariance.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\covariance.rst.txt",
    "id": "covariance.rst.txt_chunk_3",
    "header": "Shrunk Covariance",
    "text": "Shrunk Covariance\n================="
  },
  {
    "filename": "covariance.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\covariance.rst.txt",
    "id": "covariance.rst.txt_chunk_4",
    "header": "Basic shrinkage",
    "text": "Basic shrinkage\n---------------\n\nDespite being an asymptotically unbiased estimator of the covariance matrix,\nthe Maximum Likelihood Estimator is not a good estimator of the\neigenvalues of the covariance matrix, so the precision matrix obtained\nfrom its inversion is not accurate. Sometimes, it even occurs that the\nempirical covariance matrix cannot be inverted for numerical\nreasons. To avoid such an inversion problem, a transformation of the\nempirical covariance matrix has been introduced: the ``shrinkage``.\n\nIn scikit-learn, this transformation (with a user-defined shrinkage\ncoefficient) can be directly applied to a pre-computed covariance with\nthe :func:`shrunk_covariance` method. Also, a shrunk estimator of the\ncovariance can be fitted to data with a :class:`ShrunkCovariance` object\nand its :meth:`ShrunkCovariance.fit` method. Again, results depend on\nwhether the data are centered, so one may want to use the\n``assume_centered`` parameter accurately.\n\n\nMathematically, this shrinkage consists in reducing the ratio between the\nsmallest and the largest eigenvalues of the empirical covariance matrix.\nIt can be done by simply shifting every eigenvalue according to a given\noffset, which is equivalent of finding the l2-penalized Maximum\nLikelihood Estimator of the covariance matrix. In practice, shrinkage\nboils down to a simple convex transformation : :math:`\\Sigma_{\\rm\nshrunk} = (1-\\alpha)\\hat{\\Sigma} + \\alpha\\frac{{\\rm\nTr}\\hat{\\Sigma}}{p}\\rm Id`.\n\nChoosing the amount of shrinkage, :math:`\\alpha` amounts to setting a\nbias/variance trade-off, and is discussed below.\n\n.. rubric:: Examples\n\n* See :ref:`sphx_glr_auto_examples_covariance_plot_covariance_estimation.py` for\n  an example on how to fit a :class:`ShrunkCovariance` object to data."
  },
  {
    "filename": "covariance.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\covariance.rst.txt",
    "id": "covariance.rst.txt_chunk_5",
    "header": "Ledoit-Wolf shrinkage",
    "text": "Ledoit-Wolf shrinkage\n---------------------\n\nIn their 2004 paper [1]_, O. Ledoit and M. Wolf propose a formula\nto compute the optimal shrinkage coefficient :math:`\\alpha` that\nminimizes the Mean Squared Error between the estimated and the real\ncovariance matrix.\n\nThe Ledoit-Wolf estimator of the covariance matrix can be computed on\na sample with the :meth:`ledoit_wolf` function of the\n:mod:`sklearn.covariance` package, or it can be otherwise obtained by\nfitting a :class:`LedoitWolf` object to the same sample.\n\n.. note:: **Case when population covariance matrix is isotropic**\n\n    It is important to note that when the number of samples is much larger than\n    the number of features, one would expect that no shrinkage would be\n    necessary. The intuition behind this is that if the population covariance\n    is full rank, when the number of samples grows, the sample covariance will\n    also become positive definite. As a result, no shrinkage would be necessary\n    and the method should automatically do this.\n\n    This, however, is not the case in the Ledoit-Wolf procedure when the\n    population covariance happens to be a multiple of the identity matrix. In\n    this case, the Ledoit-Wolf shrinkage estimate approaches 1 as the number of\n    samples increases. This indicates that the optimal estimate of the\n    covariance matrix in the Ledoit-Wolf sense is a multiple of the identity.\n    Since the population covariance is already a multiple of the identity\n    matrix, the Ledoit-Wolf solution is indeed a reasonable estimate.\n\n.. rubric:: Examples\n\n* See :ref:`sphx_glr_auto_examples_covariance_plot_covariance_estimation.py` for\n  an example on how to fit a :class:`LedoitWolf` object to data and\n  for visualizing the performances of the Ledoit-Wolf estimator in\n  terms of likelihood.\n\n.. rubric:: References\n\n.. [1] O. Ledoit and M. Wolf, \"A Well-Conditioned Estimator for Large-Dimensional\n       Covariance Matrices\", Journal of Multivariate Analysis, Volume 88, Issue 2,\n       February 2004, pages 365-411.\n\n.. _oracle_approximating_shrinkage:"
  },
  {
    "filename": "covariance.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\covariance.rst.txt",
    "id": "covariance.rst.txt_chunk_6",
    "header": "Oracle Approximating Shrinkage",
    "text": "Oracle Approximating Shrinkage\n------------------------------\n\nUnder the assumption that the data are Gaussian distributed, Chen et\nal. [2]_ derived a formula aimed at choosing a shrinkage coefficient that\nyields a smaller Mean Squared Error than the one given by Ledoit and\nWolf's formula. The resulting estimator is known as the Oracle\nShrinkage Approximating estimator of the covariance.\n\nThe OAS estimator of the covariance matrix can be computed on a sample\nwith the :meth:`oas` function of the :mod:`sklearn.covariance`\npackage, or it can be otherwise obtained by fitting an :class:`OAS`\nobject to the same sample.\n\n.. figure:: ../auto_examples/covariance/images/sphx_glr_plot_covariance_estimation_001.png\n   :target: ../auto_examples/covariance/plot_covariance_estimation.html\n   :align: center\n   :scale: 65%\n\n   Bias-variance trade-off when setting the shrinkage: comparing the\n   choices of Ledoit-Wolf and OAS estimators\n\n.. rubric:: References\n\n.. [2] :arxiv:`\"Shrinkage algorithms for MMSE covariance estimation.\",\n       Chen, Y., Wiesel, A., Eldar, Y. C., & Hero, A. O.\n       IEEE Transactions on Signal Processing, 58(10), 5016-5029, 2010.\n       <0907.4698>`\n\n.. rubric:: Examples\n\n* See :ref:`sphx_glr_auto_examples_covariance_plot_covariance_estimation.py` for\n  an example on how to fit an :class:`OAS` object to data.\n\n* See :ref:`sphx_glr_auto_examples_covariance_plot_lw_vs_oas.py` to visualize the\n  Mean Squared Error difference between a :class:`LedoitWolf` and\n  an :class:`OAS` estimator of the covariance.\n\n\n.. figure:: ../auto_examples/covariance/images/sphx_glr_plot_lw_vs_oas_001.png\n   :target: ../auto_examples/covariance/plot_lw_vs_oas.html\n   :align: center\n   :scale: 75%\n\n\n.. _sparse_inverse_covariance:"
  },
  {
    "filename": "covariance.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\covariance.rst.txt",
    "id": "covariance.rst.txt_chunk_7",
    "header": "Sparse inverse covariance",
    "text": "Sparse inverse covariance\n==========================\n\nThe matrix inverse of the covariance matrix, often called the precision\nmatrix, is proportional to the partial correlation matrix. It gives the\npartial independence relationship. In other words, if two features are\nindependent conditionally on the others, the corresponding coefficient in\nthe precision matrix will be zero. This is why it makes sense to\nestimate a sparse precision matrix: the estimation of the covariance\nmatrix is better conditioned by learning independence relations from\nthe data. This is known as *covariance selection*.\n\nIn the small-samples situation, in which ``n_samples`` is on the order\nof ``n_features`` or smaller, sparse inverse covariance estimators tend to work\nbetter than shrunk covariance estimators. However, in the opposite\nsituation, or for very correlated data, they can be numerically unstable.\nIn addition, unlike shrinkage estimators, sparse estimators are able to\nrecover off-diagonal structure.\n\nThe :class:`GraphicalLasso` estimator uses an l1 penalty to enforce sparsity on\nthe precision matrix: the higher its ``alpha`` parameter, the more sparse\nthe precision matrix. The corresponding :class:`GraphicalLassoCV` object uses\ncross-validation to automatically set the ``alpha`` parameter.\n\n.. figure:: ../auto_examples/covariance/images/sphx_glr_plot_sparse_cov_001.png\n   :target: ../auto_examples/covariance/plot_sparse_cov.html\n   :align: center\n   :scale: 60%\n\n   *A comparison of maximum likelihood, shrinkage and sparse estimates of\n   the covariance and precision matrix in the very small samples\n   settings.*\n\n.. note:: **Structure recovery**\n\n   Recovering a graphical structure from correlations in the data is a\n   challenging thing. If you are interested in such recovery keep in mind\n   that:\n\n   * Recovery is easier from a correlation matrix than a covariance\n     matrix: standardize your observations before running :class:`GraphicalLasso`\n\n   * If the underlying graph has nodes with much more connections than\n     the average node, the algorithm will miss some of these connections.\n\n   * If your number of observations is not large compared to the number\n     of edges in your underlying graph, you will not recover it.\n\n   * Even if you are in favorable recovery conditions, the alpha\n     parameter chosen by cross-validation (e.g. using the\n     :class:`GraphicalLassoCV` object) will lead to selecting too many edges.\n     However, the relevant edges will have heavier weights than the\n     irrelevant ones.\n\nThe mathematical formulation is the following:\n\n.. math::\n\n    \\hat{K} = \\mathrm{argmin}_K \\big(\n                \\mathrm{tr} S K - \\mathrm{log} \\mathrm{det} K\n                + \\alpha \\|K\\|_1\n                \\big)\n\nWhere :math:`K` is the precision matrix to be estimated, and :math:`S` is the\nsample covariance matrix. :math:`\\|K\\|_1` is the sum of the absolute values of\noff-diagonal coefficients of :math:`K`. The algorithm employed to solve this\nproblem is the GLasso algorithm, from the Friedman 2008 Biostatistics\npaper. It is the same algorithm as in the R ``glasso`` package.\n\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_covariance_plot_sparse_cov.py`: example on synthetic\n  data showing some recovery of a structure, and comparing to other\n  covariance estimators.\n\n* :ref:`sphx_glr_auto_examples_applications_plot_stock_market.py`: example on real\n  stock market data, finding which symbols are most linked.\n\n.. rubric:: References\n\n* Friedman et al, `\"Sparse inverse covariance estimation with the\n  graphical lasso\" <https://biostatistics.oxfordjournals.org/content/9/3/432.short>`_,\n  Biostatistics 9, pp 432, 2008\n\n.. _robust_covariance:"
  },
  {
    "filename": "covariance.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\covariance.rst.txt",
    "id": "covariance.rst.txt_chunk_8",
    "header": "Robust Covariance Estimation",
    "text": "Robust Covariance Estimation\n============================\n\nReal data sets are often subject to measurement or recording\nerrors. Regular but uncommon observations may also appear for a variety\nof reasons. Observations which are very uncommon are called\noutliers.\nThe empirical covariance estimator and the shrunk covariance\nestimators presented above are very sensitive to the presence of\noutliers in the data. Therefore, one should use robust\ncovariance estimators to estimate the covariance of its real data\nsets. Alternatively, robust covariance estimators can be used to\nperform outlier detection and discard/downweight some observations\naccording to further processing of the data.\n\nThe ``sklearn.covariance`` package implements a robust estimator of covariance,\nthe Minimum Covariance Determinant [3]_."
  },
  {
    "filename": "covariance.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\covariance.rst.txt",
    "id": "covariance.rst.txt_chunk_9",
    "header": "Minimum Covariance Determinant",
    "text": "Minimum Covariance Determinant\n------------------------------\n\nThe Minimum Covariance Determinant estimator is a robust estimator of\na data set's covariance introduced by P.J. Rousseeuw in [3]_.  The idea\nis to find a given proportion (h) of \"good\" observations which are not\noutliers and compute their empirical covariance matrix.  This\nempirical covariance matrix is then rescaled to compensate the\nperformed selection of observations (\"consistency step\").  Having\ncomputed the Minimum Covariance Determinant estimator, one can give\nweights to observations according to their Mahalanobis distance,\nleading to a reweighted estimate of the covariance matrix of the data\nset (\"reweighting step\").\n\nRousseeuw and Van Driessen [4]_ developed the FastMCD algorithm in order\nto compute the Minimum Covariance Determinant. This algorithm is used\nin scikit-learn when fitting an MCD object to data. The FastMCD\nalgorithm also computes a robust estimate of the data set location at\nthe same time.\n\nRaw estimates can be accessed as ``raw_location_`` and ``raw_covariance_``\nattributes of a :class:`MinCovDet` robust covariance estimator object.\n\n.. rubric:: References\n\n.. [3] P. J. Rousseeuw. Least median of squares regression.\n       J. Am Stat Ass, 79:871, 1984.\n.. [4] A Fast Algorithm for the Minimum Covariance Determinant Estimator,\n       1999, American Statistical Association and the American Society\n       for Quality, TECHNOMETRICS.\n\n.. rubric:: Examples\n\n* See :ref:`sphx_glr_auto_examples_covariance_plot_robust_vs_empirical_covariance.py` for\n  an example on how to fit a :class:`MinCovDet` object to data and see how\n  the estimate remains accurate despite the presence of outliers.\n\n* See :ref:`sphx_glr_auto_examples_covariance_plot_mahalanobis_distances.py` to\n  visualize the difference between :class:`EmpiricalCovariance` and\n  :class:`MinCovDet` covariance estimators in terms of Mahalanobis distance\n  (so we get a better estimate of the precision matrix too).\n\n.. |robust_vs_emp| image:: ../auto_examples/covariance/images/sphx_glr_plot_robust_vs_empirical_covariance_001.png\n   :target: ../auto_examples/covariance/plot_robust_vs_empirical_covariance.html\n   :scale: 49%\n\n.. |mahalanobis| image:: ../auto_examples/covariance/images/sphx_glr_plot_mahalanobis_distances_001.png\n   :target: ../auto_examples/covariance/plot_mahalanobis_distances.html\n   :scale: 49%\n\n\n\n____\n\n.. list-table::\n    :header-rows: 1\n\n    * - Influence of outliers on location and covariance estimates\n      - Separating inliers from outliers using a Mahalanobis distance\n\n    * - |robust_vs_emp|\n      - |mahalanobis|"
  },
  {
    "filename": "cross_decomposition.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\cross_decomposition.rst.txt",
    "id": "cross_decomposition.rst.txt_chunk_0",
    "header": ".. _cross_decomposition:",
    "text": ".. _cross_decomposition:\n\n==================="
  },
  {
    "filename": "cross_decomposition.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\cross_decomposition.rst.txt",
    "id": "cross_decomposition.rst.txt_chunk_1",
    "header": "Cross decomposition",
    "text": "Cross decomposition\n===================\n\n.. currentmodule:: sklearn.cross_decomposition\n\nThe cross decomposition module contains **supervised** estimators for\ndimensionality reduction and regression, belonging to the \"Partial Least\nSquares\" family.\n\n.. figure:: ../auto_examples/cross_decomposition/images/sphx_glr_plot_compare_cross_decomposition_001.png\n   :target: ../auto_examples/cross_decomposition/plot_compare_cross_decomposition.html\n   :scale: 75%\n   :align: center\n\n\nCross decomposition algorithms find the fundamental relations between two\nmatrices (X and Y). They are latent variable approaches to modeling the\ncovariance structures in these two spaces. They will try to find the\nmultidimensional direction in the X space that explains the maximum\nmultidimensional variance direction in the Y space. In other words, PLS\nprojects both `X` and `Y` into a lower-dimensional subspace such that the\ncovariance between `transformed(X)` and `transformed(Y)` is maximal.\n\nPLS draws similarities with `Principal Component Regression\n<https://en.wikipedia.org/wiki/Principal_component_regression>`_ (PCR), where\nthe samples are first projected into a lower-dimensional subspace, and the\ntargets `y` are predicted using `transformed(X)`. One issue with PCR is that\nthe dimensionality reduction is unsupervised, and may lose some important\nvariables: PCR would keep the features with the most variance, but it's\npossible that features with small variances are relevant for predicting\nthe target. In a way, PLS allows for the same kind of dimensionality\nreduction, but by taking into account the targets `y`. An illustration of\nthis fact is given in the following example:\n* :ref:`sphx_glr_auto_examples_cross_decomposition_plot_pcr_vs_pls.py`.\n\nApart from CCA, the PLS estimators are particularly suited when the matrix of\npredictors has more variables than observations, and when there is\nmulticollinearity among the features. By contrast, standard linear regression\nwould fail in these cases unless it is regularized.\n\nClasses included in this module are :class:`PLSRegression`,\n:class:`PLSCanonical`, :class:`CCA` and :class:`PLSSVD`"
  },
  {
    "filename": "cross_decomposition.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\cross_decomposition.rst.txt",
    "id": "cross_decomposition.rst.txt_chunk_2",
    "header": "PLSCanonical",
    "text": "PLSCanonical\n------------\n\nWe here describe the algorithm used in :class:`PLSCanonical`. The other\nestimators use variants of this algorithm, and are detailed below.\nWe recommend section [1]_ for more details and comparisons between these\nalgorithms. In [1]_, :class:`PLSCanonical` corresponds to \"PLSW2A\".\n\nGiven two centered matrices :math:`X \\in \\mathbb{R}^{n \\times d}` and\n:math:`Y \\in \\mathbb{R}^{n \\times t}`, and a number of components :math:`K`,\n:class:`PLSCanonical` proceeds as follows:\n\nSet :math:`X_1` to :math:`X` and :math:`Y_1` to :math:`Y`. Then, for each\n:math:`k \\in [1, K]`:\n\n- a) compute :math:`u_k \\in \\mathbb{R}^d` and :math:`v_k \\in \\mathbb{R}^t`,\n  the first left and right singular vectors of the cross-covariance matrix\n  :math:`C = X_k^T Y_k`.\n  :math:`u_k` and :math:`v_k` are called the *weights*.\n  By definition, :math:`u_k` and :math:`v_k` are\n  chosen so that they maximize the covariance between the projected\n  :math:`X_k` and the projected target, that is :math:`\\text{Cov}(X_k u_k,\n  Y_k v_k)`.\n- b) Project :math:`X_k` and :math:`Y_k` on the singular vectors to obtain\n  *scores*: :math:`\\xi_k = X_k u_k` and :math:`\\omega_k = Y_k v_k`\n- c) Regress :math:`X_k` on :math:`\\xi_k`, i.e. find a vector :math:`\\gamma_k\n  \\in \\mathbb{R}^d` such that the rank-1 matrix :math:`\\xi_k \\gamma_k^T`\n  is as close as possible to :math:`X_k`. Do the same on :math:`Y_k` with\n  :math:`\\omega_k` to obtain :math:`\\delta_k`. The vectors\n  :math:`\\gamma_k` and :math:`\\delta_k` are called the *loadings*.\n- d) *deflate* :math:`X_k` and :math:`Y_k`, i.e. subtract the rank-1\n  approximations: :math:`X_{k+1} = X_k - \\xi_k \\gamma_k^T`, and\n  :math:`Y_{k + 1} = Y_k - \\omega_k \\delta_k^T`.\n\nAt the end, we have approximated :math:`X` as a sum of rank-1 matrices:\n:math:`X = \\Xi \\Gamma^T` where :math:`\\Xi \\in \\mathbb{R}^{n \\times K}`\ncontains the scores in its columns, and :math:`\\Gamma^T \\in \\mathbb{R}^{K\n\\times d}` contains the loadings in its rows. Similarly for :math:`Y`, we\nhave :math:`Y = \\Omega \\Delta^T`.\n\nNote that the scores matrices :math:`\\Xi` and :math:`\\Omega` correspond to\nthe projections of the training data :math:`X` and :math:`Y`, respectively.\n\nStep *a)* may be performed in two ways: either by computing the whole SVD of\n:math:`C` and only retaining the singular vectors with the biggest singular\nvalues, or by directly computing the singular vectors using the power method (cf section 11.3 in [1]_),\nwhich corresponds to the `'nipals'` option of the `algorithm` parameter.\n\n.. dropdown:: Transforming data\n\n  To transform :math:`X` into :math:`\\bar{X}`, we need to find a projection\n  matrix :math:`P` such that :math:`\\bar{X} = XP`. We know that for the\n  training data, :math:`\\Xi = XP`, and :math:`X = \\Xi \\Gamma^T`. Setting\n  :math:`P = U(\\Gamma^T U)^{-1}` where :math:`U` is the matrix with the\n  :math:`u_k` in the columns, we have :math:`XP = X U(\\Gamma^T U)^{-1} = \\Xi\n  (\\Gamma^T U) (\\Gamma^T U)^{-1} = \\Xi` as desired. The rotation matrix\n  :math:`P` can be accessed from the `x_rotations_` attribute.\n\n  Similarly, :math:`Y` can be transformed using the rotation matrix\n  :math:`V(\\Delta^T V)^{-1}`, accessed via the `y_rotations_` attribute.\n\n.. dropdown:: Predicting the targets `Y`\n\n  To predict the targets of some data :math:`X`, we are looking for a\n  coefficient matrix :math:`\\beta \\in R^{d \\times t}` such that :math:`Y =\n  X\\beta`.\n\n  The idea is to try to predict the transformed targets :math:`\\Omega` as a\n  function of the transformed samples :math:`\\Xi`, by computing :math:`\\alpha\n  \\in \\mathbb{R}` such that :math:`\\Omega = \\alpha \\Xi`.\n\n  Then, we have :math:`Y = \\Omega \\Delta^T = \\alpha \\Xi \\Delta^T`, and since\n  :math:`\\Xi` is the transformed training data we have that :math:`Y = X \\alpha\n  P \\Delta^T`, and as a result the coefficient matrix :math:`\\beta = \\alpha P\n  \\Delta^T`.\n\n  :math:`\\beta` can be accessed through the `coef_` attribute."
  },
  {
    "filename": "cross_decomposition.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\cross_decomposition.rst.txt",
    "id": "cross_decomposition.rst.txt_chunk_3",
    "header": "PLSSVD",
    "text": "PLSSVD\n------\n\n:class:`PLSSVD` is a simplified version of :class:`PLSCanonical`\ndescribed earlier: instead of iteratively deflating the matrices :math:`X_k`\nand :math:`Y_k`, :class:`PLSSVD` computes the SVD of :math:`C = X^TY`\nonly *once*, and stores the `n_components` singular vectors corresponding to\nthe biggest singular values in the matrices `U` and `V`, corresponding to the\n`x_weights_` and `y_weights_` attributes. Here, the transformed data is\nsimply `transformed(X) = XU` and `transformed(Y) = YV`.\n\nIf `n_components == 1`, :class:`PLSSVD` and :class:`PLSCanonical` are\nstrictly equivalent."
  },
  {
    "filename": "cross_decomposition.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\cross_decomposition.rst.txt",
    "id": "cross_decomposition.rst.txt_chunk_4",
    "header": "PLSRegression",
    "text": "PLSRegression\n-------------\n\nThe :class:`PLSRegression` estimator is similar to\n:class:`PLSCanonical` with `algorithm='nipals'`, with 2 significant\ndifferences:\n\n- at step a) in the power method to compute :math:`u_k` and :math:`v_k`,\n  :math:`v_k` is never normalized.\n- at step c), the targets :math:`Y_k` are approximated using the projection\n  of :math:`X_k` (i.e. :math:`\\xi_k`) instead of the projection of\n  :math:`Y_k` (i.e. :math:`\\omega_k`). In other words, the loadings\n  computation is different. As a result, the deflation in step d) will also\n  be affected.\n\nThese two modifications affect the output of `predict` and `transform`,\nwhich are not the same as for :class:`PLSCanonical`. Also, while the number\nof components is limited by `min(n_samples, n_features, n_targets)` in\n:class:`PLSCanonical`, here the limit is the rank of :math:`X^TX`, i.e.\n`min(n_samples, n_features)`.\n\n:class:`PLSRegression` is also known as PLS1 (single targets) and PLS2\n(multiple targets). Much like :class:`~sklearn.linear_model.Lasso`,\n:class:`PLSRegression` is a form of regularized linear regression where the\nnumber of components controls the strength of the regularization."
  },
  {
    "filename": "cross_decomposition.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\cross_decomposition.rst.txt",
    "id": "cross_decomposition.rst.txt_chunk_5",
    "header": "Canonical Correlation Analysis",
    "text": "Canonical Correlation Analysis\n------------------------------\n\nCanonical Correlation Analysis was developed prior and independently to PLS.\nBut it turns out that :class:`CCA` is a special case of PLS, and corresponds\nto PLS in \"Mode B\" in the literature.\n\n:class:`CCA` differs from :class:`PLSCanonical` in the way the weights\n:math:`u_k` and :math:`v_k` are computed in the power method of step a).\nDetails can be found in section 10 of [1]_.\n\nSince :class:`CCA` involves the inversion of :math:`X_k^TX_k` and\n:math:`Y_k^TY_k`, this estimator can be unstable if the number of features or\ntargets is greater than the number of samples.\n\n.. rubric:: References\n\n.. [1] `A survey of Partial Least Squares (PLS) methods, with emphasis on the two-block\n  case <https://stat.uw.edu/sites/default/files/files/reports/2000/tr371.pdf>`_,\n  JA Wegelin\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_cross_decomposition_plot_compare_cross_decomposition.py`\n* :ref:`sphx_glr_auto_examples_cross_decomposition_plot_pcr_vs_pls.py`"
  },
  {
    "filename": "cross_validation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\cross_validation.rst.txt",
    "id": "cross_validation.rst.txt_chunk_0",
    "header": "",
    "text": ".. _cross_validation:\n\n==================================================="
  },
  {
    "filename": "cross_validation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\cross_validation.rst.txt",
    "id": "cross_validation.rst.txt_chunk_1",
    "header": "Cross-validation: evaluating estimator performance",
    "text": "Cross-validation: evaluating estimator performance\n===================================================\n\n.. currentmodule:: sklearn.model_selection\n\nLearning the parameters of a prediction function and testing it on the\nsame data is a methodological mistake: a model that would just repeat\nthe labels of the samples that it has just seen would have a perfect\nscore but would fail to predict anything useful on yet-unseen data.\nThis situation is called **overfitting**.\nTo avoid it, it is common practice when performing\na (supervised) machine learning experiment\nto hold out part of the available data as a **test set** ``X_test, y_test``.\nNote that the word \"experiment\" is not intended\nto denote academic use only,\nbecause even in commercial settings\nmachine learning usually starts out experimentally.\nHere is a flowchart of typical cross validation workflow in model training.\nThe best parameters can be determined by\n:ref:`grid search <grid_search>` techniques.\n\n.. image:: ../images/grid_search_workflow.png\n   :width: 400px\n   :height: 240px\n   :alt: Grid Search Workflow\n   :align: center\n\nIn scikit-learn a random split into training and test sets\ncan be quickly computed with the :func:`train_test_split` helper function.\nLet's load the iris data set to fit a linear support vector machine on it::\n\n  >>> import numpy as np\n  >>> from sklearn.model_selection import train_test_split\n  >>> from sklearn import datasets\n  >>> from sklearn import svm\n\n  >>> X, y = datasets.load_iris(return_X_y=True)\n  >>> X.shape, y.shape\n  ((150, 4), (150,))\n\nWe can now quickly sample a training set while holding out 40% of the\ndata for testing (evaluating) our classifier::\n\n  >>> X_train, X_test, y_train, y_test = train_test_split(\n  ...     X, y, test_size=0.4, random_state=0)\n\n  >>> X_train.shape, y_train.shape\n  ((90, 4), (90,))\n  >>> X_test.shape, y_test.shape\n  ((60, 4), (60,))\n\n  >>> clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n  >>> clf.score(X_test, y_test)\n  0.96\n\nWhen evaluating different settings (\"hyperparameters\") for estimators,\nsuch as the ``C`` setting that must be manually set for an SVM,\nthere is still a risk of overfitting *on the test set*\nbecause the parameters can be tweaked until the estimator performs optimally.\nThis way, knowledge about the test set can \"leak\" into the model\nand evaluation metrics no longer report on generalization performance.\nTo solve this problem, yet another part of the dataset can be held out\nas a so-called \"validation set\": training proceeds on the training set,\nafter which evaluation is done on the validation set,\nand when the experiment seems to be successful,\nfinal evaluation can be done on the test set.\n\nHowever, by partitioning the available data into three sets,\nwe drastically reduce the number of samples\nwhich can be used for learning the model,\nand the results can depend on a particular random choice for the pair of\n(train, validation) sets.\n\nA solution to this problem is a procedure called\n`cross-validation <https://en.wikipedia.org/wiki/Cross-validation_(statistics)>`_\n(CV for short).\nA test set should still be held out for final evaluation,\nbut the validation set is no longer needed when doing CV.\nIn the basic approach, called *k*-fold CV,\nthe training set is split into *k* smaller sets\n(other approaches are described below,\nbut generally follow the same principles).\nThe following procedure is followed for each of the *k* \"folds\":\n\n* A model is trained using :math:`k-1` of the folds as training data;\n* the resulting model is validated on the remaining part of the data\n  (i.e., it is used as a test set to compute a performance measure\n  such as accuracy).\n\nThe performance measure reported by *k*-fold cross-validation\nis then the average of the values computed in the loop.\nThis approach can be computationally expensive,\nbut does not waste too much data\n(as is the case when fixing an arbitrary validation set),\nwhich is a major advantage in problems such as inverse inference\nwhere the number of samples is very small.\n\n.. image:: ../images/grid_search_cross_validation.png\n   :width: 500px\n   :height: 300px\n   :alt: A depiction of a 5 fold cross validation on a training set, while holding out a test set.\n   :align: center"
  },
  {
    "filename": "cross_validation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\cross_validation.rst.txt",
    "id": "cross_validation.rst.txt_chunk_2",
    "header": "Computing cross-validated metrics",
    "text": "Computing cross-validated metrics\n=================================\n\nThe simplest way to use cross-validation is to call the\n:func:`cross_val_score` helper function on the estimator and the dataset.\n\nThe following example demonstrates how to estimate the accuracy of a linear\nkernel support vector machine on the iris dataset by splitting the data, fitting\na model and computing the score 5 consecutive times (with different splits each\ntime)::\n\n  >>> from sklearn.model_selection import cross_val_score\n  >>> clf = svm.SVC(kernel='linear', C=1, random_state=42)\n  >>> scores = cross_val_score(clf, X, y, cv=5)\n  >>> scores\n  array([0.96, 1. , 0.96, 0.96, 1. ])\n\nThe mean score and the standard deviation are hence given by::\n\n  >>> print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n  0.98 accuracy with a standard deviation of 0.02\n\nBy default, the score computed at each CV iteration is the ``score``\nmethod of the estimator. It is possible to change this by using the\nscoring parameter::\n\n  >>> from sklearn import metrics\n  >>> scores = cross_val_score(\n  ...     clf, X, y, cv=5, scoring='f1_macro')\n  >>> scores\n  array([0.96, 1., 0.96, 0.96, 1.])\n\nSee :ref:`scoring_parameter` for details.\nIn the case of the Iris dataset, the samples are balanced across target\nclasses hence the accuracy and the F1-score are almost equal.\n\nWhen the ``cv`` argument is an integer, :func:`cross_val_score` uses the\n:class:`KFold` or :class:`StratifiedKFold` strategies by default, the latter\nbeing used if the estimator derives from :class:`ClassifierMixin\n<sklearn.base.ClassifierMixin>`.\n\nIt is also possible to use other cross validation strategies by passing a cross\nvalidation iterator instead, for instance::\n\n  >>> from sklearn.model_selection import ShuffleSplit\n  >>> n_samples = X.shape[0]\n  >>> cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n  >>> cross_val_score(clf, X, y, cv=cv)\n  array([0.977, 0.977, 1., 0.955, 1.])\n\nAnother option is to use an iterable yielding (train, test) splits as arrays of\nindices, for example::\n\n  >>> def custom_cv_2folds(X):\n  ...     n = X.shape[0]\n  ...     i = 1\n  ...     while i <= 2:\n  ...         idx = np.arange(n * (i - 1) / 2, n * i / 2, dtype=int)\n  ...         yield idx, idx\n  ...         i += 1\n  ...\n  >>> custom_cv = custom_cv_2folds(X)\n  >>> cross_val_score(clf, X, y, cv=custom_cv)\n  array([1.        , 0.973])\n\n.. dropdown:: Data transformation with held-out data\n\n  Just as it is important to test a predictor on data held-out from\n  training, preprocessing (such as standardization, feature selection, etc.)\n  and similar :ref:`data transformations <data-transforms>` similarly should\n  be learnt from a training set and applied to held-out data for prediction::\n\n    >>> from sklearn import preprocessing\n    >>> X_train, X_test, y_train, y_test = train_test_split(\n    ...     X, y, test_size=0.4, random_state=0)\n    >>> scaler = preprocessing.StandardScaler().fit(X_train)\n    >>> X_train_transformed = scaler.transform(X_train)\n    >>> clf = svm.SVC(C=1).fit(X_train_transformed, y_train)\n    >>> X_test_transformed = scaler.transform(X_test)\n    >>> clf.score(X_test_transformed, y_test)\n    0.9333\n\n  A :class:`Pipeline <sklearn.pipeline.Pipeline>` makes it easier to compose\n  estimators, providing this behavior under cross-validation::\n\n    >>> from sklearn.pipeline import make_pipeline\n    >>> clf = make_pipeline(preprocessing.StandardScaler(), svm.SVC(C=1))\n    >>> cross_val_score(clf, X, y, cv=cv)\n    array([0.977, 0.933, 0.955, 0.933, 0.977])\n\n  See :ref:`combining_estimators`.\n\n\n.. _multimetric_cross_validation:"
  },
  {
    "filename": "cross_validation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\cross_validation.rst.txt",
    "id": "cross_validation.rst.txt_chunk_3",
    "header": "The cross_validate function and multiple metric evaluation",
    "text": "The cross_validate function and multiple metric evaluation\n----------------------------------------------------------\n\nThe :func:`cross_validate` function differs from :func:`cross_val_score` in\ntwo ways:\n\n- It allows specifying multiple metrics for evaluation.\n\n- It returns a dict containing fit-times, score-times\n  (and optionally training scores, fitted estimators, train-test split indices)\n  in addition to the test score.\n\nFor single metric evaluation, where the scoring parameter is a string,\ncallable or None, the keys will be - ``['test_score', 'fit_time', 'score_time']``\n\nAnd for multiple metric evaluation, the return value is a dict with the\nfollowing keys -\n``['test_<scorer1_name>', 'test_<scorer2_name>', 'test_<scorer...>', 'fit_time', 'score_time']``\n\n``return_train_score`` is set to ``False`` by default to save computation time.\nTo evaluate the scores on the training set as well you need to set it to\n``True``. You may also retain the estimator fitted on each training set by\nsetting ``return_estimator=True``. Similarly, you may set\n`return_indices=True` to retain the training and testing indices used to split\nthe dataset into train and test sets for each cv split.\n\nThe multiple metrics can be specified either as a list, tuple or set of\npredefined scorer names::\n\n    >>> from sklearn.model_selection import cross_validate\n    >>> from sklearn.metrics import recall_score\n    >>> scoring = ['precision_macro', 'recall_macro']\n    >>> clf = svm.SVC(kernel='linear', C=1, random_state=0)\n    >>> scores = cross_validate(clf, X, y, scoring=scoring)\n    >>> sorted(scores.keys())\n    ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro']\n    >>> scores['test_recall_macro']\n    array([0.96, 1., 0.96, 0.96, 1.])\n\nOr as a dict mapping scorer name to a predefined or custom scoring function::\n\n    >>> from sklearn.metrics import make_scorer\n    >>> scoring = {'prec_macro': 'precision_macro',\n    ...            'rec_macro': make_scorer(recall_score, average='macro')}\n    >>> scores = cross_validate(clf, X, y, scoring=scoring,\n    ...                         cv=5, return_train_score=True)\n    >>> sorted(scores.keys())\n    ['fit_time', 'score_time', 'test_prec_macro', 'test_rec_macro',\n     'train_prec_macro', 'train_rec_macro']\n    >>> scores['train_rec_macro']\n    array([0.97, 0.97, 0.99, 0.98, 0.98])\n\nHere is an example of ``cross_validate`` using a single metric::\n\n    >>> scores = cross_validate(clf, X, y,\n    ...                         scoring='precision_macro', cv=5,\n    ...                         return_estimator=True)\n    >>> sorted(scores.keys())\n    ['estimator', 'fit_time', 'score_time', 'test_score']"
  },
  {
    "filename": "cross_validation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\cross_validation.rst.txt",
    "id": "cross_validation.rst.txt_chunk_4",
    "header": "Obtaining predictions by cross-validation",
    "text": "Obtaining predictions by cross-validation\n-----------------------------------------\n\nThe function :func:`cross_val_predict` has a similar interface to\n:func:`cross_val_score`, but returns, for each element in the input, the\nprediction that was obtained for that element when it was in the test set. Only\ncross-validation strategies that assign all elements to a test set exactly once\ncan be used (otherwise, an exception is raised).\n\n\n.. warning:: Note on inappropriate usage of cross_val_predict\n\n    The result of :func:`cross_val_predict` may be different from those\n    obtained using :func:`cross_val_score` as the elements are grouped in\n    different ways. The function :func:`cross_val_score` takes an average\n    over cross-validation folds, whereas :func:`cross_val_predict` simply\n    returns the labels (or probabilities) from several distinct models\n    undistinguished. Thus, :func:`cross_val_predict` is not an appropriate\n    measure of generalization error.\n\n\nThe function :func:`cross_val_predict` is appropriate for:\n  - Visualization of predictions obtained from different models.\n  - Model blending: When predictions of one supervised estimator are used to\n    train another estimator in ensemble methods.\n\n\nThe available cross validation iterators are introduced in the following\nsection.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_model_selection_plot_roc_crossval.py`,\n* :ref:`sphx_glr_auto_examples_feature_selection_plot_rfe_with_cross_validation.py`,\n* :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_digits.py`,\n* :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_text_feature_extraction.py`,\n* :ref:`sphx_glr_auto_examples_model_selection_plot_cv_predict.py`,\n* :ref:`sphx_glr_auto_examples_model_selection_plot_nested_cross_validation_iris.py`."
  },
  {
    "filename": "cross_validation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\cross_validation.rst.txt",
    "id": "cross_validation.rst.txt_chunk_5",
    "header": "Cross validation iterators",
    "text": "Cross validation iterators\n==========================\n\nThe following sections list utilities to generate indices\nthat can be used to generate dataset splits according to different cross\nvalidation strategies.\n\n.. _iid_cv:"
  },
  {
    "filename": "cross_validation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\cross_validation.rst.txt",
    "id": "cross_validation.rst.txt_chunk_6",
    "header": "Cross-validation iterators for i.i.d. data",
    "text": "Cross-validation iterators for i.i.d. data\n------------------------------------------\n\nAssuming that some data is Independent and Identically Distributed (i.i.d.) is\nmaking the assumption that all samples stem from the same generative process\nand that the generative process is assumed to have no memory of past generated\nsamples.\n\nThe following cross-validators can be used in such cases.\n\n.. note::\n\n  While i.i.d. data is a common assumption in machine learning theory, it rarely\n  holds in practice. If one knows that the samples have been generated using a\n  time-dependent process, it is safer to\n  use a :ref:`time-series aware cross-validation scheme <timeseries_cv>`.\n  Similarly, if we know that the generative process has a group structure\n  (samples collected from different subjects, experiments, measurement\n  devices), it is safer to use :ref:`group-wise cross-validation <group_cv>`.\n\n.. _k_fold:"
  },
  {
    "filename": "cross_validation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\cross_validation.rst.txt",
    "id": "cross_validation.rst.txt_chunk_7",
    "header": "K-fold",
    "text": "K-fold\n^^^^^^\n\n:class:`KFold` divides all the samples in :math:`k` groups of samples,\ncalled folds (if :math:`k = n`, this is equivalent to the *Leave One\nOut* strategy), of equal sizes (if possible). The prediction function is\nlearned using :math:`k - 1` folds, and the fold left out is used for test.\n\nExample of 2-fold cross-validation on a dataset with 4 samples::\n\n  >>> import numpy as np\n  >>> from sklearn.model_selection import KFold\n\n  >>> X = [\"a\", \"b\", \"c\", \"d\"]\n  >>> kf = KFold(n_splits=2)\n  >>> for train, test in kf.split(X):\n  ...     print(\"%s %s\" % (train, test))\n  [2 3] [0 1]\n  [0 1] [2 3]\n\nHere is a visualization of the cross-validation behavior. Note that\n:class:`KFold` is not affected by classes or groups.\n\n.. figure:: ../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_006.png\n   :target: ../auto_examples/model_selection/plot_cv_indices.html\n   :align: center\n   :scale: 75%\n\nEach fold is constituted by two arrays: the first one is related to the\n*training set*, and the second one to the *test set*.\nThus, one can create the training/test sets using numpy indexing::\n\n  >>> X = np.array([[0., 0.], [1., 1.], [-1., -1.], [2., 2.]])\n  >>> y = np.array([0, 1, 0, 1])\n  >>> X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]\n\n.. _repeated_k_fold:"
  },
  {
    "filename": "cross_validation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\cross_validation.rst.txt",
    "id": "cross_validation.rst.txt_chunk_8",
    "header": "Repeated K-Fold",
    "text": "Repeated K-Fold\n^^^^^^^^^^^^^^^\n\n:class:`RepeatedKFold` repeats :class:`KFold` :math:`n` times, producing different splits in\neach repetition.\n\nExample of 2-fold K-Fold repeated 2 times::\n\n  >>> import numpy as np\n  >>> from sklearn.model_selection import RepeatedKFold\n  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n  >>> random_state = 12883823\n  >>> rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=random_state)\n  >>> for train, test in rkf.split(X):\n  ...     print(\"%s %s\" % (train, test))\n  ...\n  [2 3] [0 1]\n  [0 1] [2 3]\n  [0 2] [1 3]\n  [1 3] [0 2]\n\n\nSimilarly, :class:`RepeatedStratifiedKFold` repeats :class:`StratifiedKFold` :math:`n` times\nwith different randomization in each repetition.\n\n.. _leave_one_out:"
  },
  {
    "filename": "cross_validation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\cross_validation.rst.txt",
    "id": "cross_validation.rst.txt_chunk_9",
    "header": "Leave One Out (LOO)",
    "text": "Leave One Out (LOO)\n^^^^^^^^^^^^^^^^^^^\n\n:class:`LeaveOneOut` (or LOO) is a simple cross-validation. Each learning\nset is created by taking all the samples except one, the test set being\nthe sample left out. Thus, for :math:`n` samples, we have :math:`n` different\ntraining sets and :math:`n` different test sets. This cross-validation\nprocedure does not waste much data as only one sample is removed from the\ntraining set::\n\n  >>> from sklearn.model_selection import LeaveOneOut\n\n  >>> X = [1, 2, 3, 4]\n  >>> loo = LeaveOneOut()\n  >>> for train, test in loo.split(X):\n  ...     print(\"%s %s\" % (train, test))\n  [1 2 3] [0]\n  [0 2 3] [1]\n  [0 1 3] [2]\n  [0 1 2] [3]\n\n\nPotential users of LOO for model selection should weigh a few known caveats.\nWhen compared with :math:`k`-fold cross validation, one builds :math:`n` models\nfrom :math:`n` samples instead of :math:`k` models, where :math:`n > k`.\nMoreover, each is trained on :math:`n - 1` samples rather than\n:math:`(k-1) n / k`. In both ways, assuming :math:`k` is not too large\nand :math:`k < n`, LOO is more computationally expensive than :math:`k`-fold\ncross validation.\n\nIn terms of accuracy, LOO often results in high variance as an estimator for the\ntest error. Intuitively, since :math:`n - 1` of\nthe :math:`n` samples are used to build each model, models constructed from\nfolds are virtually identical to each other and to the model built from the\nentire training set.\n\nHowever, if the learning curve is steep for the training size in question,\nthen 5 or 10-fold cross validation can overestimate the generalization error.\n\nAs a general rule, most authors and empirical evidence suggest that 5 or 10-fold\ncross validation should be preferred to LOO.\n\n.. dropdown:: References\n\n  * `<http://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-12.html>`_;\n  * T. Hastie, R. Tibshirani, J. Friedman,  `The Elements of Statistical Learning\n    <https://web.stanford.edu/~hastie/ElemStatLearn/>`_, Springer 2009\n  * L. Breiman, P. Spector `Submodel selection and evaluation in regression: The X-random case\n    <https://digitalassets.lib.berkeley.edu/sdtr/ucb/text/197.pdf>`_, International Statistical Review 1992;\n  * R. Kohavi, `A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection\n    <https://www.ijcai.org/Proceedings/95-2/Papers/016.pdf>`_, Intl. Jnt. Conf. AI\n  * R. Bharat Rao, G. Fung, R. Rosales, `On the Dangers of Cross-Validation. An Experimental Evaluation\n    <https://people.csail.mit.edu/romer/papers/CrossVal_SDM08.pdf>`_, SIAM 2008;\n  * G. James, D. Witten, T. Hastie, R. Tibshirani, `An Introduction to\n    Statistical Learning <https://www.statlearning.com>`_, Springer 2013.\n\n.. _leave_p_out:"
  },
  {
    "filename": "cross_validation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\cross_validation.rst.txt",
    "id": "cross_validation.rst.txt_chunk_10",
    "header": "Leave P Out (LPO)",
    "text": "Leave P Out (LPO)\n^^^^^^^^^^^^^^^^^\n\n:class:`LeavePOut` is very similar to :class:`LeaveOneOut` as it creates all\nthe possible training/test sets by removing :math:`p` samples from the complete\nset. For :math:`n` samples, this produces :math:`{n \\choose p}` train-test\npairs. Unlike :class:`LeaveOneOut` and :class:`KFold`, the test sets will\noverlap for :math:`p > 1`.\n\nExample of Leave-2-Out on a dataset with 4 samples::\n\n  >>> from sklearn.model_selection import LeavePOut\n\n  >>> X = np.ones(4)\n  >>> lpo = LeavePOut(p=2)\n  >>> for train, test in lpo.split(X):\n  ...     print(\"%s %s\" % (train, test))\n  [2 3] [0 1]\n  [1 3] [0 2]\n  [1 2] [0 3]\n  [0 3] [1 2]\n  [0 2] [1 3]\n  [0 1] [2 3]\n\n\n.. _ShuffleSplit:"
  },
  {
    "filename": "cross_validation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\cross_validation.rst.txt",
    "id": "cross_validation.rst.txt_chunk_11",
    "header": "Random permutations cross-validation a.k.a. Shuffle & Split",
    "text": "Random permutations cross-validation a.k.a. Shuffle & Split\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe :class:`ShuffleSplit` iterator will generate a user defined number of\nindependent train / test dataset splits. Samples are first shuffled and\nthen split into a pair of train and test sets.\n\nIt is possible to control the randomness for reproducibility of the\nresults by explicitly seeding the ``random_state`` pseudo random number\ngenerator.\n\nHere is a usage example::\n\n  >>> from sklearn.model_selection import ShuffleSplit\n  >>> X = np.arange(10)\n  >>> ss = ShuffleSplit(n_splits=5, test_size=0.25, random_state=0)\n  >>> for train_index, test_index in ss.split(X):\n  ...     print(\"%s %s\" % (train_index, test_index))\n  [9 1 6 7 3 0 5] [2 8 4]\n  [2 9 8 0 6 7 4] [3 5 1]\n  [4 5 1 0 6 9 7] [2 3 8]\n  [2 7 5 8 0 3 4] [6 1 9]\n  [4 1 0 6 8 9 3] [5 2 7]\n\nHere is a visualization of the cross-validation behavior. Note that\n:class:`ShuffleSplit` is not affected by classes or groups.\n\n.. figure:: ../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_008.png\n   :target: ../auto_examples/model_selection/plot_cv_indices.html\n   :align: center\n   :scale: 75%\n\n:class:`ShuffleSplit` is thus a good alternative to :class:`KFold` cross\nvalidation that allows a finer control on the number of iterations and\nthe proportion of samples on each side of the train / test split.\n\n.. _stratification:"
  },
  {
    "filename": "cross_validation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\cross_validation.rst.txt",
    "id": "cross_validation.rst.txt_chunk_12",
    "header": "Cross-validation iterators with stratification based on class labels",
    "text": "Cross-validation iterators with stratification based on class labels\n--------------------------------------------------------------------\n\nSome classification tasks can naturally exhibit rare classes: for instance,\nthere could be orders of magnitude more negative observations than positive\nobservations (e.g. medical screening, fraud detection, etc). As a result,\ncross-validation splitting can generate train or validation folds without any\noccurrence of a particular class. This typically leads to undefined\nclassification metrics (e.g. ROC AUC), exceptions raised when attempting to\ncall :term:`fit` or missing columns in the output of the `predict_proba` or\n`decision_function` methods of multiclass classifiers trained on different\nfolds.\n\nTo mitigate such problems, splitters such as :class:`StratifiedKFold` and\n:class:`StratifiedShuffleSplit` implement stratified sampling to ensure that\nrelative class frequencies are approximately preserved in each fold.\n\n.. note::\n\n  Stratified sampling was introduced in scikit-learn to workaround the\n  aforementioned engineering problems rather than solve a statistical one.\n\n  Stratification makes cross-validation folds more homogeneous, and as a result\n  hides some of the variability inherent to fitting models with a limited\n  number of observations.\n\n  As a result, stratification can artificially shrink the spread of the metric\n  measured across cross-validation iterations: the inter-fold variability does\n  no longer reflect the uncertainty in the performance of classifiers in the\n  presence of rare classes.\n\n.. _stratified_k_fold:"
  },
  {
    "filename": "cross_validation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\cross_validation.rst.txt",
    "id": "cross_validation.rst.txt_chunk_13",
    "header": "Stratified K-fold",
    "text": "Stratified K-fold\n^^^^^^^^^^^^^^^^^\n\n:class:`StratifiedKFold` is a variation of *K-fold* which returns *stratified*\nfolds: each set contains approximately the same percentage of samples of each\ntarget class as the complete set.\n\nHere is an example of stratified 3-fold cross-validation on a dataset with 50 samples from\ntwo unbalanced classes.  We show the number of samples in each class and compare with\n:class:`KFold`.\n\n  >>> from sklearn.model_selection import StratifiedKFold, KFold\n  >>> import numpy as np\n  >>> X, y = np.ones((50, 1)), np.hstack(([0] * 45, [1] * 5))\n  >>> skf = StratifiedKFold(n_splits=3)\n  >>> for train, test in skf.split(X, y):\n  ...     print('train -  {}   |   test -  {}'.format(\n  ...         np.bincount(y[train]), np.bincount(y[test])))\n  train -  [30  3]   |   test -  [15  2]\n  train -  [30  3]   |   test -  [15  2]\n  train -  [30  4]   |   test -  [15  1]\n  >>> kf = KFold(n_splits=3)\n  >>> for train, test in kf.split(X, y):\n  ...     print('train -  {}   |   test -  {}'.format(\n  ...         np.bincount(y[train]), np.bincount(y[test])))\n  train -  [28  5]   |   test -  [17]\n  train -  [28  5]   |   test -  [17]\n  train -  [34]   |   test -  [11  5]\n\nWe can see that :class:`StratifiedKFold` preserves the class ratios\n(approximately 1 / 10) in both train and test datasets.\n\nHere is a visualization of the cross-validation behavior.\n\n.. figure:: ../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_009.png\n   :target: ../auto_examples/model_selection/plot_cv_indices.html\n   :align: center\n   :scale: 75%\n\n:class:`RepeatedStratifiedKFold` can be used to repeat Stratified K-Fold n times\nwith different randomization in each repetition.\n\n.. _stratified_shuffle_split:"
  },
  {
    "filename": "cross_validation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\cross_validation.rst.txt",
    "id": "cross_validation.rst.txt_chunk_14",
    "header": "Stratified Shuffle Split",
    "text": "Stratified Shuffle Split\n^^^^^^^^^^^^^^^^^^^^^^^^\n\n:class:`StratifiedShuffleSplit` is a variation of *ShuffleSplit*, which returns\nstratified splits, *i.e.* which creates splits by preserving the same\npercentage for each target class as in the complete set.\n\nHere is a visualization of the cross-validation behavior.\n\n.. figure:: ../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_012.png\n   :target: ../auto_examples/model_selection/plot_cv_indices.html\n   :align: center\n   :scale: 75%\n\n.. _predefined_split:"
  },
  {
    "filename": "cross_validation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\cross_validation.rst.txt",
    "id": "cross_validation.rst.txt_chunk_15",
    "header": "Predefined fold-splits / Validation-sets",
    "text": "Predefined fold-splits / Validation-sets\n----------------------------------------\n\nFor some datasets, a pre-defined split of the data into training- and\nvalidation fold or into several cross-validation folds already\nexists. Using :class:`PredefinedSplit` it is possible to use these folds\ne.g. when searching for hyperparameters.\n\nFor example, when using a validation set, set the ``test_fold`` to 0 for all\nsamples that are part of the validation set, and to -1 for all other samples.\n\n.. _group_cv:"
  },
  {
    "filename": "cross_validation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\cross_validation.rst.txt",
    "id": "cross_validation.rst.txt_chunk_16",
    "header": "Cross-validation iterators for grouped data",
    "text": "Cross-validation iterators for grouped data\n-------------------------------------------\n\nThe i.i.d. assumption is broken if the underlying generative process yields\ngroups of dependent samples.\n\nSuch a grouping of data is domain specific. An example would be when there is\nmedical data collected from multiple patients, with multiple samples taken from\neach patient. And such data is likely to be dependent on the individual group.\nIn our example, the patient id for each sample will be its group identifier.\n\nIn this case we would like to know if a model trained on a particular set of\ngroups generalizes well to the unseen groups. To measure this, we need to\nensure that all the samples in the validation fold come from groups that are\nnot represented at all in the paired training fold.\n\nThe following cross-validation splitters can be used to do that.\nThe grouping identifier for the samples is specified via the ``groups``\nparameter.\n\n.. _group_k_fold:"
  },
  {
    "filename": "cross_validation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\cross_validation.rst.txt",
    "id": "cross_validation.rst.txt_chunk_17",
    "header": "Group K-fold",
    "text": "Group K-fold\n^^^^^^^^^^^^\n\n:class:`GroupKFold` is a variation of K-fold which ensures that the same group is\nnot represented in both testing and training sets. For example if the data is\nobtained from different subjects with several samples per-subject and if the\nmodel is flexible enough to learn from highly person specific features it\ncould fail to generalize to new subjects. :class:`GroupKFold` makes it possible\nto detect this kind of overfitting situations.\n\nImagine you have three subjects, each with an associated number from 1 to 3::\n\n  >>> from sklearn.model_selection import GroupKFold\n\n  >>> X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 8.8, 9, 10]\n  >>> y = [\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\", \"d\", \"d\", \"d\"]\n  >>> groups = [1, 1, 1, 2, 2, 2, 3, 3, 3, 3]\n\n  >>> gkf = GroupKFold(n_splits=3)\n  >>> for train, test in gkf.split(X, y, groups=groups):\n  ...     print(\"%s %s\" % (train, test))\n  [0 1 2 3 4 5] [6 7 8 9]\n  [0 1 2 6 7 8 9] [3 4 5]\n  [3 4 5 6 7 8 9] [0 1 2]\n\nEach subject is in a different testing fold, and the same subject is never in\nboth testing and training. Notice that the folds do not have exactly the same\nsize due to the imbalance in the data. If class proportions must be balanced\nacross folds, :class:`StratifiedGroupKFold` is a better option.\n\nHere is a visualization of the cross-validation behavior.\n\n.. figure:: ../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_007.png\n   :target: ../auto_examples/model_selection/plot_cv_indices.html\n   :align: center\n   :scale: 75%\n\nSimilar to :class:`KFold`, the test sets from :class:`GroupKFold` will form a\ncomplete partition of all the data.\n\nWhile :class:`GroupKFold` attempts to place the same number of samples in each\nfold when ``shuffle=False``, when ``shuffle=True`` it attempts to place an equal\nnumber of distinct groups in each fold (but does not account for group sizes).\n\n.. _stratified_group_k_fold:"
  },
  {
    "filename": "cross_validation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\cross_validation.rst.txt",
    "id": "cross_validation.rst.txt_chunk_18",
    "header": "StratifiedGroupKFold",
    "text": "StratifiedGroupKFold\n^^^^^^^^^^^^^^^^^^^^\n\n:class:`StratifiedGroupKFold` is a cross-validation scheme that combines both\n:class:`StratifiedKFold` and :class:`GroupKFold`. The idea is to try to\npreserve the distribution of classes in each split while keeping each group\nwithin a single split. That might be useful when you have an unbalanced\ndataset so that using just :class:`GroupKFold` might produce skewed splits.\n\nExample::\n\n  >>> from sklearn.model_selection import StratifiedGroupKFold\n  >>> X = list(range(18))\n  >>> y = [1] * 6 + [0] * 12\n  >>> groups = [1, 2, 3, 3, 4, 4, 1, 1, 2, 2, 3, 4, 5, 5, 5, 6, 6, 6]\n  >>> sgkf = StratifiedGroupKFold(n_splits=3)\n  >>> for train, test in sgkf.split(X, y, groups=groups):\n  ...     print(\"%s %s\" % (train, test))\n  [ 0  2  3  4  5  6  7 10 11 15 16 17] [ 1  8  9 12 13 14]\n  [ 0  1  4  5  6  7  8  9 11 12 13 14] [ 2  3 10 15 16 17]\n  [ 1  2  3  8  9 10 12 13 14 15 16 17] [ 0  4  5  6  7 11]\n\n.. dropdown:: Implementation notes\n\n  - With the current implementation full shuffle is not possible in most\n    scenarios. When shuffle=True, the following happens:\n\n    1. All groups are shuffled.\n    2. Groups are sorted by standard deviation of classes using stable sort.\n    3. Sorted groups are iterated over and assigned to folds.\n\n    That means that only groups with the same standard deviation of class\n    distribution will be shuffled, which might be useful when each group has only\n    a single class.\n  - The algorithm greedily assigns each group to one of n_splits test sets,\n    choosing the test set that minimises the variance in class distribution\n    across test sets. Group assignment proceeds from groups with highest to\n    lowest variance in class frequency, i.e. large groups peaked on one or few\n    classes are assigned first.\n  - This split is suboptimal in a sense that it might produce imbalanced splits\n    even if perfect stratification is possible. If you have relatively close\n    distribution of classes in each group, using :class:`GroupKFold` is better.\n\n\nHere is a visualization of cross-validation behavior for uneven groups:\n\n.. figure:: ../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_005.png\n   :target: ../auto_examples/model_selection/plot_cv_indices.html\n   :align: center\n   :scale: 75%\n\n.. _leave_one_group_out:"
  },
  {
    "filename": "cross_validation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\cross_validation.rst.txt",
    "id": "cross_validation.rst.txt_chunk_19",
    "header": "Leave One Group Out",
    "text": "Leave One Group Out\n^^^^^^^^^^^^^^^^^^^\n\n:class:`LeaveOneGroupOut` is a cross-validation scheme where each split holds\nout samples belonging to one specific group. Group information is\nprovided via an array that encodes the group of each sample.\n\nEach training set is thus constituted by all the samples except the ones\nrelated to a specific group. This is the same as :class:`LeavePGroupsOut` with\n`n_groups=1` and the same as :class:`GroupKFold` with `n_splits` equal to the\nnumber of unique labels passed to the `groups` parameter.\n\nFor example, in the cases of multiple experiments, :class:`LeaveOneGroupOut`\ncan be used to create a cross-validation based on the different experiments:\nwe create a training set using the samples of all the experiments except one::\n\n  >>> from sklearn.model_selection import LeaveOneGroupOut\n\n  >>> X = [1, 5, 10, 50, 60, 70, 80]\n  >>> y = [0, 1, 1, 2, 2, 2, 2]\n  >>> groups = [1, 1, 2, 2, 3, 3, 3]\n  >>> logo = LeaveOneGroupOut()\n  >>> for train, test in logo.split(X, y, groups=groups):\n  ...     print(\"%s %s\" % (train, test))\n  [2 3 4 5 6] [0 1]\n  [0 1 4 5 6] [2 3]\n  [0 1 2 3] [4 5 6]\n\nAnother common application is to use time information: for instance the\ngroups could be the year of collection of the samples and thus allow\nfor cross-validation against time-based splits.\n\n.. _leave_p_groups_out:"
  },
  {
    "filename": "cross_validation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\cross_validation.rst.txt",
    "id": "cross_validation.rst.txt_chunk_20",
    "header": "Leave P Groups Out",
    "text": "Leave P Groups Out\n^^^^^^^^^^^^^^^^^^\n\n:class:`LeavePGroupsOut` is similar to :class:`LeaveOneGroupOut`, but removes\nsamples related to :math:`P` groups for each training/test set. All possible\ncombinations of :math:`P` groups are left out, meaning test sets will overlap\nfor :math:`P>1`.\n\nExample of Leave-2-Group Out::\n\n  >>> from sklearn.model_selection import LeavePGroupsOut\n\n  >>> X = np.arange(6)\n  >>> y = [1, 1, 1, 2, 2, 2]\n  >>> groups = [1, 1, 2, 2, 3, 3]\n  >>> lpgo = LeavePGroupsOut(n_groups=2)\n  >>> for train, test in lpgo.split(X, y, groups=groups):\n  ...     print(\"%s %s\" % (train, test))\n  [4 5] [0 1 2 3]\n  [2 3] [0 1 4 5]\n  [0 1] [2 3 4 5]\n\n.. _group_shuffle_split:"
  },
  {
    "filename": "cross_validation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\cross_validation.rst.txt",
    "id": "cross_validation.rst.txt_chunk_21",
    "header": "Group Shuffle Split",
    "text": "Group Shuffle Split\n^^^^^^^^^^^^^^^^^^^\n\nThe :class:`GroupShuffleSplit` iterator behaves as a combination of\n:class:`ShuffleSplit` and :class:`LeavePGroupsOut`, and generates a\nsequence of randomized partitions in which a subset of groups are held\nout for each split. Each train/test split is performed independently meaning\nthere is no guaranteed relationship between successive test sets.\n\nHere is a usage example::\n\n  >>> from sklearn.model_selection import GroupShuffleSplit\n\n  >>> X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 0.001]\n  >>> y = [\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\", \"a\"]\n  >>> groups = [1, 1, 2, 2, 3, 3, 4, 4]\n  >>> gss = GroupShuffleSplit(n_splits=4, test_size=0.5, random_state=0)\n  >>> for train, test in gss.split(X, y, groups=groups):\n  ...     print(\"%s %s\" % (train, test))\n  ...\n  [0 1 2 3] [4 5 6 7]\n  [2 3 6 7] [0 1 4 5]\n  [2 3 4 5] [0 1 6 7]\n  [4 5 6 7] [0 1 2 3]\n\nHere is a visualization of the cross-validation behavior.\n\n.. figure:: ../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_011.png\n   :target: ../auto_examples/model_selection/plot_cv_indices.html\n   :align: center\n   :scale: 75%\n\nThis class is useful when the behavior of :class:`LeavePGroupsOut` is\ndesired, but the number of groups is large enough that generating all\npossible partitions with :math:`P` groups withheld would be prohibitively\nexpensive. In such a scenario, :class:`GroupShuffleSplit` provides\na random sample (with replacement) of the train / test splits\ngenerated by :class:`LeavePGroupsOut`."
  },
  {
    "filename": "cross_validation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\cross_validation.rst.txt",
    "id": "cross_validation.rst.txt_chunk_22",
    "header": "Using cross-validation iterators to split train and test",
    "text": "Using cross-validation iterators to split train and test\n--------------------------------------------------------\n\nThe above group cross-validation functions may also be useful for splitting a\ndataset into training and testing subsets. Note that the convenience\nfunction :func:`train_test_split` is a wrapper around :func:`ShuffleSplit`\nand thus only allows for stratified splitting (using the class labels)\nand cannot account for groups.\n\nTo perform the train and test split, use the indices for the train and test\nsubsets yielded by the generator output by the `split()` method of the\ncross-validation splitter. For example::\n\n  >>> import numpy as np\n  >>> from sklearn.model_selection import GroupShuffleSplit\n\n  >>> X = np.array([0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 0.001])\n  >>> y = np.array([\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\", \"a\"])\n  >>> groups = np.array([1, 1, 2, 2, 3, 3, 4, 4])\n  >>> train_indx, test_indx = next(\n  ...     GroupShuffleSplit(random_state=7).split(X, y, groups)\n  ... )\n  >>> X_train, X_test, y_train, y_test = \\\n  ...     X[train_indx], X[test_indx], y[train_indx], y[test_indx]\n  >>> X_train.shape, X_test.shape\n  ((6,), (2,))\n  >>> np.unique(groups[train_indx]), np.unique(groups[test_indx])\n  (array([1, 2, 4]), array([3]))\n\n.. _timeseries_cv:"
  },
  {
    "filename": "cross_validation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\cross_validation.rst.txt",
    "id": "cross_validation.rst.txt_chunk_23",
    "header": "Cross validation of time series data",
    "text": "Cross validation of time series data\n------------------------------------\n\nTime series data is characterized by the correlation between observations\nthat are near in time (*autocorrelation*). However, classical\ncross-validation techniques such as :class:`KFold` and\n:class:`ShuffleSplit` assume the samples are independent and\nidentically distributed, and would result in unreasonable correlation\nbetween training and testing instances (yielding poor estimates of\ngeneralization error) on time series data. Therefore, it is very important\nto evaluate our model for time series data on the \"future\" observations\nleast like those that are used to train the model. To achieve this, one\nsolution is provided by :class:`TimeSeriesSplit`.\n\n.. _time_series_split:"
  },
  {
    "filename": "cross_validation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\cross_validation.rst.txt",
    "id": "cross_validation.rst.txt_chunk_24",
    "header": "Time Series Split",
    "text": "Time Series Split\n^^^^^^^^^^^^^^^^^\n\n:class:`TimeSeriesSplit` is a variation of *k-fold* which\nreturns first :math:`k` folds as train set and the :math:`(k+1)` th\nfold as test set. Note that unlike standard cross-validation methods,\nsuccessive training sets are supersets of those that come before them.\nAlso, it adds all surplus data to the first training partition, which\nis always used to train the model.\n\nThis class can be used to cross-validate time series data samples\nthat are observed at fixed time intervals. Indeed, the folds must\nrepresent the same duration, in order to have comparable metrics across folds.\n\nExample of 3-split time series cross-validation on a dataset with 6 samples::\n\n  >>> from sklearn.model_selection import TimeSeriesSplit\n\n  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n  >>> y = np.array([1, 2, 3, 4, 5, 6])\n  >>> tscv = TimeSeriesSplit(n_splits=3)\n  >>> print(tscv)\n  TimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None)\n  >>> for train, test in tscv.split(X):\n  ...     print(\"%s %s\" % (train, test))\n  [0 1 2] [3]\n  [0 1 2 3] [4]\n  [0 1 2 3 4] [5]\n\nHere is a visualization of the cross-validation behavior.\n\n.. figure:: ../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_013.png\n   :target: ../auto_examples/model_selection/plot_cv_indices.html\n   :align: center\n   :scale: 75%"
  },
  {
    "filename": "cross_validation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\cross_validation.rst.txt",
    "id": "cross_validation.rst.txt_chunk_25",
    "header": "A note on shuffling",
    "text": "A note on shuffling\n===================\n\nIf the data ordering is not arbitrary (e.g. samples with the same class label\nare contiguous), shuffling it first may be essential to get a meaningful\ncross-validation result. However, the opposite may be true if the samples are not\nindependently and identically distributed. For example, if samples correspond\nto news articles, and are ordered by their time of publication, then shuffling\nthe data will likely lead to a model that is overfit and an inflated validation\nscore: it will be tested on samples that are artificially similar (close in\ntime) to training samples.\n\nSome cross validation iterators, such as :class:`KFold`, have an inbuilt option\nto shuffle the data indices before splitting them. Note that:\n\n* This consumes less memory than shuffling the data directly.\n* By default no shuffling occurs, including for the (stratified) K fold\n  cross-validation performed by specifying ``cv=some_integer`` to\n  :func:`cross_val_score`, grid search, etc. Keep in mind that\n  :func:`train_test_split` still returns a random split.\n* The ``random_state`` parameter defaults to ``None``, meaning that the\n  shuffling will be different every time ``KFold(..., shuffle=True)`` is\n  iterated. However, ``GridSearchCV`` will use the same shuffling for each set\n  of parameters validated by a single call to its ``fit`` method.\n* To get identical results for each split, set ``random_state`` to an integer.\n\nFor more details on how to control the randomness of cv splitters and avoid\ncommon pitfalls, see :ref:`randomness`."
  },
  {
    "filename": "cross_validation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\cross_validation.rst.txt",
    "id": "cross_validation.rst.txt_chunk_26",
    "header": "Cross validation and model selection",
    "text": "Cross validation and model selection\n====================================\n\nCross validation iterators can also be used to directly perform model\nselection using Grid Search for the optimal hyperparameters of the\nmodel. This is the topic of the next section: :ref:`grid_search`.\n\n.. _permutation_test_score:"
  },
  {
    "filename": "cross_validation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\cross_validation.rst.txt",
    "id": "cross_validation.rst.txt_chunk_27",
    "header": "Permutation test score",
    "text": "Permutation test score\n======================\n\n:func:`~sklearn.model_selection.permutation_test_score` offers another way\nto evaluate the performance of a :term:`predictor`. It provides a\npermutation-based p-value, which represents how likely an observed performance of the\nestimator would be obtained by chance. The null hypothesis in this test is\nthat the estimator fails to leverage any statistical dependency between the\nfeatures and the targets to make correct predictions on left-out data.\n:func:`~sklearn.model_selection.permutation_test_score` generates a null\ndistribution by calculating `n_permutations` different permutations of the\ndata. In each permutation the target values are randomly shuffled, thereby removing\nany dependency between the features and the targets. The p-value output is the fraction\nof permutations whose cross-validation score is better or equal than the true score\nwithout permuting targets. For reliable results ``n_permutations`` should typically be\nlarger than 100 and ``cv`` between 3-10 folds.\n\nA low p-value provides evidence that the dataset contains some real dependency between\nfeatures and targets **and** that the estimator was able to utilize this dependency to\nobtain good results. A high p-value, in reverse, could be due to either one of these:\n\n- a lack of dependency between features and targets (i.e., there is no systematic\n  relationship and any observed patterns are likely due to random chance)\n- **or** because the estimator was not able to use the dependency in the data (for\n  instance because it underfit).\n\nIn the latter case, using a more appropriate estimator that is able to use the\nstructure in the data, would result in a lower p-value.\n\nCross-validation provides information about how well an estimator generalizes\nby estimating the range of its expected scores. However, an\nestimator trained on a high dimensional dataset with no structure may still\nperform better than expected on cross-validation, just by chance.\nThis can typically happen with small datasets with less than a few hundred\nsamples.\n:func:`~sklearn.model_selection.permutation_test_score` provides information\non whether the estimator has found a real dependency between features and targets and\ncan help in evaluating the performance of the estimator.\n\nIt is important to note that this test has been shown to produce low\np-values even if there is only weak structure in the data because in the\ncorresponding permutated datasets there is absolutely no structure. This\ntest is therefore only able to show whether the model reliably outperforms\nrandom guessing.\n\nFinally, :func:`~sklearn.model_selection.permutation_test_score` is computed\nusing brute force and internally fits ``(n_permutations + 1) * n_cv`` models.\nIt is therefore only tractable with small datasets for which fitting an\nindividual model is very fast. Using the `n_jobs` parameter parallelizes the\ncomputation and thus speeds it up.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_model_selection_plot_permutation_tests_for_classification.py`\n\n.. dropdown:: References\n\n  * Ojala and Garriga. `Permutation Tests for Studying Classifier Performance\n    <http://www.jmlr.org/papers/volume11/ojala10a/ojala10a.pdf>`_.\n    J. Mach. Learn. Res. 2010."
  },
  {
    "filename": "decomposition.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\decomposition.rst.txt",
    "id": "decomposition.rst.txt_chunk_0",
    "header": ".. _decompositions:",
    "text": ".. _decompositions:\n\n\n================================================================="
  },
  {
    "filename": "decomposition.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\decomposition.rst.txt",
    "id": "decomposition.rst.txt_chunk_1",
    "header": "Decomposing signals in components (matrix factorization problems)",
    "text": "Decomposing signals in components (matrix factorization problems)\n=================================================================\n\n.. currentmodule:: sklearn.decomposition\n\n\n.. _PCA:"
  },
  {
    "filename": "decomposition.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\decomposition.rst.txt",
    "id": "decomposition.rst.txt_chunk_2",
    "header": "Principal component analysis (PCA)",
    "text": "Principal component analysis (PCA)\n=================================="
  },
  {
    "filename": "decomposition.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\decomposition.rst.txt",
    "id": "decomposition.rst.txt_chunk_3",
    "header": "Exact PCA and probabilistic interpretation",
    "text": "Exact PCA and probabilistic interpretation\n------------------------------------------\n\nPCA is used to decompose a multivariate dataset in a set of successive\northogonal components that explain a maximum amount of the variance. In\nscikit-learn, :class:`PCA` is implemented as a *transformer* object\nthat learns :math:`n` components in its ``fit`` method, and can be used on new\ndata to project it on these components.\n\nPCA centers but does not scale the input data for each feature before\napplying the SVD. The optional parameter ``whiten=True`` makes it\npossible to project the data onto the singular space while scaling each\ncomponent to unit variance. This is often useful if the models down-stream make\nstrong assumptions on the isotropy of the signal: this is for example the case\nfor Support Vector Machines with the RBF kernel and the K-Means clustering\nalgorithm.\n\nBelow is an example of the iris dataset, which is comprised of 4\nfeatures, projected on the 2 dimensions that explain most variance:\n\n.. figure:: ../auto_examples/decomposition/images/sphx_glr_plot_pca_vs_lda_001.png\n    :target: ../auto_examples/decomposition/plot_pca_vs_lda.html\n    :align: center\n    :scale: 75%\n\n\nThe :class:`PCA` object also provides a\nprobabilistic interpretation of the PCA that can give a likelihood of\ndata based on the amount of variance it explains. As such it implements a\n:term:`score` method that can be used in cross-validation:\n\n.. figure:: ../auto_examples/decomposition/images/sphx_glr_plot_pca_vs_fa_model_selection_001.png\n    :target: ../auto_examples/decomposition/plot_pca_vs_fa_model_selection.html\n    :align: center\n    :scale: 75%\n\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_decomposition_plot_pca_iris.py`\n* :ref:`sphx_glr_auto_examples_decomposition_plot_pca_vs_lda.py`\n* :ref:`sphx_glr_auto_examples_decomposition_plot_pca_vs_fa_model_selection.py`\n\n\n.. _IncrementalPCA:"
  },
  {
    "filename": "decomposition.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\decomposition.rst.txt",
    "id": "decomposition.rst.txt_chunk_4",
    "header": "Incremental PCA",
    "text": "Incremental PCA\n---------------\n\nThe :class:`PCA` object is very useful, but has certain limitations for\nlarge datasets. The biggest limitation is that :class:`PCA` only supports\nbatch processing, which means all of the data to be processed must fit in main\nmemory. The :class:`IncrementalPCA` object uses a different form of\nprocessing and allows for partial computations which almost\nexactly match the results of :class:`PCA` while processing the data in a\nminibatch fashion. :class:`IncrementalPCA` makes it possible to implement\nout-of-core Principal Component Analysis either by:\n\n* Using its ``partial_fit`` method on chunks of data fetched sequentially\n  from the local hard drive or a network database.\n\n* Calling its fit method on a memory mapped file using\n  ``numpy.memmap``.\n\n:class:`IncrementalPCA` only stores estimates of component and noise variances,\nin order to update ``explained_variance_ratio_`` incrementally. This is why\nmemory usage depends on the number of samples per batch, rather than the\nnumber of samples to be processed in the dataset.\n\nAs in :class:`PCA`, :class:`IncrementalPCA` centers but does not scale the\ninput data for each feature before applying the SVD.\n\n.. figure:: ../auto_examples/decomposition/images/sphx_glr_plot_incremental_pca_001.png\n    :target: ../auto_examples/decomposition/plot_incremental_pca.html\n    :align: center\n    :scale: 75%\n\n.. figure:: ../auto_examples/decomposition/images/sphx_glr_plot_incremental_pca_002.png\n    :target: ../auto_examples/decomposition/plot_incremental_pca.html\n    :align: center\n    :scale: 75%\n\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_decomposition_plot_incremental_pca.py`\n\n\n.. _RandomizedPCA:"
  },
  {
    "filename": "decomposition.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\decomposition.rst.txt",
    "id": "decomposition.rst.txt_chunk_5",
    "header": "PCA using randomized SVD",
    "text": "PCA using randomized SVD\n------------------------\n\nIt is often interesting to project data to a lower-dimensional\nspace that preserves most of the variance, by dropping the singular vector\nof components associated with lower singular values.\n\nFor instance, if we work with 64x64 pixel gray-level pictures\nfor face recognition,\nthe dimensionality of the data is 4096 and it is slow to train an\nRBF support vector machine on such wide data. Furthermore we know that\nthe intrinsic dimensionality of the data is much lower than 4096 since all\npictures of human faces look somewhat alike.\nThe samples lie on a manifold of much lower\ndimension (say around 200 for instance). The PCA algorithm can be used\nto linearly transform the data while both reducing the dimensionality\nand preserving most of the explained variance at the same time.\n\nThe class :class:`PCA` used with the optional parameter\n``svd_solver='randomized'`` is very useful in that case: since we are going\nto drop most of the singular vectors it is much more efficient to limit the\ncomputation to an approximated estimate of the singular vectors we will keep\nto actually perform the transform.\n\nFor instance, the following shows 16 sample portraits (centered around\n0.0) from the Olivetti dataset. On the right hand side are the first 16\nsingular vectors reshaped as portraits. Since we only require the top\n16 singular vectors of a dataset with size :math:`n_{samples} = 400`\nand :math:`n_{features} = 64 \\times 64 = 4096`, the computation time is\nless than 1s:\n\n.. |orig_img| image:: ../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_001.png\n   :target: ../auto_examples/decomposition/plot_faces_decomposition.html\n   :scale: 60%\n\n.. |pca_img| image:: ../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_002.png\n   :target: ../auto_examples/decomposition/plot_faces_decomposition.html\n   :scale: 60%\n\n.. centered:: |orig_img| |pca_img|\n\nIf we note :math:`n_{\\max} = \\max(n_{\\mathrm{samples}}, n_{\\mathrm{features}})` and\n:math:`n_{\\min} = \\min(n_{\\mathrm{samples}}, n_{\\mathrm{features}})`, the time complexity\nof the randomized :class:`PCA` is :math:`O(n_{\\max}^2 \\cdot n_{\\mathrm{components}})`\ninstead of :math:`O(n_{\\max}^2 \\cdot n_{\\min})` for the exact method\nimplemented in :class:`PCA`.\n\nThe memory footprint of randomized :class:`PCA` is also proportional to\n:math:`2 \\cdot n_{\\max} \\cdot n_{\\mathrm{components}}` instead of :math:`n_{\\max}\n\\cdot n_{\\min}` for the exact method.\n\nNote: the implementation of ``inverse_transform`` in :class:`PCA` with\n``svd_solver='randomized'`` is not the exact inverse transform of\n``transform`` even when ``whiten=False`` (default).\n\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_applications_plot_face_recognition.py`\n* :ref:`sphx_glr_auto_examples_decomposition_plot_faces_decomposition.py`\n\n.. rubric:: References\n\n* Algorithm 4.3 in\n  :arxiv:`\"Finding structure with randomness: Stochastic algorithms for\n  constructing approximate matrix decompositions\" <0909.4061>`\n  Halko, et al., 2009\n\n* :arxiv:`\"An implementation of a randomized algorithm for principal component\n  analysis\" <1412.3510>` A. Szlam et al. 2014\n\n.. _SparsePCA:"
  },
  {
    "filename": "decomposition.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\decomposition.rst.txt",
    "id": "decomposition.rst.txt_chunk_6",
    "header": "Sparse principal components analysis (SparsePCA and MiniBatchSparsePCA)",
    "text": "Sparse principal components analysis (SparsePCA and MiniBatchSparsePCA)\n-----------------------------------------------------------------------\n\n:class:`SparsePCA` is a variant of PCA, with the goal of extracting the\nset of sparse components that best reconstruct the data.\n\nMini-batch sparse PCA (:class:`MiniBatchSparsePCA`) is a variant of\n:class:`SparsePCA` that is faster but less accurate. The increased speed is\nreached by iterating over small chunks of the set of features, for a given\nnumber of iterations.\n\n\nPrincipal component analysis (:class:`PCA`) has the disadvantage that the\ncomponents extracted by this method have exclusively dense expressions, i.e.\nthey have non-zero coefficients when expressed as linear combinations of the\noriginal variables. This can make interpretation difficult. In many cases,\nthe real underlying components can be more naturally imagined as sparse\nvectors; for example in face recognition, components might naturally map to\nparts of faces.\n\nSparse principal components yield a more parsimonious, interpretable\nrepresentation, clearly emphasizing which of the original features contribute\nto the differences between samples.\n\nThe following example illustrates 16 components extracted using sparse PCA from\nthe Olivetti faces dataset.  It can be seen how the regularization term induces\nmany zeros. Furthermore, the natural structure of the data causes the non-zero\ncoefficients to be vertically adjacent. The model does not enforce this\nmathematically: each component is a vector :math:`h \\in \\mathbf{R}^{4096}`, and\nthere is no notion of vertical adjacency except during the human-friendly\nvisualization as 64x64 pixel images. The fact that the components shown below\nappear local is the effect of the inherent structure of the data, which makes\nsuch local patterns minimize reconstruction error. There exist sparsity-inducing\nnorms that take into account adjacency and different kinds of structure; see\n[Jen09]_ for a review of such methods.\nFor more details on how to use Sparse PCA, see the Examples section, below.\n\n\n.. |spca_img| image:: ../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_005.png\n   :target: ../auto_examples/decomposition/plot_faces_decomposition.html\n   :scale: 60%\n\n.. centered:: |pca_img| |spca_img|\n\nNote that there are many different formulations for the Sparse PCA\nproblem. The one implemented here is based on [Mrl09]_ . The optimization\nproblem solved is a PCA problem (dictionary learning) with an\n:math:`\\ell_1` penalty on the components:\n\n.. math::\n   (U^*, V^*) = \\underset{U, V}{\\operatorname{arg\\,min\\,}} & \\frac{1}{2}\n                ||X-UV||_{\\text{Fro}}^2+\\alpha||V||_{1,1} \\\\\n                \\text{subject to } & ||U_k||_2 \\leq 1 \\text{ for all }\n                0 \\leq k < n_{components}\n\n:math:`||.||_{\\text{Fro}}` stands for the Frobenius norm and :math:`||.||_{1,1}`\nstands for the entry-wise matrix norm which is the sum of the absolute values\nof all the entries in the matrix.\nThe sparsity-inducing :math:`||.||_{1,1}` matrix norm also prevents learning\ncomponents from noise when few training samples are available. The degree\nof penalization (and thus sparsity) can be adjusted through the\nhyperparameter ``alpha``. Small values lead to a gently regularized\nfactorization, while larger values shrink many coefficients to zero.\n\n.. note::\n\n  While in the spirit of an online algorithm, the class\n  :class:`MiniBatchSparsePCA` does not implement ``partial_fit`` because\n  the algorithm is online along the features direction, not the samples\n  direction.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_decomposition_plot_faces_decomposition.py`\n\n.. rubric:: References\n\n.. [Mrl09] `\"Online Dictionary Learning for Sparse Coding\"\n   <https://www.di.ens.fr/~fbach/mairal_icml09.pdf>`_\n   J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009\n.. [Jen09] `\"Structured Sparse Principal Component Analysis\"\n   <https://www.di.ens.fr/~fbach/sspca_AISTATS2010.pdf>`_\n   R. Jenatton, G. Obozinski, F. Bach, 2009\n\n\n.. _kernel_PCA:"
  },
  {
    "filename": "decomposition.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\decomposition.rst.txt",
    "id": "decomposition.rst.txt_chunk_7",
    "header": "Kernel Principal Component Analysis (kPCA)",
    "text": "Kernel Principal Component Analysis (kPCA)\n=========================================="
  },
  {
    "filename": "decomposition.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\decomposition.rst.txt",
    "id": "decomposition.rst.txt_chunk_8",
    "header": "Exact Kernel PCA",
    "text": "Exact Kernel PCA\n----------------\n\n:class:`KernelPCA` is an extension of PCA which achieves non-linear\ndimensionality reduction through the use of kernels (see :ref:`metrics`) [Scholkopf1997]_. It\nhas many applications including denoising, compression and structured\nprediction (kernel dependency estimation). :class:`KernelPCA` supports both\n``transform`` and ``inverse_transform``.\n\n.. figure:: ../auto_examples/decomposition/images/sphx_glr_plot_kernel_pca_002.png\n    :target: ../auto_examples/decomposition/plot_kernel_pca.html\n    :align: center\n    :scale: 75%\n\n.. note::\n    :meth:`KernelPCA.inverse_transform` relies on a kernel ridge to learn the\n    function mapping samples from the PCA basis into the original feature\n    space [Bakir2003]_. Thus, the reconstruction obtained with\n    :meth:`KernelPCA.inverse_transform` is an approximation. See the example\n    linked below for more details.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_decomposition_plot_kernel_pca.py`\n* :ref:`sphx_glr_auto_examples_applications_plot_digits_denoising.py`\n\n.. rubric:: References\n\n.. [Scholkopf1997] Sch\u00f6lkopf, Bernhard, Alexander Smola, and Klaus-Robert M\u00fcller.\n   `\"Kernel principal component analysis.\"\n   <https://people.eecs.berkeley.edu/~wainwrig/stat241b/scholkopf_kernel.pdf>`_\n   International conference on artificial neural networks.\n   Springer, Berlin, Heidelberg, 1997.\n\n.. [Bakir2003] Bak\u0131r, G\u00f6khan H., Jason Weston, and Bernhard Sch\u00f6lkopf.\n   `\"Learning to find pre-images.\"\n   <https://papers.nips.cc/paper/2003/file/ac1ad983e08ad3304a97e147f522747e-Paper.pdf>`_\n   Advances in neural information processing systems 16 (2003): 449-456.\n\n.. _kPCA_Solvers:"
  },
  {
    "filename": "decomposition.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\decomposition.rst.txt",
    "id": "decomposition.rst.txt_chunk_9",
    "header": "Choice of solver for Kernel PCA",
    "text": "Choice of solver for Kernel PCA\n-------------------------------\n\nWhile in :class:`PCA` the number of components is bounded by the number of\nfeatures, in :class:`KernelPCA` the number of components is bounded by the\nnumber of samples. Many real-world datasets have large number of samples! In\nthese cases finding *all* the components with a full kPCA is a waste of\ncomputation time, as data is mostly described by the first few components\n(e.g. ``n_components<=100``). In other words, the centered Gram matrix that\nis eigendecomposed in the Kernel PCA fitting process has an effective rank that\nis much smaller than its size. This is a situation where approximate\neigensolvers can provide speedup with very low precision loss.\n\n\n.. dropdown:: Eigensolvers\n\n    The optional parameter ``eigen_solver='randomized'`` can be used to\n    *significantly* reduce the computation time when the number of requested\n    ``n_components`` is small compared with the number of samples. It relies on\n    randomized decomposition methods to find an approximate solution in a shorter\n    time.\n\n    The time complexity of the randomized :class:`KernelPCA` is\n    :math:`O(n_{\\mathrm{samples}}^2 \\cdot n_{\\mathrm{components}})`\n    instead of :math:`O(n_{\\mathrm{samples}}^3)` for the exact method\n    implemented with ``eigen_solver='dense'``.\n\n    The memory footprint of randomized :class:`KernelPCA` is also proportional to\n    :math:`2 \\cdot n_{\\mathrm{samples}} \\cdot n_{\\mathrm{components}}` instead of\n    :math:`n_{\\mathrm{samples}}^2` for the exact method.\n\n    Note: this technique is the same as in :ref:`RandomizedPCA`.\n\n    In addition to the above two solvers, ``eigen_solver='arpack'`` can be used as\n    an alternate way to get an approximate decomposition. In practice, this method\n    only provides reasonable execution times when the number of components to find\n    is extremely small. It is enabled by default when the desired number of\n    components is less than 10 (strict) and the number of samples is more than 200\n    (strict). See :class:`KernelPCA` for details.\n\n    .. rubric:: References\n\n    * *dense* solver:\n      `scipy.linalg.eigh documentation\n      <https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.eigh.html>`_\n\n    * *randomized* solver:\n\n      * Algorithm 4.3 in\n        :arxiv:`\"Finding structure with randomness: Stochastic\n        algorithms for constructing approximate matrix decompositions\" <0909.4061>`\n        Halko, et al. (2009)\n\n      * :arxiv:`\"An implementation of a randomized algorithm\n        for principal component analysis\" <1412.3510>`\n        A. Szlam et al. (2014)\n\n    * *arpack* solver:\n      `scipy.sparse.linalg.eigsh documentation\n      <https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.eigsh.html>`_\n      R. B. Lehoucq, D. C. Sorensen, and C. Yang, (1998)\n\n\n.. _LSA:"
  },
  {
    "filename": "decomposition.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\decomposition.rst.txt",
    "id": "decomposition.rst.txt_chunk_10",
    "header": "Truncated singular value decomposition and latent semantic analysis",
    "text": "Truncated singular value decomposition and latent semantic analysis\n===================================================================\n\n:class:`TruncatedSVD` implements a variant of singular value decomposition\n(SVD) that only computes the :math:`k` largest singular values,\nwhere :math:`k` is a user-specified parameter.\n\n:class:`TruncatedSVD` is very similar to :class:`PCA`, but differs\nin that the matrix :math:`X` does not need to be centered.\nWhen the columnwise (per-feature) means of :math:`X`\nare subtracted from the feature values,\ntruncated SVD on the resulting matrix is equivalent to PCA.\n\n.. dropdown:: About truncated SVD and latent semantic analysis (LSA)\n\n    When truncated SVD is applied to term-document matrices\n    (as returned by :class:`~sklearn.feature_extraction.text.CountVectorizer` or\n    :class:`~sklearn.feature_extraction.text.TfidfVectorizer`),\n    this transformation is known as\n    `latent semantic analysis <https://nlp.stanford.edu/IR-book/pdf/18lsi.pdf>`_\n    (LSA), because it transforms such matrices\n    to a \"semantic\" space of low dimensionality.\n    In particular, LSA is known to combat the effects of synonymy and polysemy\n    (both of which roughly mean there are multiple meanings per word),\n    which cause term-document matrices to be overly sparse\n    and exhibit poor similarity under measures such as cosine similarity.\n\n    .. note::\n        LSA is also known as latent semantic indexing, LSI,\n        though strictly that refers to its use in persistent indexes\n        for information retrieval purposes.\n\n    Mathematically, truncated SVD applied to training samples :math:`X`\n    produces a low-rank approximation :math:`X`:\n\n    .. math::\n        X \\approx X_k = U_k \\Sigma_k V_k^\\top\n\n    After this operation, :math:`U_k \\Sigma_k`\n    is the transformed training set with :math:`k` features\n    (called ``n_components`` in the API).\n\n    To also transform a test set :math:`X`, we multiply it with :math:`V_k`:\n\n    .. math::\n        X' = X V_k\n\n    .. note::\n        Most treatments of LSA in the natural language processing (NLP)\n        and information retrieval (IR) literature\n        swap the axes of the matrix :math:`X` so that it has shape\n        ``(n_features, n_samples)``.\n        We present LSA in a different way that matches the scikit-learn API better,\n        but the singular values found are the same.\n\n    While the :class:`TruncatedSVD` transformer\n    works with any feature matrix,\n    using it on tf-idf matrices is recommended over raw frequency counts\n    in an LSA/document processing setting.\n    In particular, sublinear scaling and inverse document frequency\n    should be turned on (``sublinear_tf=True, use_idf=True``)\n    to bring the feature values closer to a Gaussian distribution,\n    compensating for LSA's erroneous assumptions about textual data.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`\n\n.. rubric:: References\n\n* Christopher D. Manning, Prabhakar Raghavan and Hinrich Sch\u00fctze (2008),\n  *Introduction to Information Retrieval*, Cambridge University Press,\n  chapter 18: `Matrix decompositions & latent semantic indexing\n  <https://nlp.stanford.edu/IR-book/pdf/18lsi.pdf>`_\n\n\n\n.. _DictionaryLearning:"
  },
  {
    "filename": "decomposition.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\decomposition.rst.txt",
    "id": "decomposition.rst.txt_chunk_11",
    "header": "Dictionary Learning",
    "text": "Dictionary Learning\n===================\n\n.. _SparseCoder:"
  },
  {
    "filename": "decomposition.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\decomposition.rst.txt",
    "id": "decomposition.rst.txt_chunk_12",
    "header": "Sparse coding with a precomputed dictionary",
    "text": "Sparse coding with a precomputed dictionary\n-------------------------------------------\n\nThe :class:`SparseCoder` object is an estimator that can be used to transform signals\ninto sparse linear combination of atoms from a fixed, precomputed dictionary\nsuch as a discrete wavelet basis. This object therefore does not\nimplement a ``fit`` method. The transformation amounts\nto a sparse coding problem: finding a representation of the data as a linear\ncombination of as few dictionary atoms as possible. All variations of\ndictionary learning implement the following transform methods, controllable via\nthe ``transform_method`` initialization parameter:\n\n* Orthogonal matching pursuit (:ref:`omp`)\n\n* Least-angle regression (:ref:`least_angle_regression`)\n\n* Lasso computed by least-angle regression\n\n* Lasso using coordinate descent (:ref:`lasso`)\n\n* Thresholding\n\nThresholding is very fast but it does not yield accurate reconstructions.\nThey have been shown useful in literature for classification tasks. For image\nreconstruction tasks, orthogonal matching pursuit yields the most accurate,\nunbiased reconstruction.\n\nThe dictionary learning objects offer, via the ``split_code`` parameter, the\npossibility to separate the positive and negative values in the results of\nsparse coding. This is useful when dictionary learning is used for extracting\nfeatures that will be used for supervised learning, because it allows the\nlearning algorithm to assign different weights to negative loadings of a\nparticular atom, from to the corresponding positive loading.\n\nThe split code for a single sample has length ``2 * n_components``\nand is constructed using the following rule: First, the regular code of length\n``n_components`` is computed. Then, the first ``n_components`` entries of the\n``split_code`` are\nfilled with the positive part of the regular code vector. The second half of\nthe split code is filled with the negative part of the code vector, only with\na positive sign. Therefore, the split_code is non-negative.\n\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_decomposition_plot_sparse_coding.py`"
  },
  {
    "filename": "decomposition.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\decomposition.rst.txt",
    "id": "decomposition.rst.txt_chunk_13",
    "header": "Generic dictionary learning",
    "text": "Generic dictionary learning\n---------------------------\n\nDictionary learning (:class:`DictionaryLearning`) is a matrix factorization\nproblem that amounts to finding a (usually overcomplete) dictionary that will\nperform well at sparsely encoding the fitted data.\n\nRepresenting data as sparse combinations of atoms from an overcomplete\ndictionary is suggested to be the way the mammalian primary visual cortex works.\nConsequently, dictionary learning applied on image patches has been shown to\ngive good results in image processing tasks such as image completion,\ninpainting and denoising, as well as for supervised recognition tasks.\n\nDictionary learning is an optimization problem solved by alternatively updating\nthe sparse code, as a solution to multiple Lasso problems, considering the\ndictionary fixed, and then updating the dictionary to best fit the sparse code.\n\n.. math::\n   (U^*, V^*) = \\underset{U, V}{\\operatorname{arg\\,min\\,}} & \\frac{1}{2}\n                ||X-UV||_{\\text{Fro}}^2+\\alpha||U||_{1,1} \\\\\n                \\text{subject to } & ||V_k||_2 \\leq 1 \\text{ for all }\n                0 \\leq k < n_{\\mathrm{atoms}}\n\n\n.. |pca_img2| image:: ../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_002.png\n   :target: ../auto_examples/decomposition/plot_faces_decomposition.html\n   :scale: 60%\n\n.. |dict_img2| image:: ../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_007.png\n   :target: ../auto_examples/decomposition/plot_faces_decomposition.html\n   :scale: 60%\n\n.. centered:: |pca_img2| |dict_img2|\n\n:math:`||.||_{\\text{Fro}}` stands for the Frobenius norm and :math:`||.||_{1,1}`\nstands for the entry-wise matrix norm which is the sum of the absolute values\nof all the entries in the matrix.\nAfter using such a procedure to fit the dictionary, the transform is simply a\nsparse coding step that shares the same implementation with all dictionary\nlearning objects (see :ref:`SparseCoder`).\n\nIt is also possible to constrain the dictionary and/or code to be positive to\nmatch constraints that may be present in the data. Below are the faces with\ndifferent positivity constraints applied. Red indicates negative values, blue\nindicates positive values, and white represents zeros.\n\n\n.. |dict_img_pos1| image:: ../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_010.png\n    :target: ../auto_examples/decomposition/plot_image_denoising.html\n    :scale: 60%\n\n.. |dict_img_pos2| image:: ../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_011.png\n    :target: ../auto_examples/decomposition/plot_image_denoising.html\n    :scale: 60%\n\n.. |dict_img_pos3| image:: ../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_012.png\n    :target: ../auto_examples/decomposition/plot_image_denoising.html\n    :scale: 60%\n\n.. |dict_img_pos4| image:: ../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_013.png\n    :target: ../auto_examples/decomposition/plot_image_denoising.html\n    :scale: 60%\n\n.. centered:: |dict_img_pos1| |dict_img_pos2|\n.. centered:: |dict_img_pos3| |dict_img_pos4|\n\n\nThe following image shows how a dictionary learned from 4x4 pixel image patches\nextracted from part of the image of a raccoon face looks like.\n\n\n.. figure:: ../auto_examples/decomposition/images/sphx_glr_plot_image_denoising_001.png\n    :target: ../auto_examples/decomposition/plot_image_denoising.html\n    :align: center\n    :scale: 50%\n\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_decomposition_plot_image_denoising.py`\n\n\n.. rubric:: References\n\n* `\"Online dictionary learning for sparse coding\"\n  <https://www.di.ens.fr/~fbach/mairal_icml09.pdf>`_\n  J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009\n\n.. _MiniBatchDictionaryLearning:"
  },
  {
    "filename": "decomposition.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\decomposition.rst.txt",
    "id": "decomposition.rst.txt_chunk_14",
    "header": "Mini-batch dictionary learning",
    "text": "Mini-batch dictionary learning\n------------------------------\n\n:class:`MiniBatchDictionaryLearning` implements a faster, but less accurate\nversion of the dictionary learning algorithm that is better suited for large\ndatasets.\n\nBy default, :class:`MiniBatchDictionaryLearning` divides the data into\nmini-batches and optimizes in an online manner by cycling over the mini-batches\nfor the specified number of iterations. However, at the moment it does not\nimplement a stopping condition.\n\nThe estimator also implements ``partial_fit``, which updates the dictionary by\niterating only once over a mini-batch. This can be used for online learning\nwhen the data is not readily available from the start, or for when the data\ndoes not fit into memory.\n\n.. currentmodule:: sklearn.cluster\n\n.. image:: ../auto_examples/cluster/images/sphx_glr_plot_dict_face_patches_001.png\n    :target: ../auto_examples/cluster/plot_dict_face_patches.html\n    :scale: 50%\n    :align: right\n\n.. topic:: **Clustering for dictionary learning**\n\n   Note that when using dictionary learning to extract a representation\n   (e.g. for sparse coding) clustering can be a good proxy to learn the\n   dictionary. For instance the :class:`MiniBatchKMeans` estimator is\n   computationally efficient and implements on-line learning with a\n   ``partial_fit`` method.\n\n   Example: :ref:`sphx_glr_auto_examples_cluster_plot_dict_face_patches.py`\n\n.. currentmodule:: sklearn.decomposition\n\n.. _FA:"
  },
  {
    "filename": "decomposition.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\decomposition.rst.txt",
    "id": "decomposition.rst.txt_chunk_15",
    "header": "Factor Analysis",
    "text": "Factor Analysis\n===============\n\nIn unsupervised learning we only have a dataset :math:`X = \\{x_1, x_2, \\dots, x_n\n\\}`. How can this dataset be described mathematically? A very simple\n`continuous latent variable` model for :math:`X` is\n\n.. math:: x_i = W h_i + \\mu + \\epsilon\n\nThe vector :math:`h_i` is called \"latent\" because it is unobserved. :math:`\\epsilon` is\nconsidered a noise term distributed according to a Gaussian with mean 0 and\ncovariance :math:`\\Psi` (i.e. :math:`\\epsilon \\sim \\mathcal{N}(0, \\Psi)`), :math:`\\mu` is some\narbitrary offset vector. Such a model is called \"generative\" as it describes\nhow :math:`x_i` is generated from :math:`h_i`. If we use all the :math:`x_i`'s as columns to form\na matrix :math:`\\mathbf{X}` and all the :math:`h_i`'s as columns of a matrix :math:`\\mathbf{H}`\nthen we can write (with suitably defined :math:`\\mathbf{M}` and :math:`\\mathbf{E}`):\n\n.. math::\n    \\mathbf{X} = W \\mathbf{H} + \\mathbf{M} + \\mathbf{E}\n\nIn other words, we *decomposed* matrix :math:`\\mathbf{X}`.\n\nIf :math:`h_i` is given, the above equation automatically implies the following\nprobabilistic interpretation:\n\n.. math:: p(x_i|h_i) = \\mathcal{N}(Wh_i + \\mu, \\Psi)\n\nFor a complete probabilistic model we also need a prior distribution for the\nlatent variable :math:`h`. The most straightforward assumption (based on the nice\nproperties of the Gaussian distribution) is :math:`h \\sim \\mathcal{N}(0,\n\\mathbf{I})`.  This yields a Gaussian as the marginal distribution of :math:`x`:\n\n.. math:: p(x) = \\mathcal{N}(\\mu, WW^T + \\Psi)\n\nNow, without any further assumptions the idea of having a latent variable :math:`h`\nwould be superfluous -- :math:`x` can be completely modelled with a mean\nand a covariance. We need to impose some more specific structure on one\nof these two parameters. A simple additional assumption regards the\nstructure of the error covariance :math:`\\Psi`:\n\n* :math:`\\Psi = \\sigma^2 \\mathbf{I}`: This assumption leads to\n  the probabilistic model of :class:`PCA`.\n\n* :math:`\\Psi = \\mathrm{diag}(\\psi_1, \\psi_2, \\dots, \\psi_n)`: This model is called\n  :class:`FactorAnalysis`, a classical statistical model. The matrix W is\n  sometimes called the \"factor loading matrix\".\n\nBoth models essentially estimate a Gaussian with a low-rank covariance matrix.\nBecause both models are probabilistic they can be integrated in more complex\nmodels, e.g. Mixture of Factor Analysers. One gets very different models (e.g.\n:class:`FastICA`) if non-Gaussian priors on the latent variables are assumed.\n\nFactor analysis *can* produce similar components (the columns of its loading\nmatrix) to :class:`PCA`. However, one can not make any general statements\nabout these components (e.g. whether they are orthogonal):\n\n.. |pca_img3| image:: ../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_002.png\n    :target: ../auto_examples/decomposition/plot_faces_decomposition.html\n    :scale: 60%\n\n.. |fa_img3| image:: ../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_008.png\n    :target: ../auto_examples/decomposition/plot_faces_decomposition.html\n    :scale: 60%\n\n.. centered:: |pca_img3| |fa_img3|\n\nThe main advantage for Factor Analysis over :class:`PCA` is that\nit can model the variance in every direction of the input space independently\n(heteroscedastic noise):\n\n.. figure:: ../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_009.png\n    :target: ../auto_examples/decomposition/plot_faces_decomposition.html\n    :align: center\n    :scale: 75%\n\nThis allows better model selection than probabilistic PCA in the presence\nof heteroscedastic noise:\n\n.. figure:: ../auto_examples/decomposition/images/sphx_glr_plot_pca_vs_fa_model_selection_002.png\n    :target: ../auto_examples/decomposition/plot_pca_vs_fa_model_selection.html\n    :align: center\n    :scale: 75%\n\nFactor Analysis is often followed by a rotation of the factors (with the\nparameter `rotation`), usually to improve interpretability. For example,\nVarimax rotation maximizes the sum of the variances of the squared loadings,\ni.e., it tends to produce sparser factors, which are influenced by only a few\nfeatures each (the \"simple structure\"). See e.g., the first example below.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_decomposition_plot_varimax_fa.py`\n* :ref:`sphx_glr_auto_examples_decomposition_plot_pca_vs_fa_model_selection.py`\n\n\n.. _ICA:"
  },
  {
    "filename": "decomposition.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\decomposition.rst.txt",
    "id": "decomposition.rst.txt_chunk_16",
    "header": "Independent component analysis (ICA)",
    "text": "Independent component analysis (ICA)\n====================================\n\nIndependent component analysis separates a multivariate signal into\nadditive subcomponents that are maximally independent. It is\nimplemented in scikit-learn using the :class:`Fast ICA <FastICA>`\nalgorithm. Typically, ICA is not used for reducing dimensionality but\nfor separating superimposed signals. Since the ICA model does not include\na noise term, for the model to be correct, whitening must be applied.\nThis can be done internally using the `whiten` argument or manually using one\nof the PCA variants.\n\nIt is classically used to separate mixed signals (a problem known as\n*blind source separation*), as in the example below:\n\n.. figure:: ../auto_examples/decomposition/images/sphx_glr_plot_ica_blind_source_separation_001.png\n    :target: ../auto_examples/decomposition/plot_ica_blind_source_separation.html\n    :align: center\n    :scale: 60%\n\n\nICA can also be used as yet another non linear decomposition that finds\ncomponents with some sparsity:\n\n.. |pca_img4| image:: ../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_002.png\n    :target: ../auto_examples/decomposition/plot_faces_decomposition.html\n    :scale: 60%\n\n.. |ica_img4| image:: ../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_004.png\n    :target: ../auto_examples/decomposition/plot_faces_decomposition.html\n    :scale: 60%\n\n.. centered:: |pca_img4| |ica_img4|\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_decomposition_plot_ica_blind_source_separation.py`\n* :ref:`sphx_glr_auto_examples_decomposition_plot_ica_vs_pca.py`\n* :ref:`sphx_glr_auto_examples_decomposition_plot_faces_decomposition.py`\n\n\n.. _NMF:"
  },
  {
    "filename": "decomposition.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\decomposition.rst.txt",
    "id": "decomposition.rst.txt_chunk_17",
    "header": "Non-negative matrix factorization (NMF or NNMF)",
    "text": "Non-negative matrix factorization (NMF or NNMF)\n==============================================="
  },
  {
    "filename": "decomposition.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\decomposition.rst.txt",
    "id": "decomposition.rst.txt_chunk_18",
    "header": "NMF with the Frobenius norm",
    "text": "NMF with the Frobenius norm\n---------------------------\n\n:class:`NMF` [1]_ is an alternative approach to decomposition that assumes that the\ndata and the components are non-negative. :class:`NMF` can be plugged in\ninstead of :class:`PCA` or its variants, in the cases where the data matrix\ndoes not contain negative values. It finds a decomposition of samples\n:math:`X` into two matrices :math:`W` and :math:`H` of non-negative elements,\nby optimizing the distance :math:`d` between :math:`X` and the matrix product\n:math:`WH`. The most widely used distance function is the squared Frobenius\nnorm, which is an obvious extension of the Euclidean norm to matrices:\n\n.. math::\n    d_{\\mathrm{Fro}}(X, Y) = \\frac{1}{2} ||X - Y||_{\\mathrm{Fro}}^2 = \\frac{1}{2} \\sum_{i,j} (X_{ij} - {Y}_{ij})^2\n\nUnlike :class:`PCA`, the representation of a vector is obtained in an additive\nfashion, by superimposing the components, without subtracting. Such additive\nmodels are efficient for representing images and text.\n\nIt has been observed in [Hoyer, 2004] [2]_ that, when carefully constrained,\n:class:`NMF` can produce a parts-based representation of the dataset,\nresulting in interpretable models. The following example displays 16\nsparse components found by :class:`NMF` from the images in the Olivetti\nfaces dataset, in comparison with the PCA eigenfaces.\n\n.. |pca_img5| image:: ../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_002.png\n    :target: ../auto_examples/decomposition/plot_faces_decomposition.html\n    :scale: 60%\n\n.. |nmf_img5| image:: ../auto_examples/decomposition/images/sphx_glr_plot_faces_decomposition_003.png\n    :target: ../auto_examples/decomposition/plot_faces_decomposition.html\n    :scale: 60%\n\n.. centered:: |pca_img5| |nmf_img5|\n\n\nThe `init` attribute determines the initialization method applied, which\nhas a great impact on the performance of the method. :class:`NMF` implements the\nmethod Nonnegative Double Singular Value Decomposition. NNDSVD [4]_ is based on\ntwo SVD processes, one approximating the data matrix, the other approximating\npositive sections of the resulting partial SVD factors utilizing an algebraic\nproperty of unit rank matrices. The basic NNDSVD algorithm is better fit for\nsparse factorization. Its variants NNDSVDa (in which all zeros are set equal to\nthe mean of all elements of the data), and NNDSVDar (in which the zeros are set\nto random perturbations less than the mean of the data divided by 100) are\nrecommended in the dense case.\n\nNote that the Multiplicative Update ('mu') solver cannot update zeros present in\nthe initialization, so it leads to poorer results when used jointly with the\nbasic NNDSVD algorithm which introduces a lot of zeros; in this case, NNDSVDa or\nNNDSVDar should be preferred.\n\n:class:`NMF` can also be initialized with correctly scaled random non-negative\nmatrices by setting `init=\"random\"`. An integer seed or a\n``RandomState`` can also be passed to `random_state` to control\nreproducibility.\n\nIn :class:`NMF`, L1 and L2 priors can be added to the loss function in order to\nregularize the model. The L2 prior uses the Frobenius norm, while the L1 prior\nuses an elementwise L1 norm. As in :class:`~sklearn.linear_model.ElasticNet`,\nwe control the combination of L1 and L2 with the `l1_ratio` (:math:`\\rho`)\nparameter, and the intensity of the regularization with the `alpha_W` and\n`alpha_H` (:math:`\\alpha_W` and :math:`\\alpha_H`) parameters. The priors are\nscaled by the number of samples (:math:`n\\_samples`) for `H` and the number of\nfeatures (:math:`n\\_features`) for `W` to keep their impact balanced with\nrespect to one another and to the data fit term as independent as possible of\nthe size of the training set. Then the priors terms are:\n\n.. math::\n    (\\alpha_W \\rho ||W||_1 + \\frac{\\alpha_W(1-\\rho)}{2} ||W||_{\\mathrm{Fro}} ^ 2) * n\\_features\n    + (\\alpha_H \\rho ||H||_1 + \\frac{\\alpha_H(1-\\rho)}{2} ||H||_{\\mathrm{Fro}} ^ 2) * n\\_samples\n\nand the regularized objective function is:\n\n.. math::\n    d_{\\mathrm{Fro}}(X, WH)\n    + (\\alpha_W \\rho ||W||_1 + \\frac{\\alpha_W(1-\\rho)}{2} ||W||_{\\mathrm{Fro}} ^ 2) * n\\_features\n    + (\\alpha_H \\rho ||H||_1 + \\frac{\\alpha_H(1-\\rho)}{2} ||H||_{\\mathrm{Fro}} ^ 2) * n\\_samples"
  },
  {
    "filename": "decomposition.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\decomposition.rst.txt",
    "id": "decomposition.rst.txt_chunk_19",
    "header": "NMF with a beta-divergence",
    "text": "NMF with a beta-divergence\n--------------------------\n\nAs described previously, the most widely used distance function is the squared\nFrobenius norm, which is an obvious extension of the Euclidean norm to\nmatrices:\n\n.. math::\n    d_{\\mathrm{Fro}}(X, Y) = \\frac{1}{2} ||X - Y||_{Fro}^2 = \\frac{1}{2} \\sum_{i,j} (X_{ij} - {Y}_{ij})^2\n\nOther distance functions can be used in NMF as, for example, the (generalized)\nKullback-Leibler (KL) divergence, also referred as I-divergence:\n\n.. math::\n    d_{KL}(X, Y) = \\sum_{i,j} (X_{ij} \\log(\\frac{X_{ij}}{Y_{ij}}) - X_{ij} + Y_{ij})\n\nOr, the Itakura-Saito (IS) divergence:\n\n.. math::\n    d_{IS}(X, Y) = \\sum_{i,j} (\\frac{X_{ij}}{Y_{ij}} - \\log(\\frac{X_{ij}}{Y_{ij}}) - 1)\n\nThese three distances are special cases of the beta-divergence family, with\n:math:`\\beta = 2, 1, 0` respectively [6]_. The beta-divergence is\ndefined by :\n\n.. math::\n    d_{\\beta}(X, Y) = \\sum_{i,j} \\frac{1}{\\beta(\\beta - 1)}(X_{ij}^\\beta + (\\beta-1)Y_{ij}^\\beta - \\beta X_{ij} Y_{ij}^{\\beta - 1})\n\n.. image:: ../images/beta_divergence.png\n    :align: center\n    :scale: 75%\n\nNote that this definition is not valid if :math:`\\beta \\in (0; 1)`, yet it can\nbe continuously extended to the definitions of :math:`d_{KL}` and :math:`d_{IS}`\nrespectively.\n\n.. dropdown:: NMF implemented solvers\n\n    :class:`NMF` implements two solvers, using Coordinate Descent ('cd') [5]_, and\n    Multiplicative Update ('mu') [6]_. The 'mu' solver can optimize every\n    beta-divergence, including of course the Frobenius norm (:math:`\\beta=2`), the\n    (generalized) Kullback-Leibler divergence (:math:`\\beta=1`) and the\n    Itakura-Saito divergence (:math:`\\beta=0`). Note that for\n    :math:`\\beta \\in (1; 2)`, the 'mu' solver is significantly faster than for other\n    values of :math:`\\beta`. Note also that with a negative (or 0, i.e.\n    'itakura-saito') :math:`\\beta`, the input matrix cannot contain zero values.\n\n    The 'cd' solver can only optimize the Frobenius norm. Due to the\n    underlying non-convexity of NMF, the different solvers may converge to\n    different minima, even when optimizing the same distance function.\n\nNMF is best used with the ``fit_transform`` method, which returns the matrix W.\nThe matrix H is stored into the fitted model in the ``components_`` attribute;\nthe method ``transform`` will decompose a new matrix X_new based on these\nstored components::\n\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n    >>> from sklearn.decomposition import NMF\n    >>> model = NMF(n_components=2, init='random', random_state=0)\n    >>> W = model.fit_transform(X)\n    >>> H = model.components_\n    >>> X_new = np.array([[1, 0], [1, 6.1], [1, 0], [1, 4], [3.2, 1], [0, 4]])\n    >>> W_new = model.transform(X_new)\n\n\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_decomposition_plot_faces_decomposition.py`\n* :ref:`sphx_glr_auto_examples_applications_plot_topics_extraction_with_nmf_lda.py`\n\n.. _MiniBatchNMF:"
  },
  {
    "filename": "decomposition.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\decomposition.rst.txt",
    "id": "decomposition.rst.txt_chunk_20",
    "header": "Mini-batch Non Negative Matrix Factorization",
    "text": "Mini-batch Non Negative Matrix Factorization\n--------------------------------------------\n\n:class:`MiniBatchNMF` [7]_ implements a faster, but less accurate version of the\nnon negative matrix factorization (i.e. :class:`~sklearn.decomposition.NMF`),\nbetter suited for large datasets.\n\nBy default, :class:`MiniBatchNMF` divides the data into mini-batches and\noptimizes the NMF model in an online manner by cycling over the mini-batches\nfor the specified number of iterations. The ``batch_size`` parameter controls\nthe size of the batches.\n\nIn order to speed up the mini-batch algorithm it is also possible to scale\npast batches, giving them less importance than newer batches. This is done\nby introducing a so-called forgetting factor controlled by the ``forget_factor``\nparameter.\n\nThe estimator also implements ``partial_fit``, which updates ``H`` by iterating\nonly once over a mini-batch. This can be used for online learning when the data\nis not readily available from the start, or when the data does not fit into memory.\n\n.. rubric:: References\n\n.. [1] `\"Learning the parts of objects by non-negative matrix factorization\"\n  <http://www.cs.columbia.edu/~blei/fogm/2020F/readings/LeeSeung1999.pdf>`_\n  D. Lee, S. Seung, 1999\n\n.. [2] `\"Non-negative Matrix Factorization with Sparseness Constraints\"\n  <https://www.jmlr.org/papers/volume5/hoyer04a/hoyer04a.pdf>`_\n  P. Hoyer, 2004\n\n.. [4] `\"SVD based initialization: A head start for nonnegative\n  matrix factorization\"\n  <https://www.boutsidis.org/Boutsidis_PRE_08.pdf>`_\n  C. Boutsidis, E. Gallopoulos, 2008\n\n.. [5] `\"Fast local algorithms for large scale nonnegative matrix and tensor\n  factorizations.\"\n  <https://www.researchgate.net/profile/Anh-Huy-Phan/publication/220241471_Fast_Local_Algorithms_for_Large_Scale_Nonnegative_Matrix_and_Tensor_Factorizations>`_\n  A. Cichocki, A. Phan, 2009\n\n.. [6] :arxiv:`\"Algorithms for nonnegative matrix factorization with\n  the beta-divergence\" <1010.1763>`\n  C. Fevotte, J. Idier, 2011\n\n.. [7] :arxiv:`\"Online algorithms for nonnegative matrix factorization with the\n  Itakura-Saito divergence\" <1106.4198>`\n  A. Lefevre, F. Bach, C. Fevotte, 2011\n\n.. _LatentDirichletAllocation:"
  },
  {
    "filename": "decomposition.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\decomposition.rst.txt",
    "id": "decomposition.rst.txt_chunk_21",
    "header": "Latent Dirichlet Allocation (LDA)",
    "text": "Latent Dirichlet Allocation (LDA)\n=================================\n\nLatent Dirichlet Allocation is a generative probabilistic model for collections of\ndiscrete datasets such as text corpora. It is also a topic model that is used for\ndiscovering abstract topics from a collection of documents.\n\nThe graphical model of LDA is a three-level generative model:\n\n.. image:: ../images/lda_model_graph.png\n   :align: center\n\nNote on notations presented in the graphical model above, which can be found in\nHoffman et al. (2013):\n\n* The corpus is a collection of :math:`D` documents.\n* A document is a sequence of :math:`N` words.\n* There are :math:`K` topics in the corpus.\n* The boxes represent repeated sampling.\n\nIn the graphical model, each node is a random variable and has a role in the\ngenerative process. A shaded node indicates an observed variable and an unshaded\nnode indicates a hidden (latent) variable. In this case, words in the corpus are\nthe only data that we observe. The latent variables determine the random mixture\nof topics in the corpus and the distribution of words in the documents.\nThe goal of LDA is to use the observed words to infer the hidden topic\nstructure.\n\n.. dropdown:: Details on modeling text corpora\n\n    When modeling text corpora, the model assumes the following generative process\n    for a corpus with :math:`D` documents and :math:`K` topics, with :math:`K`\n    corresponding to `n_components` in the API:\n\n    1. For each topic :math:`k \\in K`, draw :math:`\\beta_k \\sim\n       \\mathrm{Dirichlet}(\\eta)`. This provides a distribution over the words,\n       i.e. the probability of a word appearing in topic :math:`k`.\n       :math:`\\eta` corresponds to `topic_word_prior`.\n\n    2. For each document :math:`d \\in D`, draw the topic proportions\n       :math:`\\theta_d \\sim \\mathrm{Dirichlet}(\\alpha)`. :math:`\\alpha`\n       corresponds to `doc_topic_prior`.\n\n    3. For each word :math:`i` in document :math:`d`:\n\n       a. Draw the topic assignment :math:`z_{di} \\sim \\mathrm{Multinomial}\n          (\\theta_d)`\n       b. Draw the observed word :math:`w_{ij} \\sim \\mathrm{Multinomial}\n          (\\beta_{z_{di}})`\n\n    For parameter estimation, the posterior distribution is:\n\n    .. math::\n        p(z, \\theta, \\beta |w, \\alpha, \\eta) =\n        \\frac{p(z, \\theta, \\beta|\\alpha, \\eta)}{p(w|\\alpha, \\eta)}\n\n    Since the posterior is intractable, variational Bayesian method\n    uses a simpler distribution :math:`q(z,\\theta,\\beta | \\lambda, \\phi, \\gamma)`\n    to approximate it, and those variational parameters :math:`\\lambda`,\n    :math:`\\phi`, :math:`\\gamma` are optimized to maximize the Evidence\n    Lower Bound (ELBO):\n\n    .. math::\n        \\log\\: P(w | \\alpha, \\eta) \\geq L(w,\\phi,\\gamma,\\lambda) \\overset{\\triangle}{=}\n        E_{q}[\\log\\:p(w,z,\\theta,\\beta|\\alpha,\\eta)] - E_{q}[\\log\\:q(z, \\theta, \\beta)]\n\n    Maximizing ELBO is equivalent to minimizing the Kullback-Leibler(KL) divergence\n    between :math:`q(z,\\theta,\\beta)` and the true posterior\n    :math:`p(z, \\theta, \\beta |w, \\alpha, \\eta)`.\n\n\n:class:`LatentDirichletAllocation` implements the online variational Bayes\nalgorithm and supports both online and batch update methods.\nWhile the batch method updates variational variables after each full pass through\nthe data, the online method updates variational variables from mini-batch data\npoints.\n\n.. note::\n\n  Although the online method is guaranteed to converge to a local optimum point, the quality of\n  the optimum point and the speed of convergence may depend on mini-batch size and\n  attributes related to learning rate setting.\n\nWhen :class:`LatentDirichletAllocation` is applied on a \"document-term\" matrix, the matrix\nwill be decomposed into a \"topic-term\" matrix and a \"document-topic\" matrix. While\n\"topic-term\" matrix is stored as `components_` in the model, \"document-topic\" matrix\ncan be calculated from ``transform`` method.\n\n:class:`LatentDirichletAllocation` also implements ``partial_fit`` method. This is used\nwhen data can be fetched sequentially.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_applications_plot_topics_extraction_with_nmf_lda.py`\n\n.. rubric:: References\n\n* `\"Latent Dirichlet Allocation\"\n  <https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf>`_\n  D. Blei, A. Ng, M. Jordan, 2003\n\n* `\"Online Learning for Latent Dirichlet Allocation\u201d\n  <https://papers.nips.cc/paper/3902-online-learning-for-latent-dirichlet-allocation.pdf>`_\n  M. Hoffman, D. Blei, F. Bach, 2010\n\n* `\"Stochastic Variational Inference\"\n  <https://www.cs.columbia.edu/~blei/papers/HoffmanBleiWangPaisley2013.pdf>`_\n  M. Hoffman, D. Blei, C. Wang, J. Paisley, 2013\n\n* `\"The varimax criterion for analytic rotation in factor analysis\"\n  <https://link.springer.com/article/10.1007%2FBF02289233>`_\n  H. F. Kaiser, 1958\n\nSee also :ref:`nca_dim_reduction` for dimensionality reduction with\nNeighborhood Components Analysis."
  },
  {
    "filename": "density.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\density.rst.txt",
    "id": "density.rst.txt_chunk_0",
    "header": ".. _density_estimation:",
    "text": ".. _density_estimation:\n\n=================="
  },
  {
    "filename": "density.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\density.rst.txt",
    "id": "density.rst.txt_chunk_1",
    "header": "Density Estimation",
    "text": "Density Estimation\n==================\n.. sectionauthor:: Jake Vanderplas <vanderplas@astro.washington.edu>\n\nDensity estimation walks the line between unsupervised learning, feature\nengineering, and data modeling.  Some of the most popular and useful\ndensity estimation techniques are mixture models such as\nGaussian Mixtures (:class:`~sklearn.mixture.GaussianMixture`), and\nneighbor-based approaches such as the kernel density estimate\n(:class:`~sklearn.neighbors.KernelDensity`).\nGaussian Mixtures are discussed more fully in the context of\n:ref:`clustering <clustering>`, because the technique is also useful as\nan unsupervised clustering scheme.\n\nDensity estimation is a very simple concept, and most people are already\nfamiliar with one common density estimation technique: the histogram."
  },
  {
    "filename": "density.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\density.rst.txt",
    "id": "density.rst.txt_chunk_2",
    "header": "Density Estimation: Histograms",
    "text": "Density Estimation: Histograms\n==============================\nA histogram is a simple visualization of data where bins are defined, and the\nnumber of data points within each bin is tallied.  An example of a histogram\ncan be seen in the upper-left panel of the following figure:\n\n.. |hist_to_kde| image:: ../auto_examples/neighbors/images/sphx_glr_plot_kde_1d_001.png\n   :target: ../auto_examples/neighbors/plot_kde_1d.html\n   :scale: 80\n\n.. centered:: |hist_to_kde|\n\nA major problem with histograms, however, is that the choice of binning can\nhave a disproportionate effect on the resulting visualization.  Consider the\nupper-right panel of the above figure.  It shows a histogram over the same\ndata, with the bins shifted right.  The results of the two visualizations look\nentirely different, and might lead to different interpretations of the data.\n\nIntuitively, one can also think of a histogram as a stack of blocks, one block\nper point.  By stacking the blocks in the appropriate grid space, we recover\nthe histogram.  But what if, instead of stacking the blocks on a regular grid,\nwe center each block on the point it represents, and sum the total height at\neach location?  This idea leads to the lower-left visualization.  It is perhaps\nnot as clean as a histogram, but the fact that the data drive the block\nlocations mean that it is a much better representation of the underlying\ndata.\n\nThis visualization is an example of a *kernel density estimation*, in this case\nwith a top-hat kernel (i.e. a square block at each point).  We can recover a\nsmoother distribution by using a smoother kernel.  The bottom-right plot shows\na Gaussian kernel density estimate, in which each point contributes a Gaussian\ncurve to the total.  The result is a smooth density estimate which is derived\nfrom the data, and functions as a powerful non-parametric model of the\ndistribution of points.\n\n.. _kernel_density:"
  },
  {
    "filename": "density.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\density.rst.txt",
    "id": "density.rst.txt_chunk_3",
    "header": "Kernel Density Estimation",
    "text": "Kernel Density Estimation\n=========================\nKernel density estimation in scikit-learn is implemented in the\n:class:`~sklearn.neighbors.KernelDensity` estimator, which uses the\nBall Tree or KD Tree for efficient queries (see :ref:`neighbors` for\na discussion of these).  Though the above example\nuses a 1D data set for simplicity, kernel density estimation can be\nperformed in any number of dimensions, though in practice the curse of\ndimensionality causes its performance to degrade in high dimensions.\n\nIn the following figure, 100 points are drawn from a bimodal distribution,\nand the kernel density estimates are shown for three choices of kernels:\n\n.. |kde_1d_distribution| image:: ../auto_examples/neighbors/images/sphx_glr_plot_kde_1d_003.png\n   :target: ../auto_examples/neighbors/plot_kde_1d.html\n   :scale: 80\n\n.. centered:: |kde_1d_distribution|\n\nIt's clear how the kernel shape affects the smoothness of the resulting\ndistribution.  The scikit-learn kernel density estimator can be used as\nfollows:\n\n   >>> from sklearn.neighbors import KernelDensity\n   >>> import numpy as np\n   >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n   >>> kde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(X)\n   >>> kde.score_samples(X)\n   array([-0.41075698, -0.41075698, -0.41076071, -0.41075698, -0.41075698,\n          -0.41076071])\n\nHere we have used ``kernel='gaussian'``, as seen above.\nMathematically, a kernel is a positive function :math:`K(x;h)`\nwhich is controlled by the bandwidth parameter :math:`h`.\nGiven this kernel form, the density estimate at a point :math:`y` within\na group of points :math:`x_i; i=1, \\cdots, N` is given by:\n\n.. math::\n    \\rho_K(y) = \\sum_{i=1}^{N} K(y - x_i; h)\n\nThe bandwidth here acts as a smoothing parameter, controlling the tradeoff\nbetween bias and variance in the result.  A large bandwidth leads to a very\nsmooth (i.e. high-bias) density distribution.  A small bandwidth leads\nto an unsmooth (i.e. high-variance) density distribution.\n\nThe parameter `bandwidth` controls this smoothing. One can either set\nmanually this parameter or use Scott's and Silverman's estimation\nmethods.\n\n:class:`~sklearn.neighbors.KernelDensity` implements several common kernel\nforms, which are shown in the following figure:\n\n.. |kde_kernels| image:: ../auto_examples/neighbors/images/sphx_glr_plot_kde_1d_002.png\n   :target: ../auto_examples/neighbors/plot_kde_1d.html\n   :scale: 80\n\n.. centered:: |kde_kernels|\n\n.. dropdown:: Kernels' mathematical expressions\n\n  The form of these kernels is as follows:\n\n  * Gaussian kernel (``kernel = 'gaussian'``)\n\n    :math:`K(x; h) \\propto \\exp(- \\frac{x^2}{2h^2} )`\n\n  * Tophat kernel (``kernel = 'tophat'``)\n\n    :math:`K(x; h) \\propto 1` if :math:`x < h`\n\n  * Epanechnikov kernel (``kernel = 'epanechnikov'``)\n\n    :math:`K(x; h) \\propto 1 - \\frac{x^2}{h^2}`\n\n  * Exponential kernel (``kernel = 'exponential'``)\n\n    :math:`K(x; h) \\propto \\exp(-x/h)`\n\n  * Linear kernel (``kernel = 'linear'``)\n\n    :math:`K(x; h) \\propto 1 - x/h` if :math:`x < h`\n\n  * Cosine kernel (``kernel = 'cosine'``)\n\n    :math:`K(x; h) \\propto \\cos(\\frac{\\pi x}{2h})` if :math:`x < h`\n\n\nThe kernel density estimator can be used with any of the valid distance\nmetrics (see :class:`~sklearn.metrics.DistanceMetric` for a list of\navailable metrics), though the results are properly normalized only\nfor the Euclidean metric.  One particularly useful metric is the\n`Haversine distance <https://en.wikipedia.org/wiki/Haversine_formula>`_\nwhich measures the angular distance between points on a sphere.  Here\nis an example of using a kernel density estimate for a visualization\nof geospatial data, in this case the distribution of observations of two\ndifferent species on the South American continent:\n\n.. |species_kde| image:: ../auto_examples/neighbors/images/sphx_glr_plot_species_kde_001.png\n   :target: ../auto_examples/neighbors/plot_species_kde.html\n   :scale: 80\n\n.. centered:: |species_kde|\n\nOne other useful application of kernel density estimation is to learn a\nnon-parametric generative model of a dataset in order to efficiently\ndraw new samples from this generative model.\nHere is an example of using this process to\ncreate a new set of hand-written digits, using a Gaussian kernel learned\non a PCA projection of the data:\n\n.. |digits_kde| image:: ../auto_examples/neighbors/images/sphx_glr_plot_digits_kde_sampling_001.png\n   :target: ../auto_examples/neighbors/plot_digits_kde_sampling.html\n   :scale: 80\n\n.. centered:: |digits_kde|\n\nThe \"new\" data consists of linear combinations of the input data, with weights\nprobabilistically drawn given the KDE model.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_neighbors_plot_kde_1d.py`: computation of simple kernel\n  density estimates in one dimension.\n\n* :ref:`sphx_glr_auto_examples_neighbors_plot_digits_kde_sampling.py`: an example of using\n  Kernel Density estimation to learn a generative model of the hand-written\n  digits data, and drawing new samples from this model.\n\n* :ref:`sphx_glr_auto_examples_neighbors_plot_species_kde.py`: an example of Kernel Density\n  estimation using the Haversine distance metric to visualize geospatial data"
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_0",
    "header": ".. _ensemble:",
    "text": ".. _ensemble:\n\n==========================================================================="
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_1",
    "header": "Ensembles: Gradient boosting, random forests, bagging, voting, stacking",
    "text": "Ensembles: Gradient boosting, random forests, bagging, voting, stacking\n===========================================================================\n\n.. currentmodule:: sklearn.ensemble\n\n**Ensemble methods** combine the predictions of several\nbase estimators built with a given learning algorithm in order to improve\ngeneralizability / robustness over a single estimator.\n\nTwo very famous examples of ensemble methods are :ref:`gradient-boosted trees\n<gradient_boosting>` and :ref:`random forests <forest>`.\n\nMore generally, ensemble models can be applied to any base learner beyond\ntrees, in averaging methods such as :ref:`Bagging methods <bagging>`,\n:ref:`model stacking <stacking>`, or :ref:`Voting <voting_classifier>`, or in\nboosting, as :ref:`AdaBoost <adaboost>`.\n\n.. _gradient_boosting:"
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_2",
    "header": "Gradient-boosted trees",
    "text": "Gradient-boosted trees\n======================\n\n`Gradient Tree Boosting <https://en.wikipedia.org/wiki/Gradient_boosting>`_\nor Gradient Boosted Decision Trees (GBDT) is a generalization\nof boosting to arbitrary differentiable loss functions, see the seminal work of\n[Friedman2001]_. GBDT is an excellent model for both regression and\nclassification, in particular for tabular data.\n\n.. topic:: :class:`GradientBoostingClassifier` vs :class:`HistGradientBoostingClassifier`\n\n  Scikit-learn provides two implementations of gradient-boosted trees:\n  :class:`HistGradientBoostingClassifier` vs\n  :class:`GradientBoostingClassifier` for classification, and the\n  corresponding classes for regression. The former can be **orders of\n  magnitude faster** than the latter when the number of samples is\n  larger than tens of thousands of samples.\n\n  Missing values and categorical data are natively supported by the\n  Hist... version, removing the need for additional preprocessing such as\n  imputation.\n\n  :class:`GradientBoostingClassifier` and\n  :class:`GradientBoostingRegressor` might be preferred for small sample\n  sizes since binning may lead to split points that are too approximate\n  in this setting.\n\n.. _histogram_based_gradient_boosting:"
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_3",
    "header": "Histogram-Based Gradient Boosting",
    "text": "Histogram-Based Gradient Boosting\n----------------------------------\n\nScikit-learn 0.21 introduced two new implementations of\ngradient boosted trees, namely :class:`HistGradientBoostingClassifier`\nand :class:`HistGradientBoostingRegressor`, inspired by\n`LightGBM <https://github.com/Microsoft/LightGBM>`__ (See [LightGBM]_).\n\nThese histogram-based estimators can be **orders of magnitude faster**\nthan :class:`GradientBoostingClassifier` and\n:class:`GradientBoostingRegressor` when the number of samples is larger\nthan tens of thousands of samples.\n\nThey also have built-in support for missing values, which avoids the need\nfor an imputer.\n\nThese fast estimators first bin the input samples ``X`` into\ninteger-valued bins (typically 256 bins) which tremendously reduces the\nnumber of splitting points to consider, and allows the algorithm to\nleverage integer-based data structures (histograms) instead of relying on\nsorted continuous values when building the trees. The API of these\nestimators is slightly different, and some of the features from\n:class:`GradientBoostingClassifier` and :class:`GradientBoostingRegressor`\nare not yet supported, for instance some loss functions.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_inspection_plot_partial_dependence.py`\n* :ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`"
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_4",
    "header": "Usage",
    "text": "Usage\n^^^^^\n\nMost of the parameters are unchanged from\n:class:`GradientBoostingClassifier` and :class:`GradientBoostingRegressor`.\nOne exception is the ``max_iter`` parameter that replaces ``n_estimators``, and\ncontrols the number of iterations of the boosting process::\n\n  >>> from sklearn.ensemble import HistGradientBoostingClassifier\n  >>> from sklearn.datasets import make_hastie_10_2\n\n  >>> X, y = make_hastie_10_2(random_state=0)\n  >>> X_train, X_test = X[:2000], X[2000:]\n  >>> y_train, y_test = y[:2000], y[2000:]\n\n  >>> clf = HistGradientBoostingClassifier(max_iter=100).fit(X_train, y_train)\n  >>> clf.score(X_test, y_test)\n  0.8965\n\nAvailable losses for **regression** are:\n\n- 'squared_error', which is the default loss;\n- 'absolute_error', which is less sensitive to outliers than the squared error;\n- 'gamma', which is well suited to model strictly positive outcomes;\n- 'poisson', which is well suited to model counts and frequencies;\n- 'quantile', which allows for estimating a conditional quantile that can later\n  be used to obtain prediction intervals.\n\nFor **classification**, 'log_loss' is the only option. For binary classification\nit uses the binary log loss, also known as binomial deviance or binary\ncross-entropy. For `n_classes >= 3`, it uses the multi-class log loss function,\nwith multinomial deviance and categorical cross-entropy as alternative names.\nThe appropriate loss version is selected based on :term:`y` passed to\n:term:`fit`.\n\nThe size of the trees can be controlled through the ``max_leaf_nodes``,\n``max_depth``, and ``min_samples_leaf`` parameters.\n\nThe number of bins used to bin the data is controlled with the ``max_bins``\nparameter. Using less bins acts as a form of regularization. It is generally\nrecommended to use as many bins as possible (255), which is the default.\n\nThe ``l2_regularization`` parameter acts as a regularizer for the loss function,\nand corresponds to :math:`\\lambda` in the following expression (see equation (2)\nin [XGBoost]_):\n\n.. math::\n\n    \\mathcal{L}(\\phi) =  \\sum_i l(\\hat{y}_i, y_i) + \\frac12 \\sum_k \\lambda ||w_k||^2\n\n.. dropdown:: Details on l2 regularization\n\n  It is important to notice that the loss term :math:`l(\\hat{y}_i, y_i)` describes\n  only half of the actual loss function except for the pinball loss and absolute\n  error.\n\n  The index :math:`k` refers to the k-th tree in the ensemble of trees. In the\n  case of regression and binary classification, gradient boosting models grow one\n  tree per iteration, then :math:`k` runs up to `max_iter`. In the case of\n  multiclass classification problems, the maximal value of the index :math:`k` is\n  `n_classes` :math:`\\times` `max_iter`.\n\n  If :math:`T_k` denotes the number of leaves in the k-th tree, then :math:`w_k`\n  is a vector of length :math:`T_k`, which contains the leaf values of the form `w\n  = -sum_gradient / (sum_hessian + l2_regularization)` (see equation (5) in\n  [XGBoost]_).\n\n  The leaf values :math:`w_k` are derived by dividing the sum of the gradients of\n  the loss function by the combined sum of hessians. Adding the regularization to\n  the denominator penalizes the leaves with small hessians (flat regions),\n  resulting in smaller updates. Those :math:`w_k` values contribute then to the\n  model's prediction for a given input that ends up in the corresponding leaf. The\n  final prediction is the sum of the base prediction and the contributions from\n  each tree. The result of that sum is then transformed by the inverse link\n  function depending on the choice of the loss function (see\n  :ref:`gradient_boosting_formulation`).\n\n  Notice that the original paper [XGBoost]_ introduces a term :math:`\\gamma\\sum_k\n  T_k` that penalizes the number of leaves (making it a smooth version of\n  `max_leaf_nodes`) not presented here as it is not implemented in scikit-learn;\n  whereas :math:`\\lambda` penalizes the magnitude of the individual tree\n  predictions before being rescaled by the learning rate, see\n  :ref:`gradient_boosting_shrinkage`.\n\n\nNote that **early-stopping is enabled by default if the number of samples is\nlarger than 10,000**. The early-stopping behaviour is controlled via the\n``early_stopping``, ``scoring``, ``validation_fraction``,\n``n_iter_no_change``, and ``tol`` parameters. It is possible to early-stop\nusing an arbitrary :term:`scorer`, or just the training or validation loss.\nNote that for technical reasons, using a callable as a scorer is significantly slower\nthan using the loss. By default, early-stopping is performed if there are at least\n10,000 samples in the training set, using the validation loss.\n\n.. _nan_support_hgbt:"
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_5",
    "header": "Missing values support",
    "text": "Missing values support\n^^^^^^^^^^^^^^^^^^^^^^\n\n:class:`HistGradientBoostingClassifier` and\n:class:`HistGradientBoostingRegressor` have built-in support for missing\nvalues (NaNs).\n\nDuring training, the tree grower learns at each split point whether samples\nwith missing values should go to the left or right child, based on the\npotential gain. When predicting, samples with missing values are assigned to\nthe left or right child consequently::\n\n  >>> from sklearn.ensemble import HistGradientBoostingClassifier\n  >>> import numpy as np\n\n  >>> X = np.array([0, 1, 2, np.nan]).reshape(-1, 1)\n  >>> y = [0, 0, 1, 1]\n\n  >>> gbdt = HistGradientBoostingClassifier(min_samples_leaf=1).fit(X, y)\n  >>> gbdt.predict(X)\n  array([0, 0, 1, 1])\n\nWhen the missingness pattern is predictive, the splits can be performed on\nwhether the feature value is missing or not::\n\n  >>> X = np.array([0, np.nan, 1, 2, np.nan]).reshape(-1, 1)\n  >>> y = [0, 1, 0, 0, 1]\n  >>> gbdt = HistGradientBoostingClassifier(min_samples_leaf=1,\n  ...                                       max_depth=2,\n  ...                                       learning_rate=1,\n  ...                                       max_iter=1).fit(X, y)\n  >>> gbdt.predict(X)\n  array([0, 1, 0, 0, 1])\n\nIf no missing values were encountered for a given feature during training,\nthen samples with missing values are mapped to whichever child has the most\nsamples.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py`\n\n.. _sw_hgbdt:"
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_6",
    "header": "Sample weight support",
    "text": "Sample weight support\n^^^^^^^^^^^^^^^^^^^^^\n\n:class:`HistGradientBoostingClassifier` and\n:class:`HistGradientBoostingRegressor` support sample weights during\n:term:`fit`.\n\nThe following toy example demonstrates that samples with a sample weight of zero are ignored:\n\n    >>> X = [[1, 0],\n    ...      [1, 0],\n    ...      [1, 0],\n    ...      [0, 1]]\n    >>> y = [0, 0, 1, 0]\n    >>> # ignore the first 2 training samples by setting their weight to 0\n    >>> sample_weight = [0, 0, 1, 1]\n    >>> gb = HistGradientBoostingClassifier(min_samples_leaf=1)\n    >>> gb.fit(X, y, sample_weight=sample_weight)\n    HistGradientBoostingClassifier(...)\n    >>> gb.predict([[1, 0]])\n    array([1])\n    >>> gb.predict_proba([[1, 0]])[0, 1]\n    np.float64(0.999)\n\nAs you can see, the `[1, 0]` is comfortably classified as `1` since the first\ntwo samples are ignored due to their sample weights.\n\nImplementation detail: taking sample weights into account amounts to\nmultiplying the gradients (and the hessians) by the sample weights. Note that\nthe binning stage (specifically the quantiles computation) does not take the\nweights into account.\n\n.. _categorical_support_gbdt:"
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_7",
    "header": "Categorical Features Support",
    "text": "Categorical Features Support\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n:class:`HistGradientBoostingClassifier` and\n:class:`HistGradientBoostingRegressor` have native support for categorical\nfeatures: they can consider splits on non-ordered, categorical data.\n\nFor datasets with categorical features, using the native categorical support\nis often better than relying on one-hot encoding\n(:class:`~sklearn.preprocessing.OneHotEncoder`), because one-hot encoding\nrequires more tree depth to achieve equivalent splits. It is also usually\nbetter to rely on the native categorical support rather than to treat\ncategorical features as continuous (ordinal), which happens for ordinal-encoded\ncategorical data, since categories are nominal quantities where order does not\nmatter.\n\nTo enable categorical support, a boolean mask can be passed to the\n`categorical_features` parameter, indicating which feature is categorical. In\nthe following, the first feature will be treated as categorical and the\nsecond feature as numerical::\n\n  >>> gbdt = HistGradientBoostingClassifier(categorical_features=[True, False])\n\nEquivalently, one can pass a list of integers indicating the indices of the\ncategorical features::\n\n  >>> gbdt = HistGradientBoostingClassifier(categorical_features=[0])\n\nWhen the input is a DataFrame, it is also possible to pass a list of column\nnames::\n\n  >>> gbdt = HistGradientBoostingClassifier(categorical_features=[\"site\", \"manufacturer\"])\n\nFinally, when the input is a DataFrame we can use\n`categorical_features=\"from_dtype\"` in which case all columns with a categorical\n`dtype` will be treated as categorical features.\n\nThe cardinality of each categorical feature must be less than the `max_bins`\nparameter. For an example using histogram-based gradient boosting on categorical\nfeatures, see\n:ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_categorical.py`.\n\nIf there are missing values during training, the missing values will be\ntreated as a proper category. If there are no missing values during training,\nthen at prediction time, missing values are mapped to the child node that has\nthe most samples (just like for continuous features). When predicting,\ncategories that were not seen during fit time will be treated as missing\nvalues.\n\n.. dropdown:: Split finding with categorical features\n\n  The canonical way of considering categorical splits in a tree is to consider\n  all of the :math:`2^{K - 1} - 1` partitions, where :math:`K` is the number of\n  categories. This can quickly become prohibitive when :math:`K` is large.\n  Fortunately, since gradient boosting trees are always regression trees (even\n  for classification problems), there exists a faster strategy that can yield\n  equivalent splits. First, the categories of a feature are sorted according to\n  the variance of the target, for each category `k`. Once the categories are\n  sorted, one can consider *continuous partitions*, i.e. treat the categories\n  as if they were ordered continuous values (see Fisher [Fisher1958]_ for a\n  formal proof). As a result, only :math:`K - 1` splits need to be considered\n  instead of :math:`2^{K - 1} - 1`. The initial sorting is a\n  :math:`\\mathcal{O}(K \\log(K))` operation, leading to a total complexity of\n  :math:`\\mathcal{O}(K \\log(K) + K)`, instead of :math:`\\mathcal{O}(2^K)`.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_categorical.py`\n\n.. _monotonic_cst_gbdt:"
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_8",
    "header": "Monotonic Constraints",
    "text": "Monotonic Constraints\n^^^^^^^^^^^^^^^^^^^^^\n\nDepending on the problem at hand, you may have prior knowledge indicating\nthat a given feature should in general have a positive (or negative) effect\non the target value. For example, all else being equal, a higher credit\nscore should increase the probability of getting approved for a loan.\nMonotonic constraints allow you to incorporate such prior knowledge into the\nmodel.\n\nFor a predictor :math:`F` with two features:\n\n- a **monotonic increase constraint** is a constraint of the form:\n\n  .. math::\n      x_1 \\leq x_1' \\implies F(x_1, x_2) \\leq F(x_1', x_2)\n\n- a **monotonic decrease constraint** is a constraint of the form:\n\n  .. math::\n      x_1 \\leq x_1' \\implies F(x_1, x_2) \\geq F(x_1', x_2)\n\nYou can specify a monotonic constraint on each feature using the\n`monotonic_cst` parameter. For each feature, a value of 0 indicates no\nconstraint, while 1 and -1 indicate a monotonic increase and\nmonotonic decrease constraint, respectively::\n\n  >>> from sklearn.ensemble import HistGradientBoostingRegressor\n\n  ... # monotonic increase, monotonic decrease, and no constraint on the 3 features\n  >>> gbdt = HistGradientBoostingRegressor(monotonic_cst=[1, -1, 0])\n\nIn a binary classification context, imposing a monotonic increase (decrease) constraint means that higher values of the feature are supposed\nto have a positive (negative) effect on the probability of samples\nto belong to the positive class.\n\nNevertheless, monotonic constraints only marginally constrain feature effects on the output.\nFor instance, monotonic increase and decrease constraints cannot be used to enforce the\nfollowing modelling constraint:\n\n.. math::\n    x_1 \\leq x_1' \\implies F(x_1, x_2) \\leq F(x_1', x_2')\n\nAlso, monotonic constraints are not supported for multiclass classification.\n\nFor a practical implementation of monotonic constraints with the histogram-based\ngradient boosting, including how they can improve generalization when domain knowledge\nis available, see\n:ref:`sphx_glr_auto_examples_ensemble_plot_monotonic_constraints.py`.\n\n.. note::\n    Since categories are unordered quantities, it is not possible to enforce\n    monotonic constraints on categorical features.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py`\n\n.. _interaction_cst_hgbt:"
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_9",
    "header": "Interaction constraints",
    "text": "Interaction constraints\n^^^^^^^^^^^^^^^^^^^^^^^\n\nA priori, the histogram gradient boosted trees are allowed to use any feature\nto split a node into child nodes. This creates so called interactions between\nfeatures, i.e. usage of different features as split along a branch. Sometimes,\none wants to restrict the possible interactions, see [Mayer2022]_. This can be\ndone by the parameter ``interaction_cst``, where one can specify the indices\nof features that are allowed to interact.\nFor instance, with 3 features in total, ``interaction_cst=[{0}, {1}, {2}]``\nforbids all interactions.\nThe constraints ``[{0, 1}, {1, 2}]`` specify two groups of possibly\ninteracting features. Features 0 and 1 may interact with each other, as well\nas features 1 and 2. But note that features 0 and 2 are forbidden to interact.\nThe following depicts a tree and the possible splits of the tree:\n\n.. code-block:: none\n\n      1      <- Both constraint groups could be applied from now on\n     / \\\n    1   2    <- Left split still fulfills both constraint groups.\n   / \\ / \\      Right split at feature 2 has only group {1, 2} from now on.\n\nLightGBM uses the same logic for overlapping groups.\n\nNote that features not listed in ``interaction_cst`` are automatically\nassigned an interaction group for themselves. With again 3 features, this\nmeans that ``[{0}]`` is equivalent to ``[{0}, {1, 2}]``.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_inspection_plot_partial_dependence.py`\n\n.. rubric:: References\n\n.. [Mayer2022] M. Mayer, S.C. Bourassa, M. Hoesli, and D.F. Scognamiglio.\n    2022. :doi:`Machine Learning Applications to Land and Structure Valuation\n    <10.3390/jrfm15050193>`.\n    Journal of Risk and Financial Management 15, no. 5: 193"
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_10",
    "header": "Low-level parallelism",
    "text": "Low-level parallelism\n^^^^^^^^^^^^^^^^^^^^^\n\n\n:class:`HistGradientBoostingClassifier` and\n:class:`HistGradientBoostingRegressor` use OpenMP\nfor parallelization through Cython. For more details on how to control the\nnumber of threads, please refer to our :ref:`parallelism` notes.\n\nThe following parts are parallelized:\n\n- mapping samples from real values to integer-valued bins (finding the bin\n  thresholds is however sequential)\n- building histograms is parallelized over features\n- finding the best split point at a node is parallelized over features\n- during fit, mapping samples into the left and right children is\n  parallelized over samples\n- gradient and hessians computations are parallelized over samples\n- predicting is parallelized over samples\n\n.. _Why_it's_faster:"
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_11",
    "header": "Why it's faster",
    "text": "Why it's faster\n^^^^^^^^^^^^^^^\n\nThe bottleneck of a gradient boosting procedure is building the decision\ntrees. Building a traditional decision tree (as in the other GBDTs\n:class:`GradientBoostingClassifier` and :class:`GradientBoostingRegressor`)\nrequires sorting the samples at each node (for\neach feature). Sorting is needed so that the potential gain of a split point\ncan be computed efficiently. Splitting a single node has thus a complexity\nof :math:`\\mathcal{O}(n_\\text{features} \\times n \\log(n))` where :math:`n`\nis the number of samples at the node.\n\n:class:`HistGradientBoostingClassifier` and\n:class:`HistGradientBoostingRegressor`, in contrast, do not require sorting the\nfeature values and instead use a data-structure called a histogram, where the\nsamples are implicitly ordered. Building a histogram has a\n:math:`\\mathcal{O}(n)` complexity, so the node splitting procedure has a\n:math:`\\mathcal{O}(n_\\text{features} \\times n)` complexity, much smaller\nthan the previous one. In addition, instead of considering :math:`n` split\npoints, we consider only ``max_bins`` split points, which might be much\nsmaller.\n\nIn order to build histograms, the input data `X` needs to be binned into\ninteger-valued bins. This binning procedure does require sorting the feature\nvalues, but it only happens once at the very beginning of the boosting process\n(not at each node, like in :class:`GradientBoostingClassifier` and\n:class:`GradientBoostingRegressor`).\n\nFinally, many parts of the implementation of\n:class:`HistGradientBoostingClassifier` and\n:class:`HistGradientBoostingRegressor` are parallelized.\n\n.. rubric:: References\n\n.. [XGBoost] Tianqi Chen, Carlos Guestrin, :arxiv:`\"XGBoost: A Scalable Tree\n   Boosting System\" <1603.02754>`\n\n.. [LightGBM] Ke et. al. `\"LightGBM: A Highly Efficient Gradient\n   BoostingDecision Tree\" <https://papers.nips.cc/paper/\n   6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree>`_\n\n.. [Fisher1958] Fisher, W.D. (1958). `\"On Grouping for Maximum Homogeneity\"\n   <http://csiss.ncgia.ucsb.edu/SPACE/workshops/2004/SAC/files/fisher.pdf>`_\n   Journal of the American Statistical Association, 53, 789-798."
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_12",
    "header": ":class:`GradientBoostingClassifier` and :class:`GradientBoostingRegressor`",
    "text": ":class:`GradientBoostingClassifier` and :class:`GradientBoostingRegressor`\n----------------------------------------------------------------------------\n\nThe usage and the parameters of :class:`GradientBoostingClassifier` and\n:class:`GradientBoostingRegressor` are described below. The 2 most important\nparameters of these estimators are `n_estimators` and `learning_rate`.\n\n.. dropdown:: Classification\n\n  :class:`GradientBoostingClassifier` supports both binary and multi-class\n  classification.\n  The following example shows how to fit a gradient boosting classifier\n  with 100 decision stumps as weak learners::\n\n      >>> from sklearn.datasets import make_hastie_10_2\n      >>> from sklearn.ensemble import GradientBoostingClassifier\n\n      >>> X, y = make_hastie_10_2(random_state=0)\n      >>> X_train, X_test = X[:2000], X[2000:]\n      >>> y_train, y_test = y[:2000], y[2000:]\n\n      >>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n      ...     max_depth=1, random_state=0).fit(X_train, y_train)\n      >>> clf.score(X_test, y_test)\n      0.913\n\n  The number of weak learners (i.e. regression trees) is controlled by the\n  parameter ``n_estimators``; :ref:`The size of each tree\n  <gradient_boosting_tree_size>` can be controlled either by setting the tree\n  depth via ``max_depth`` or by setting the number of leaf nodes via\n  ``max_leaf_nodes``. The ``learning_rate`` is a hyper-parameter in the range\n  (0.0, 1.0] that controls overfitting via :ref:`shrinkage\n  <gradient_boosting_shrinkage>` .\n\n  .. note::\n\n    Classification with more than 2 classes requires the induction\n    of ``n_classes`` regression trees at each iteration,\n    thus, the total number of induced trees equals\n    ``n_classes * n_estimators``. For datasets with a large number\n    of classes we strongly recommend to use\n    :class:`HistGradientBoostingClassifier` as an alternative to\n    :class:`GradientBoostingClassifier` .\n\n.. dropdown:: Regression\n\n  :class:`GradientBoostingRegressor` supports a number of\n  :ref:`different loss functions <gradient_boosting_loss>`\n  for regression which can be specified via the argument\n  ``loss``; the default loss function for regression is squared error\n  (``'squared_error'``).\n\n  ::\n\n      >>> import numpy as np\n      >>> from sklearn.metrics import mean_squared_error\n      >>> from sklearn.datasets import make_friedman1\n      >>> from sklearn.ensemble import GradientBoostingRegressor\n\n      >>> X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n      >>> X_train, X_test = X[:200], X[200:]\n      >>> y_train, y_test = y[:200], y[200:]\n      >>> est = GradientBoostingRegressor(\n      ...     n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0,\n      ...     loss='squared_error'\n      ... ).fit(X_train, y_train)\n      >>> mean_squared_error(y_test, est.predict(X_test))\n      5.00\n\n  The figure below shows the results of applying :class:`GradientBoostingRegressor`\n  with least squares loss and 500 base learners to the diabetes dataset\n  (:func:`sklearn.datasets.load_diabetes`).\n  The plot shows the train and test error at each iteration.\n  The train error at each iteration is stored in the\n  `train_score_` attribute of the gradient boosting model.\n  The test error at each iteration can be obtained\n  via the :meth:`~GradientBoostingRegressor.staged_predict` method which returns a\n  generator that yields the predictions at each stage. Plots like these can be used\n  to determine the optimal number of trees (i.e. ``n_estimators``) by early stopping.\n\n  .. figure:: ../auto_examples/ensemble/images/sphx_glr_plot_gradient_boosting_regression_001.png\n    :target: ../auto_examples/ensemble/plot_gradient_boosting_regression.html\n    :align: center\n    :scale: 75\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_regression.py`\n* :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_oob.py`\n\n.. _gradient_boosting_warm_start:"
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_13",
    "header": "Fitting additional weak-learners",
    "text": "Fitting additional weak-learners\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nBoth :class:`GradientBoostingRegressor` and :class:`GradientBoostingClassifier`\nsupport ``warm_start=True`` which allows you to add more estimators to an already\nfitted model.\n\n::\n\n  >>> import numpy as np\n  >>> from sklearn.metrics import mean_squared_error\n  >>> from sklearn.datasets import make_friedman1\n  >>> from sklearn.ensemble import GradientBoostingRegressor\n\n  >>> X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n  >>> X_train, X_test = X[:200], X[200:]\n  >>> y_train, y_test = y[:200], y[200:]\n  >>> est = GradientBoostingRegressor(\n  ...     n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0,\n  ...     loss='squared_error'\n  ... )\n  >>> est = est.fit(X_train, y_train)  # fit with 100 trees\n  >>> mean_squared_error(y_test, est.predict(X_test))\n  5.00\n  >>> _ = est.set_params(n_estimators=200, warm_start=True)  # set warm_start and increase num of trees\n  >>> _ = est.fit(X_train, y_train) # fit additional 100 trees to est\n  >>> mean_squared_error(y_test, est.predict(X_test))\n  3.84\n\n.. _gradient_boosting_tree_size:"
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_14",
    "header": "Controlling the tree size",
    "text": "Controlling the tree size\n^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe size of the regression tree base learners defines the level of variable\ninteractions that can be captured by the gradient boosting model. In general,\na tree of depth ``h`` can capture interactions of order ``h`` .\nThere are two ways in which the size of the individual regression trees can\nbe controlled.\n\nIf you specify ``max_depth=h`` then complete binary trees\nof depth ``h`` will be grown. Such trees will have (at most) ``2**h`` leaf nodes\nand ``2**h - 1`` split nodes.\n\nAlternatively, you can control the tree size by specifying the number of\nleaf nodes via the parameter ``max_leaf_nodes``. In this case,\ntrees will be grown using best-first search where nodes with the highest improvement\nin impurity will be expanded first.\nA tree with ``max_leaf_nodes=k`` has ``k - 1`` split nodes and thus can\nmodel interactions of up to order ``max_leaf_nodes - 1`` .\n\nWe found that ``max_leaf_nodes=k`` gives comparable results to ``max_depth=k-1``\nbut is significantly faster to train at the expense of a slightly higher\ntraining error.\nThe parameter ``max_leaf_nodes`` corresponds to the variable ``J`` in the\nchapter on gradient boosting in [Friedman2001]_ and is related to the parameter\n``interaction.depth`` in R's gbm package where ``max_leaf_nodes == interaction.depth + 1`` .\n\n.. _gradient_boosting_formulation:"
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_15",
    "header": "Mathematical formulation",
    "text": "Mathematical formulation\n^^^^^^^^^^^^^^^^^^^^^^^^\n\nWe first present GBRT for regression, and then detail the classification\ncase.\n\n.. dropdown:: Regression\n\n  GBRT regressors are additive models whose prediction :math:`\\hat{y}_i` for a\n  given input :math:`x_i` is of the following form:\n\n  .. math::\n\n    \\hat{y}_i = F_M(x_i) = \\sum_{m=1}^{M} h_m(x_i)\n\n  where the :math:`h_m` are estimators called *weak learners* in the context\n  of boosting. Gradient Tree Boosting uses :ref:`decision tree regressors\n  <tree>` of fixed size as weak learners. The constant M corresponds to the\n  `n_estimators` parameter.\n\n  Similar to other boosting algorithms, a GBRT is built in a greedy fashion:\n\n  .. math::\n\n    F_m(x) = F_{m-1}(x) + h_m(x),\n\n  where the newly added tree :math:`h_m` is fitted in order to minimize a sum\n  of losses :math:`L_m`, given the previous ensemble :math:`F_{m-1}`:\n\n  .. math::\n\n    h_m =  \\arg\\min_{h} L_m = \\arg\\min_{h} \\sum_{i=1}^{n}\n    l(y_i, F_{m-1}(x_i) + h(x_i)),\n\n  where :math:`l(y_i, F(x_i))` is defined by the `loss` parameter, detailed\n  in the next section.\n\n  By default, the initial model :math:`F_{0}` is chosen as the constant that\n  minimizes the loss: for a least-squares loss, this is the empirical mean of\n  the target values. The initial model can also be specified via the ``init``\n  argument.\n\n  Using a first-order Taylor approximation, the value of :math:`l` can be\n  approximated as follows:\n\n  .. math::\n\n    l(y_i, F_{m-1}(x_i) + h_m(x_i)) \\approx\n    l(y_i, F_{m-1}(x_i))\n    + h_m(x_i)\n    \\left[ \\frac{\\partial l(y_i, F(x_i))}{\\partial F(x_i)} \\right]_{F=F_{m - 1}}.\n\n  .. note::\n\n    Briefly, a first-order Taylor approximation says that\n    :math:`l(z) \\approx l(a) + (z - a) \\frac{\\partial l}{\\partial z}(a)`.\n    Here, :math:`z` corresponds to :math:`F_{m - 1}(x_i) + h_m(x_i)`, and\n    :math:`a` corresponds to :math:`F_{m-1}(x_i)`\n\n  The quantity :math:`\\left[ \\frac{\\partial l(y_i, F(x_i))}{\\partial F(x_i)}\n  \\right]_{F=F_{m - 1}}` is the derivative of the loss with respect to its\n  second parameter, evaluated at :math:`F_{m-1}(x)`. It is easy to compute for\n  any given :math:`F_{m - 1}(x_i)` in a closed form since the loss is\n  differentiable. We will denote it by :math:`g_i`.\n\n  Removing the constant terms, we have:\n\n  .. math::\n\n    h_m \\approx \\arg\\min_{h} \\sum_{i=1}^{n} h(x_i) g_i\n\n  This is minimized if :math:`h(x_i)` is fitted to predict a value that is\n  proportional to the negative gradient :math:`-g_i`. Therefore, at each\n  iteration, **the estimator** :math:`h_m` **is fitted to predict the negative\n  gradients of the samples**. The gradients are updated at each iteration.\n  This can be considered as some kind of gradient descent in a functional\n  space.\n\n  .. note::\n\n    For some losses, e.g. ``'absolute_error'`` where the gradients\n    are :math:`\\pm 1`, the values predicted by a fitted :math:`h_m` are not\n    accurate enough: the tree can only output integer values. As a result, the\n    leaves values of the tree :math:`h_m` are modified once the tree is\n    fitted, such that the leaves values minimize the loss :math:`L_m`. The\n    update is loss-dependent: for the absolute error loss, the value of\n    a leaf is updated to the median of the samples in that leaf.\n\n.. dropdown:: Classification\n\n  Gradient boosting for classification is very similar to the regression case.\n  However, the sum of the trees :math:`F_M(x_i) = \\sum_m h_m(x_i)` is not\n  homogeneous to a prediction: it cannot be a class, since the trees predict\n  continuous values.\n\n  The mapping from the value :math:`F_M(x_i)` to a class or a probability is\n  loss-dependent. For the log-loss, the probability that\n  :math:`x_i` belongs to the positive class is modeled as :math:`p(y_i = 1 |\n  x_i) = \\sigma(F_M(x_i))` where :math:`\\sigma` is the sigmoid or expit function.\n\n  For multiclass classification, K trees (for K classes) are built at each of\n  the :math:`M` iterations. The probability that :math:`x_i` belongs to class\n  k is modeled as a softmax of the :math:`F_{M,k}(x_i)` values.\n\n  Note that even for a classification task, the :math:`h_m` sub-estimator is\n  still a regressor, not a classifier. This is because the sub-estimators are\n  trained to predict (negative) *gradients*, which are always continuous\n  quantities.\n\n.. _gradient_boosting_loss:"
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_16",
    "header": "Loss Functions",
    "text": "Loss Functions\n^^^^^^^^^^^^^^\n\nThe following loss functions are supported and can be specified using\nthe parameter ``loss``:\n\n.. dropdown:: Regression\n\n  * Squared error (``'squared_error'``): The natural choice for regression\n    due to its superior computational properties. The initial model is\n    given by the mean of the target values.\n  * Absolute error (``'absolute_error'``): A robust loss function for\n    regression. The initial model is given by the median of the\n    target values.\n  * Huber (``'huber'``): Another robust loss function that combines\n    least squares and least absolute deviation; use ``alpha`` to\n    control the sensitivity with regards to outliers (see [Friedman2001]_ for\n    more details).\n  * Quantile (``'quantile'``): A loss function for quantile regression.\n    Use ``0 < alpha < 1`` to specify the quantile. This loss function\n    can be used to create prediction intervals\n    (see :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_quantile.py`).\n\n.. dropdown:: Classification\n\n  * Binary log-loss (``'log-loss'``): The binomial\n    negative log-likelihood loss function for binary classification. It provides\n    probability estimates.  The initial model is given by the\n    log odds-ratio.\n  * Multi-class log-loss (``'log-loss'``): The multinomial\n    negative log-likelihood loss function for multi-class classification with\n    ``n_classes`` mutually exclusive classes. It provides\n    probability estimates.  The initial model is given by the\n    prior probability of each class. At each iteration ``n_classes``\n    regression trees have to be constructed which makes GBRT rather\n    inefficient for data sets with a large number of classes.\n  * Exponential loss (``'exponential'``): The same loss function\n    as :class:`AdaBoostClassifier`. Less robust to mislabeled\n    examples than ``'log-loss'``; can only be used for binary\n    classification.\n\n.. _gradient_boosting_shrinkage:"
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_17",
    "header": "Shrinkage via learning rate",
    "text": "Shrinkage via learning rate\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n[Friedman2001]_ proposed a simple regularization strategy that scales\nthe contribution of each weak learner by a constant factor :math:`\\nu`:\n\n.. math::\n\n    F_m(x) = F_{m-1}(x) + \\nu h_m(x)\n\nThe parameter :math:`\\nu` is also called the **learning rate** because\nit scales the step length of the gradient descent procedure; it can\nbe set via the ``learning_rate`` parameter.\n\nThe parameter ``learning_rate`` strongly interacts with the parameter\n``n_estimators``, the number of weak learners to fit. Smaller values\nof ``learning_rate`` require larger numbers of weak learners to maintain\na constant training error. Empirical evidence suggests that small\nvalues of ``learning_rate`` favor better test error. [HTF]_\nrecommend to set the learning rate to a small constant\n(e.g. ``learning_rate <= 0.1``) and choose ``n_estimators`` large enough\nthat early stopping applies,\nsee :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_early_stopping.py`\nfor a more detailed discussion of the interaction between\n``learning_rate`` and ``n_estimators`` see [R2007]_."
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_18",
    "header": "Subsampling",
    "text": "Subsampling\n^^^^^^^^^^^^\n\n[Friedman2002]_ proposed stochastic gradient boosting, which combines gradient\nboosting with bootstrap averaging (bagging). At each iteration\nthe base classifier is trained on a fraction ``subsample`` of\nthe available training data. The subsample is drawn without replacement.\nA typical value of ``subsample`` is 0.5.\n\nThe figure below illustrates the effect of shrinkage and subsampling\non the goodness-of-fit of the model. We can clearly see that shrinkage\noutperforms no-shrinkage. Subsampling with shrinkage can further increase\nthe accuracy of the model. Subsampling without shrinkage, on the other hand,\ndoes poorly.\n\n.. figure:: ../auto_examples/ensemble/images/sphx_glr_plot_gradient_boosting_regularization_001.png\n   :target: ../auto_examples/ensemble/plot_gradient_boosting_regularization.html\n   :align: center\n   :scale: 75\n\nAnother strategy to reduce the variance is by subsampling the features\nanalogous to the random splits in :class:`RandomForestClassifier`.\nThe number of subsampled features can be controlled via the ``max_features``\nparameter.\n\n.. note:: Using a small ``max_features`` value can significantly decrease the runtime.\n\nStochastic gradient boosting allows to compute out-of-bag estimates of the\ntest deviance by computing the improvement in deviance on the examples that are\nnot included in the bootstrap sample (i.e. the out-of-bag examples).\nThe improvements are stored in the attribute `oob_improvement_`.\n``oob_improvement_[i]`` holds the improvement in terms of the loss on the OOB samples\nif you add the i-th stage to the current predictions.\nOut-of-bag estimates can be used for model selection, for example to determine\nthe optimal number of iterations. OOB estimates are usually very pessimistic thus\nwe recommend to use cross-validation instead and only use OOB if cross-validation\nis too time consuming.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_regularization.py`\n* :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_oob.py`\n* :ref:`sphx_glr_auto_examples_ensemble_plot_ensemble_oob.py`"
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_19",
    "header": "Interpretation with feature importance",
    "text": "Interpretation with feature importance\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nIndividual decision trees can be interpreted easily by simply\nvisualizing the tree structure. Gradient boosting models, however,\ncomprise hundreds of regression trees thus they cannot be easily\ninterpreted by visual inspection of the individual trees. Fortunately,\na number of techniques have been proposed to summarize and interpret\ngradient boosting models.\n\nOften features do not contribute equally to predict the target\nresponse; in many situations the majority of the features are in fact\nirrelevant.\nWhen interpreting a model, the first question usually is: what are\nthose important features and how do they contribute in predicting\nthe target response?\n\nIndividual decision trees intrinsically perform feature selection by selecting\nappropriate split points. This information can be used to measure the\nimportance of each feature; the basic idea is: the more often a\nfeature is used in the split points of a tree the more important that\nfeature is. This notion of importance can be extended to decision tree\nensembles by simply averaging the impurity-based feature importance of each tree (see\n:ref:`random_forest_feature_importance` for more details).\n\nThe feature importance scores of a fit gradient boosting model can be\naccessed via the ``feature_importances_`` property::\n\n    >>> from sklearn.datasets import make_hastie_10_2\n    >>> from sklearn.ensemble import GradientBoostingClassifier\n\n    >>> X, y = make_hastie_10_2(random_state=0)\n    >>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n    ...     max_depth=1, random_state=0).fit(X, y)\n    >>> clf.feature_importances_\n    array([0.107, 0.105, 0.113, 0.0987, 0.0947,\n           0.107, 0.0916, 0.0972, 0.0958, 0.0906])\n\nNote that this computation of feature importance is based on entropy, and it\nis distinct from :func:`sklearn.inspection.permutation_importance` which is\nbased on permutation of the features.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_regression.py`\n\n.. rubric:: References\n\n.. [Friedman2001] Friedman, J.H. (2001). :doi:`Greedy function approximation: A gradient\n   boosting machine <10.1214/aos/1013203451>`.\n   Annals of Statistics, 29, 1189-1232.\n\n.. [Friedman2002] Friedman, J.H. (2002). `Stochastic gradient boosting.\n   <https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=48caac2f65bce47f6d27400ae4f60d8395cec2f3>`_.\n   Computational Statistics & Data Analysis, 38, 367-378.\n\n.. [R2007] G. Ridgeway (2006). `Generalized Boosted Models: A guide to the gbm\n   package <https://cran.r-project.org/web/packages/gbm/vignettes/gbm.pdf>`_\n\n.. _forest:"
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_20",
    "header": "Random forests and other randomized tree ensembles",
    "text": "Random forests and other randomized tree ensembles\n===================================================\n\nThe :mod:`sklearn.ensemble` module includes two averaging algorithms based\non randomized :ref:`decision trees <tree>`: the RandomForest algorithm\nand the Extra-Trees method. Both algorithms are perturb-and-combine\ntechniques [B1998]_ specifically designed for trees. This means a diverse\nset of classifiers is created by introducing randomness in the classifier\nconstruction.  The prediction of the ensemble is given as the averaged\nprediction of the individual classifiers.\n\nAs other classifiers, forest classifiers have to be fitted with two\narrays: a sparse or dense array X of shape ``(n_samples, n_features)``\nholding the training samples, and an array Y of shape ``(n_samples,)``\nholding the target values (class labels) for the training samples::\n\n    >>> from sklearn.ensemble import RandomForestClassifier\n    >>> X = [[0, 0], [1, 1]]\n    >>> Y = [0, 1]\n    >>> clf = RandomForestClassifier(n_estimators=10)\n    >>> clf = clf.fit(X, Y)\n\nLike :ref:`decision trees <tree>`, forests of trees also extend to\n:ref:`multi-output problems <tree_multioutput>`  (if Y is an array\nof shape ``(n_samples, n_outputs)``)."
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_21",
    "header": "Random Forests",
    "text": "Random Forests\n--------------\n\nIn random forests (see :class:`RandomForestClassifier` and\n:class:`RandomForestRegressor` classes), each tree in the ensemble is built\nfrom a sample drawn with replacement (i.e., a bootstrap sample) from the\ntraining set.\n\nFurthermore, when splitting each node during the construction of a tree, the\nbest split is found through an exhaustive search of the feature values of\neither all input features or a random subset of size ``max_features``.\n(See the :ref:`parameter tuning guidelines <random_forest_parameters>` for more details.)\n\nThe purpose of these two sources of randomness is to decrease the variance of\nthe forest estimator. Indeed, individual decision trees typically exhibit high\nvariance and tend to overfit. The injected randomness in forests yield decision\ntrees with somewhat decoupled prediction errors. By taking an average of those\npredictions, some errors can cancel out. Random forests achieve a reduced\nvariance by combining diverse trees, sometimes at the cost of a slight increase\nin bias. In practice the variance reduction is often significant hence yielding\nan overall better model.\n\nIn contrast to the original publication [B2001]_, the scikit-learn\nimplementation combines classifiers by averaging their probabilistic\nprediction, instead of letting each classifier vote for a single class.\n\nA competitive alternative to random forests are\n:ref:`histogram_based_gradient_boosting` (HGBT) models:\n\n-  Building trees: Random forests typically rely on deep trees (that overfit\n   individually) which uses much computational resources, as they require\n   several splittings and evaluations of candidate splits. Boosting models\n   build shallow trees (that underfit individually) which are faster to fit\n   and predict.\n\n-  Sequential boosting: In HGBT, the decision trees are built sequentially,\n   where each tree is trained to correct the errors made by the previous ones.\n   This allows them to iteratively improve the model's performance using\n   relatively few trees. In contrast, random forests use a majority vote to\n   predict the outcome, which can require a larger number of trees to achieve\n   the same level of accuracy.\n\n-  Efficient binning: HGBT uses an efficient binning algorithm that can handle\n   large datasets with a high number of features. The binning algorithm can\n   pre-process the data to speed up the subsequent tree construction (see\n   :ref:`Why it's faster <Why_it's_faster>`). In contrast, the scikit-learn\n   implementation of random forests does not use binning and relies on exact\n   splitting, which can be computationally expensive.\n\nOverall, the computational cost of HGBT versus RF depends on the specific\ncharacteristics of the dataset and the modeling task. It's a good idea\nto try both models and compare their performance and computational efficiency\non your specific problem to determine which model is the best fit.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`"
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_22",
    "header": "Extremely Randomized Trees",
    "text": "Extremely Randomized Trees\n--------------------------\n\nIn extremely randomized trees (see :class:`ExtraTreesClassifier`\nand :class:`ExtraTreesRegressor` classes), randomness goes one step\nfurther in the way splits are computed. As in random forests, a random\nsubset of candidate features is used, but instead of looking for the\nmost discriminative thresholds, thresholds are drawn at random for each\ncandidate feature and the best of these randomly-generated thresholds is\npicked as the splitting rule. This usually allows to reduce the variance\nof the model a bit more, at the expense of a slightly greater increase\nin bias::\n\n    >>> from sklearn.model_selection import cross_val_score\n    >>> from sklearn.datasets import make_blobs\n    >>> from sklearn.ensemble import RandomForestClassifier\n    >>> from sklearn.ensemble import ExtraTreesClassifier\n    >>> from sklearn.tree import DecisionTreeClassifier\n\n    >>> X, y = make_blobs(n_samples=10000, n_features=10, centers=100,\n    ...     random_state=0)\n\n    >>> clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,\n    ...     random_state=0)\n    >>> scores = cross_val_score(clf, X, y, cv=5)\n    >>> scores.mean()\n    np.float64(0.98)\n\n    >>> clf = RandomForestClassifier(n_estimators=10, max_depth=None,\n    ...     min_samples_split=2, random_state=0)\n    >>> scores = cross_val_score(clf, X, y, cv=5)\n    >>> scores.mean()\n    np.float64(0.999)\n\n    >>> clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,\n    ...     min_samples_split=2, random_state=0)\n    >>> scores = cross_val_score(clf, X, y, cv=5)\n    >>> scores.mean() > 0.999\n    np.True_\n\n.. figure:: ../auto_examples/ensemble/images/sphx_glr_plot_forest_iris_001.png\n    :target: ../auto_examples/ensemble/plot_forest_iris.html\n    :align: center\n    :scale: 75%\n\n.. _random_forest_parameters:"
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_23",
    "header": "Parameters",
    "text": "Parameters\n----------\n\nThe main parameters to adjust when using these methods is ``n_estimators`` and\n``max_features``. The former is the number of trees in the forest. The larger\nthe better, but also the longer it will take to compute. In addition, note that\nresults will stop getting significantly better beyond a critical number of\ntrees. The latter is the size of the random subsets of features to consider\nwhen splitting a node. The lower the greater the reduction of variance, but\nalso the greater the increase in bias. Empirical good default values are\n``max_features=1.0`` or equivalently ``max_features=None`` (always considering\nall features instead of a random subset) for regression problems, and\n``max_features=\"sqrt\"`` (using a random subset of size ``sqrt(n_features)``)\nfor classification tasks (where ``n_features`` is the number of features in\nthe data). The default value of ``max_features=1.0`` is equivalent to bagged\ntrees and more randomness can be achieved by setting smaller values (e.g. 0.3\nis a typical default in the literature). Good results are often achieved when\nsetting ``max_depth=None`` in combination with ``min_samples_split=2`` (i.e.,\nwhen fully developing the trees). Bear in mind though that these values are\nusually not optimal, and might result in models that consume a lot of RAM.\nThe best parameter values should always be cross-validated. In addition, note\nthat in random forests, bootstrap samples are used by default\n(``bootstrap=True``) while the default strategy for extra-trees is to use the\nwhole dataset (``bootstrap=False``). When using bootstrap sampling the\ngeneralization error can be estimated on the left out or out-of-bag samples.\nThis can be enabled by setting ``oob_score=True``.\n\n.. note::\n\n    The size of the model with the default parameters is :math:`O( M * N * log (N) )`,\n    where :math:`M` is the number of trees and :math:`N` is the number of samples.\n    In order to reduce the size of the model, you can change these parameters:\n    ``min_samples_split``, ``max_leaf_nodes``, ``max_depth`` and ``min_samples_leaf``."
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_24",
    "header": "Parallelization",
    "text": "Parallelization\n---------------\n\nFinally, this module also features the parallel construction of the trees\nand the parallel computation of the predictions through the ``n_jobs``\nparameter. If ``n_jobs=k`` then computations are partitioned into\n``k`` jobs, and run on ``k`` cores of the machine. If ``n_jobs=-1``\nthen all cores available on the machine are used. Note that because of\ninter-process communication overhead, the speedup might not be linear\n(i.e., using ``k`` jobs will unfortunately not be ``k`` times as\nfast). Significant speedup can still be achieved though when building\na large number of trees, or when building a single tree requires a fair\namount of time (e.g., on large datasets).\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_ensemble_plot_forest_iris.py`\n* :ref:`sphx_glr_auto_examples_miscellaneous_plot_multioutput_face_completion.py`\n\n.. rubric:: References\n\n.. [B2001] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n\n.. [B1998] L. Breiman, \"Arcing Classifiers\", Annals of Statistics 1998.\n\n* P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n  trees\", Machine Learning, 63(1), 3-42, 2006.\n\n.. _random_forest_feature_importance:"
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_25",
    "header": "Feature importance evaluation",
    "text": "Feature importance evaluation\n-----------------------------\n\nThe relative rank (i.e. depth) of a feature used as a decision node in a\ntree can be used to assess the relative importance of that feature with\nrespect to the predictability of the target variable. Features used at\nthe top of the tree contribute to the final prediction decision of a\nlarger fraction of the input samples. The **expected fraction of the\nsamples** they contribute to can thus be used as an estimate of the\n**relative importance of the features**. In scikit-learn, the fraction of\nsamples a feature contributes to is combined with the decrease in impurity\nfrom splitting them to create a normalized estimate of the predictive power\nof that feature.\n\nBy **averaging** the estimates of predictive ability over several randomized\ntrees one can **reduce the variance** of such an estimate and use it\nfor feature selection. This is known as the mean decrease in impurity, or MDI.\nRefer to [L2014]_ for more information on MDI and feature importance\nevaluation with Random Forests.\n\n.. warning::\n\n  The impurity-based feature importances computed on tree-based models suffer\n  from two flaws that can lead to misleading conclusions. First they are\n  computed on statistics derived from the training dataset and therefore **do\n  not necessarily inform us on which features are most important to make good\n  predictions on held-out dataset**. Secondly, **they favor high cardinality\n  features**, that is features with many unique values.\n  :ref:`permutation_importance` is an alternative to impurity-based feature\n  importance that does not suffer from these flaws. These two methods of\n  obtaining feature importance are explored in:\n  :ref:`sphx_glr_auto_examples_inspection_plot_permutation_importance.py`.\n\nIn practice those estimates are stored as an attribute named\n``feature_importances_`` on the fitted model. This is an array with shape\n``(n_features,)`` whose values are positive and sum to 1.0. The higher\nthe value, the more important is the contribution of the matching feature\nto the prediction function.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_ensemble_plot_forest_importances.py`\n\n.. rubric:: References\n\n.. [L2014] G. Louppe, :arxiv:`\"Understanding Random Forests: From Theory to\n   Practice\" <1407.7502>`,\n   PhD Thesis, U. of Liege, 2014.\n\n.. _random_trees_embedding:"
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_26",
    "header": "Totally Random Trees Embedding",
    "text": "Totally Random Trees Embedding\n------------------------------\n\n:class:`RandomTreesEmbedding` implements an unsupervised transformation of the\ndata.  Using a forest of completely random trees, :class:`RandomTreesEmbedding`\nencodes the data by the indices of the leaves a data point ends up in.  This\nindex is then encoded in a one-of-K manner, leading to a high dimensional,\nsparse binary coding.\nThis coding can be computed very efficiently and can then be used as a basis\nfor other learning tasks.\nThe size and sparsity of the code can be influenced by choosing the number of\ntrees and the maximum depth per tree. For each tree in the ensemble, the coding\ncontains one entry of one. The size of the coding is at most ``n_estimators * 2\n** max_depth``, the maximum number of leaves in the forest.\n\nAs neighboring data points are more likely to lie within the same leaf of a\ntree, the transformation performs an implicit, non-parametric density\nestimation.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_ensemble_plot_random_forest_embedding.py`\n\n* :ref:`sphx_glr_auto_examples_manifold_plot_lle_digits.py` compares non-linear\n  dimensionality reduction techniques on handwritten digits.\n\n* :ref:`sphx_glr_auto_examples_ensemble_plot_feature_transformation.py` compares\n  supervised and unsupervised tree based feature transformations.\n\n.. seealso::\n\n   :ref:`manifold` techniques can also be useful to derive non-linear\n   representations of feature space, also these approaches focus also on\n   dimensionality reduction.\n\n.. _tree_ensemble_warm_start:"
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_27",
    "header": "Fitting additional trees",
    "text": "Fitting additional trees\n------------------------\n\nRandomForest, Extra-Trees and :class:`RandomTreesEmbedding` estimators all support\n``warm_start=True`` which allows you to add more trees to an already fitted model.\n\n::\n\n  >>> from sklearn.datasets import make_classification\n  >>> from sklearn.ensemble import RandomForestClassifier\n\n  >>> X, y = make_classification(n_samples=100, random_state=1)\n  >>> clf = RandomForestClassifier(n_estimators=10)\n  >>> clf = clf.fit(X, y)  # fit with 10 trees\n  >>> len(clf.estimators_)\n  10\n  >>> # set warm_start and increase num of estimators\n  >>> _ = clf.set_params(n_estimators=20, warm_start=True)\n  >>> _ = clf.fit(X, y) # fit additional 10 trees\n  >>> len(clf.estimators_)\n  20\n\nWhen ``random_state`` is also set, the internal random state is also preserved\nbetween ``fit`` calls. This means that training a model once with ``n`` estimators is\nthe same as building the model iteratively via multiple ``fit`` calls, where the\nfinal number of estimators is equal to ``n``.\n\n::\n\n  >>> clf = RandomForestClassifier(n_estimators=20)  # set `n_estimators` to 10 + 10\n  >>> _ = clf.fit(X, y)  # fit `estimators_` will be the same as `clf` above\n\nNote that this differs from the usual behavior of :term:`random_state` in that it does\n*not* result in the same result across different calls.\n\n.. _bagging:"
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_28",
    "header": "Bagging meta-estimator",
    "text": "Bagging meta-estimator\n======================\n\nIn ensemble algorithms, bagging methods form a class of algorithms which build\nseveral instances of a black-box estimator on random subsets of the original\ntraining set and then aggregate their individual predictions to form a final\nprediction. These methods are used as a way to reduce the variance of a base\nestimator (e.g., a decision tree), by introducing randomization into its\nconstruction procedure and then making an ensemble out of it. In many cases,\nbagging methods constitute a very simple way to improve with respect to a\nsingle model, without making it necessary to adapt the underlying base\nalgorithm. As they provide a way to reduce overfitting, bagging methods work\nbest with strong and complex models (e.g., fully developed decision trees), in\ncontrast with boosting methods which usually work best with weak models (e.g.,\nshallow decision trees).\n\nBagging methods come in many flavours but mostly differ from each other by the\nway they draw random subsets of the training set:\n\n* When random subsets of the dataset are drawn as random subsets of the\n  samples, then this algorithm is known as Pasting [B1999]_.\n\n* When samples are drawn with replacement, then the method is known as\n  Bagging [B1996]_.\n\n* When random subsets of the dataset are drawn as random subsets of\n  the features, then the method is known as Random Subspaces [H1998]_.\n\n* Finally, when base estimators are built on subsets of both samples and\n  features, then the method is known as Random Patches [LG2012]_.\n\nIn scikit-learn, bagging methods are offered as a unified\n:class:`BaggingClassifier` meta-estimator  (resp. :class:`BaggingRegressor`),\ntaking as input a user-specified estimator along with parameters\nspecifying the strategy to draw random subsets. In particular, ``max_samples``\nand ``max_features`` control the size of the subsets (in terms of samples and\nfeatures), while ``bootstrap`` and ``bootstrap_features`` control whether\nsamples and features are drawn with or without replacement. When using a subset\nof the available samples the generalization accuracy can be estimated with the\nout-of-bag samples by setting ``oob_score=True``. As an example, the\nsnippet below illustrates how to instantiate a bagging ensemble of\n:class:`~sklearn.neighbors.KNeighborsClassifier` estimators, each built on random\nsubsets of 50% of the samples and 50% of the features.\n\n    >>> from sklearn.ensemble import BaggingClassifier\n    >>> from sklearn.neighbors import KNeighborsClassifier\n    >>> bagging = BaggingClassifier(KNeighborsClassifier(),\n    ...                             max_samples=0.5, max_features=0.5)\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_ensemble_plot_bias_variance.py`\n\n.. rubric:: References\n\n.. [B1999] L. Breiman, \"Pasting small votes for classification in large\n   databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n\n.. [B1996] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2),\n   123-140, 1996.\n\n.. [H1998] T. Ho, \"The random subspace method for constructing decision\n   forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844, 1998.\n\n.. [LG2012] G. Louppe and P. Geurts, \"Ensembles on Random Patches\",\n   Machine Learning and Knowledge Discovery in Databases, 346-361, 2012.\n\n\n\n.. _voting_classifier:"
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_29",
    "header": "Voting Classifier",
    "text": "Voting Classifier\n========================\n\nThe idea behind the :class:`VotingClassifier` is to combine\nconceptually different machine learning classifiers and use a majority vote\nor the average predicted probabilities (soft vote) to predict the class labels.\nSuch a classifier can be useful for a set of equally well performing models\nin order to balance out their individual weaknesses."
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_30",
    "header": "Majority Class Labels (Majority/Hard Voting)",
    "text": "Majority Class Labels (Majority/Hard Voting)\n--------------------------------------------\n\nIn majority voting, the predicted class label for a particular sample is\nthe class label that represents the majority (mode) of the class labels\npredicted by each individual classifier.\n\nE.g., if the prediction for a given sample is\n\n- classifier 1 -> class 1\n- classifier 2 -> class 1\n- classifier 3 -> class 2\n\nthe VotingClassifier (with ``voting='hard'``) would classify the sample\nas \"class 1\" based on the majority class label.\n\nIn the cases of a tie, the :class:`VotingClassifier` will select the class\nbased on the ascending sort order. E.g., in the following scenario\n\n- classifier 1 -> class 2\n- classifier 2 -> class 1\n\nthe class label 1 will be assigned to the sample."
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_31",
    "header": "Usage",
    "text": "Usage\n-----\n\nThe following example shows how to fit the majority rule classifier::\n\n   >>> from sklearn import datasets\n   >>> from sklearn.model_selection import cross_val_score\n   >>> from sklearn.linear_model import LogisticRegression\n   >>> from sklearn.naive_bayes import GaussianNB\n   >>> from sklearn.ensemble import RandomForestClassifier\n   >>> from sklearn.ensemble import VotingClassifier\n\n   >>> iris = datasets.load_iris()\n   >>> X, y = iris.data[:, 1:3], iris.target\n\n   >>> clf1 = LogisticRegression(random_state=1)\n   >>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n   >>> clf3 = GaussianNB()\n\n   >>> eclf = VotingClassifier(\n   ...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n   ...     voting='hard')\n\n   >>> for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n   ...     scores = cross_val_score(clf, X, y, scoring='accuracy', cv=5)\n   ...     print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n   Accuracy: 0.95 (+/- 0.04) [Logistic Regression]\n   Accuracy: 0.94 (+/- 0.04) [Random Forest]\n   Accuracy: 0.91 (+/- 0.04) [naive Bayes]\n   Accuracy: 0.95 (+/- 0.04) [Ensemble]"
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_32",
    "header": "Weighted Average Probabilities (Soft Voting)",
    "text": "Weighted Average Probabilities (Soft Voting)\n--------------------------------------------\n\nIn contrast to majority voting (hard voting), soft voting\nreturns the class label as argmax of the sum of predicted probabilities.\n\nSpecific weights can be assigned to each classifier via the ``weights``\nparameter. When weights are provided, the predicted class probabilities\nfor each classifier are collected, multiplied by the classifier weight,\nand averaged. The final class label is then derived from the class label\nwith the highest average probability.\n\nTo illustrate this with a simple example, let's assume we have 3\nclassifiers and a 3-class classification problem where we assign\nequal weights to all classifiers: w1=1, w2=1, w3=1.\n\nThe weighted average probabilities for a sample would then be\ncalculated as follows:\n\n================  ==========    ==========      ==========\nclassifier        class 1       class 2         class 3\n================  ==========    ==========      ==========\nclassifier 1      w1 * 0.2      w1 * 0.5        w1 * 0.3\nclassifier 2      w2 * 0.6      w2 * 0.3        w2 * 0.1\nclassifier 3      w3 * 0.3      w3 * 0.4        w3 * 0.3\nweighted average  0.37          0.4             0.23\n================  ==========    ==========      ==========\n\nHere, the predicted class label is 2, since it has the highest average\npredicted probability. See the example on\n:ref:`sphx_glr_auto_examples_ensemble_plot_voting_decision_regions.py` for a\ndemonstration of how the predicted class label can be obtained from the weighted\naverage of predicted probabilities.\n\nThe following figure illustrates how the decision regions may change when\na soft :class:`VotingClassifier` is trained with weights on three linear\nmodels:\n\n.. figure:: ../auto_examples/ensemble/images/sphx_glr_plot_voting_decision_regions_002.png\n    :target: ../auto_examples/ensemble/plot_voting_decision_regions.html\n    :align: center\n    :scale: 75%"
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_33",
    "header": "Usage",
    "text": "Usage\n-----\n\nIn order to predict the class labels based on the predicted\nclass-probabilities (scikit-learn estimators in the VotingClassifier\nmust support ``predict_proba`` method)::\n\n   >>> eclf = VotingClassifier(\n   ...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n   ...     voting='soft'\n   ... )\n\nOptionally, weights can be provided for the individual classifiers::\n\n   >>> eclf = VotingClassifier(\n   ...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n   ...     voting='soft', weights=[2,5,1]\n   ... )\n\n.. dropdown:: Using the :class:`VotingClassifier` with :class:`~sklearn.model_selection.GridSearchCV`\n\n  The :class:`VotingClassifier` can also be used together with\n  :class:`~sklearn.model_selection.GridSearchCV` in order to tune the\n  hyperparameters of the individual estimators::\n\n    >>> from sklearn.model_selection import GridSearchCV\n    >>> clf1 = LogisticRegression(random_state=1)\n    >>> clf2 = RandomForestClassifier(random_state=1)\n    >>> clf3 = GaussianNB()\n    >>> eclf = VotingClassifier(\n    ...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n    ...     voting='soft'\n    ... )\n\n    >>> params = {'lr__C': [1.0, 100.0], 'rf__n_estimators': [20, 200]}\n\n    >>> grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\n    >>> grid = grid.fit(iris.data, iris.target)\n\n.. _voting_regressor:"
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_34",
    "header": "Voting Regressor",
    "text": "Voting Regressor\n================\n\nThe idea behind the :class:`VotingRegressor` is to combine conceptually\ndifferent machine learning regressors and return the average predicted values.\nSuch a regressor can be useful for a set of equally well performing models\nin order to balance out their individual weaknesses."
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_35",
    "header": "Usage",
    "text": "Usage\n-----\n\nThe following example shows how to fit the VotingRegressor::\n\n   >>> from sklearn.datasets import load_diabetes\n   >>> from sklearn.ensemble import GradientBoostingRegressor\n   >>> from sklearn.ensemble import RandomForestRegressor\n   >>> from sklearn.linear_model import LinearRegression\n   >>> from sklearn.ensemble import VotingRegressor\n\n   >>> # Loading some example data\n   >>> X, y = load_diabetes(return_X_y=True)\n\n   >>> # Training classifiers\n   >>> reg1 = GradientBoostingRegressor(random_state=1)\n   >>> reg2 = RandomForestRegressor(random_state=1)\n   >>> reg3 = LinearRegression()\n   >>> ereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('lr', reg3)])\n   >>> ereg = ereg.fit(X, y)\n\n.. figure:: ../auto_examples/ensemble/images/sphx_glr_plot_voting_regressor_001.png\n    :target: ../auto_examples/ensemble/plot_voting_regressor.html\n    :align: center\n    :scale: 75%\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_ensemble_plot_voting_regressor.py`\n\n.. _stacking:"
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_36",
    "header": "Stacked generalization",
    "text": "Stacked generalization\n======================\n\nStacked generalization is a method for combining estimators to reduce their\nbiases [W1992]_ [HTF]_. More precisely, the predictions of each individual\nestimator are stacked together and used as input to a final estimator to\ncompute the prediction. This final estimator is trained through\ncross-validation.\n\nThe :class:`StackingClassifier` and :class:`StackingRegressor` provide such\nstrategies which can be applied to classification and regression problems.\n\nThe `estimators` parameter corresponds to the list of the estimators which\nare stacked together in parallel on the input data. It should be given as a\nlist of names and estimators::\n\n  >>> from sklearn.linear_model import RidgeCV, LassoCV\n  >>> from sklearn.neighbors import KNeighborsRegressor\n  >>> estimators = [('ridge', RidgeCV()),\n  ...               ('lasso', LassoCV(random_state=42)),\n  ...               ('knr', KNeighborsRegressor(n_neighbors=20,\n  ...                                           metric='euclidean'))]\n\nThe `final_estimator` will use the predictions of the `estimators` as input. It\nneeds to be a classifier or a regressor when using :class:`StackingClassifier`\nor :class:`StackingRegressor`, respectively::\n\n  >>> from sklearn.ensemble import GradientBoostingRegressor\n  >>> from sklearn.ensemble import StackingRegressor\n  >>> final_estimator = GradientBoostingRegressor(\n  ...     n_estimators=25, subsample=0.5, min_samples_leaf=25, max_features=1,\n  ...     random_state=42)\n  >>> reg = StackingRegressor(\n  ...     estimators=estimators,\n  ...     final_estimator=final_estimator)\n\nTo train the `estimators` and `final_estimator`, the `fit` method needs\nto be called on the training data::\n\n  >>> from sklearn.datasets import load_diabetes\n  >>> X, y = load_diabetes(return_X_y=True)\n  >>> from sklearn.model_selection import train_test_split\n  >>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n  ...                                                     random_state=42)\n  >>> reg.fit(X_train, y_train)\n  StackingRegressor(...)\n\nDuring training, the `estimators` are fitted on the whole training data\n`X_train`. They will be used when calling `predict` or `predict_proba`. To\ngeneralize and avoid over-fitting, the `final_estimator` is trained on\nout-samples using :func:`sklearn.model_selection.cross_val_predict` internally.\n\nFor :class:`StackingClassifier`, note that the output of the ``estimators`` is\ncontrolled by the parameter `stack_method` and it is called by each estimator.\nThis parameter is either a string, being estimator method names, or `'auto'`\nwhich will automatically identify an available method depending on the\navailability, tested in the order of preference: `predict_proba`,\n`decision_function` and `predict`.\n\nA :class:`StackingRegressor` and :class:`StackingClassifier` can be used as\nany other regressor or classifier, exposing a `predict`, `predict_proba`, or\n`decision_function` method, e.g.::\n\n   >>> y_pred = reg.predict(X_test)\n   >>> from sklearn.metrics import r2_score\n   >>> print('R2 score: {:.2f}'.format(r2_score(y_test, y_pred)))\n   R2 score: 0.53\n\nNote that it is also possible to get the output of the stacked\n`estimators` using the `transform` method::\n\n  >>> reg.transform(X_test[:5])\n  array([[142, 138, 146],\n         [179, 182, 151],\n         [139, 132, 158],\n         [286, 292, 225],\n         [126, 124, 164]])\n\nIn practice, a stacking predictor predicts as good as the best predictor of the\nbase layer and even sometimes outperforms it by combining the different\nstrengths of these predictors. However, training a stacking predictor is\ncomputationally expensive.\n\n.. note::\n   For :class:`StackingClassifier`, when using `stack_method_='predict_proba'`,\n   the first column is dropped when the problem is a binary classification\n   problem. Indeed, both probability columns predicted by each estimator are\n   perfectly collinear.\n\n.. note::\n   Multiple stacking layers can be achieved by assigning `final_estimator` to\n   a :class:`StackingClassifier` or :class:`StackingRegressor`::\n\n    >>> final_layer_rfr = RandomForestRegressor(\n    ...     n_estimators=10, max_features=1, max_leaf_nodes=5,random_state=42)\n    >>> final_layer_gbr = GradientBoostingRegressor(\n    ...     n_estimators=10, max_features=1, max_leaf_nodes=5,random_state=42)\n    >>> final_layer = StackingRegressor(\n    ...     estimators=[('rf', final_layer_rfr),\n    ...                 ('gbrt', final_layer_gbr)],\n    ...     final_estimator=RidgeCV()\n    ...     )\n    >>> multi_layer_regressor = StackingRegressor(\n    ...     estimators=[('ridge', RidgeCV()),\n    ...                 ('lasso', LassoCV(random_state=42)),\n    ...                 ('knr', KNeighborsRegressor(n_neighbors=20,\n    ...                                             metric='euclidean'))],\n    ...     final_estimator=final_layer\n    ... )\n    >>> multi_layer_regressor.fit(X_train, y_train)\n    StackingRegressor(...)\n    >>> print('R2 score: {:.2f}'\n    ...       .format(multi_layer_regressor.score(X_test, y_test)))\n    R2 score: 0.53\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_ensemble_plot_stack_predictors.py`\n\n.. rubric:: References\n\n.. [W1992] Wolpert, David H. \"Stacked generalization.\" Neural networks 5.2\n   (1992): 241-259.\n\n\n\n.. _adaboost:"
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_37",
    "header": "AdaBoost",
    "text": "AdaBoost\n========\n\nThe module :mod:`sklearn.ensemble` includes the popular boosting algorithm\nAdaBoost, introduced in 1995 by Freund and Schapire [FS1995]_.\n\nThe core principle of AdaBoost is to fit a sequence of weak learners (i.e.,\nmodels that are only slightly better than random guessing, such as small\ndecision trees) on repeatedly modified versions of the data. The predictions\nfrom all of them are then combined through a weighted majority vote (or sum) to\nproduce the final prediction. The data modifications at each so-called boosting\niteration consists of applying weights :math:`w_1`, :math:`w_2`, ..., :math:`w_N`\nto each of the training samples. Initially, those weights are all set to\n:math:`w_i = 1/N`, so that the first step simply trains a weak learner on the\noriginal data. For each successive iteration, the sample weights are\nindividually modified and the learning algorithm is reapplied to the reweighted\ndata. At a given step, those training examples that were incorrectly predicted\nby the boosted model induced at the previous step have their weights increased,\nwhereas the weights are decreased for those that were predicted correctly. As\niterations proceed, examples that are difficult to predict receive\never-increasing influence. Each subsequent weak learner is thereby forced to\nconcentrate on the examples that are missed by the previous ones in the sequence\n[HTF]_.\n\n.. figure:: ../auto_examples/ensemble/images/sphx_glr_plot_adaboost_multiclass_001.png\n   :target: ../auto_examples/ensemble/plot_adaboost_multiclass.html\n   :align: center\n   :scale: 75\n\nAdaBoost can be used both for classification and regression problems:\n\n- For multi-class classification, :class:`AdaBoostClassifier` implements\n  AdaBoost.SAMME [ZZRH2009]_.\n\n- For regression, :class:`AdaBoostRegressor` implements AdaBoost.R2 [D1997]_."
  },
  {
    "filename": "ensemble.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\ensemble.rst.txt",
    "id": "ensemble.rst.txt_chunk_38",
    "header": "Usage",
    "text": "Usage\n-----\n\nThe following example shows how to fit an AdaBoost classifier with 100 weak\nlearners::\n\n    >>> from sklearn.model_selection import cross_val_score\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn.ensemble import AdaBoostClassifier\n\n    >>> X, y = load_iris(return_X_y=True)\n    >>> clf = AdaBoostClassifier(n_estimators=100)\n    >>> scores = cross_val_score(clf, X, y, cv=5)\n    >>> scores.mean()\n    np.float64(0.95)\n\nThe number of weak learners is controlled by the parameter ``n_estimators``. The\n``learning_rate`` parameter controls the contribution of the weak learners in\nthe final combination. By default, weak learners are decision stumps. Different\nweak learners can be specified through the ``estimator`` parameter.\nThe main parameters to tune to obtain good results are ``n_estimators`` and\nthe complexity of the base estimators (e.g., its depth ``max_depth`` or\nminimum required number of samples to consider a split ``min_samples_split``).\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_ensemble_plot_adaboost_multiclass.py` shows the performance\n  of AdaBoost on a multi-class problem.\n\n* :ref:`sphx_glr_auto_examples_ensemble_plot_adaboost_twoclass.py` shows the decision boundary\n  and decision function values for a non-linearly separable two-class problem\n  using AdaBoost-SAMME.\n\n* :ref:`sphx_glr_auto_examples_ensemble_plot_adaboost_regression.py` demonstrates regression\n  with the AdaBoost.R2 algorithm.\n\n.. rubric:: References\n\n.. [FS1995] Y. Freund, and R. Schapire, \"A Decision-Theoretic Generalization of\n   On-Line Learning and an Application to Boosting\", 1997.\n\n.. [ZZRH2009] J. Zhu, H. Zou, S. Rosset, T. Hastie. \"Multi-class AdaBoost\", 2009.\n\n.. [D1997] H. Drucker. \"Improving Regressors using Boosting Techniques\", 1997.\n\n.. [HTF] T. Hastie, R. Tibshirani and J. Friedman, \"Elements of Statistical Learning\n   Ed. 2\", Springer, 2009."
  },
  {
    "filename": "feature_extraction.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\feature_extraction.rst.txt",
    "id": "feature_extraction.rst.txt_chunk_0",
    "header": ".. _feature_extraction:",
    "text": ".. _feature_extraction:\n\n=================="
  },
  {
    "filename": "feature_extraction.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\feature_extraction.rst.txt",
    "id": "feature_extraction.rst.txt_chunk_1",
    "header": "Feature extraction",
    "text": "Feature extraction\n==================\n\n.. currentmodule:: sklearn.feature_extraction\n\nThe :mod:`sklearn.feature_extraction` module can be used to extract\nfeatures in a format supported by machine learning algorithms from datasets\nconsisting of formats such as text and image.\n\n.. note::\n\n   Feature extraction is very different from :ref:`feature_selection`:\n   the former consists of transforming arbitrary data, such as text or\n   images, into numerical features usable for machine learning. The latter\n   is a machine learning technique applied to these features.\n\n.. _dict_feature_extraction:"
  },
  {
    "filename": "feature_extraction.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\feature_extraction.rst.txt",
    "id": "feature_extraction.rst.txt_chunk_2",
    "header": "Loading features from dicts",
    "text": "Loading features from dicts\n===========================\n\nThe class :class:`DictVectorizer` can be used to convert feature\narrays represented as lists of standard Python ``dict`` objects to the\nNumPy/SciPy representation used by scikit-learn estimators.\n\nWhile not particularly fast to process, Python's ``dict`` has the\nadvantages of being convenient to use, being sparse (absent features\nneed not be stored) and storing feature names in addition to values.\n\n:class:`DictVectorizer` implements what is called one-of-K or \"one-hot\"\ncoding for categorical (aka nominal, discrete) features. Categorical\nfeatures are \"attribute-value\" pairs where the value is restricted\nto a list of discrete possibilities without ordering (e.g. topic\nidentifiers, types of objects, tags, names...).\n\nIn the following, \"city\" is a categorical attribute while \"temperature\"\nis a traditional numerical feature::\n\n  >>> measurements = [\n  ...     {'city': 'Dubai', 'temperature': 33.},\n  ...     {'city': 'London', 'temperature': 12.},\n  ...     {'city': 'San Francisco', 'temperature': 18.},\n  ... ]\n\n  >>> from sklearn.feature_extraction import DictVectorizer\n  >>> vec = DictVectorizer()\n\n  >>> vec.fit_transform(measurements).toarray()\n  array([[ 1.,  0.,  0., 33.],\n         [ 0.,  1.,  0., 12.],\n         [ 0.,  0.,  1., 18.]])\n\n  >>> vec.get_feature_names_out()\n  array(['city=Dubai', 'city=London', 'city=San Francisco', 'temperature'], ...)\n\n:class:`DictVectorizer` accepts multiple string values for one\nfeature, like, e.g., multiple categories for a movie.\n\nAssume a database classifies each movie using some categories (not mandatory)\nand its year of release.\n\n    >>> movie_entry = [{'category': ['thriller', 'drama'], 'year': 2003},\n    ...                {'category': ['animation', 'family'], 'year': 2011},\n    ...                {'year': 1974}]\n    >>> vec.fit_transform(movie_entry).toarray()\n    array([[0.000e+00, 1.000e+00, 0.000e+00, 1.000e+00, 2.003e+03],\n           [1.000e+00, 0.000e+00, 1.000e+00, 0.000e+00, 2.011e+03],\n           [0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 1.974e+03]])\n    >>> vec.get_feature_names_out()\n    array(['category=animation', 'category=drama', 'category=family',\n           'category=thriller', 'year'], ...)\n    >>> vec.transform({'category': ['thriller'],\n    ...                'unseen_feature': '3'}).toarray()\n    array([[0., 0., 0., 1., 0.]])\n\n:class:`DictVectorizer` is also a useful representation transformation\nfor training sequence classifiers in Natural Language Processing models\nthat typically work by extracting feature windows around a particular\nword of interest.\n\nFor example, suppose that we have a first algorithm that extracts Part of\nSpeech (PoS) tags that we want to use as complementary tags for training\na sequence classifier (e.g. a chunker). The following dict could be\nsuch a window of features extracted around the word 'sat' in the sentence\n'The cat sat on the mat.'::\n\n  >>> pos_window = [\n  ...     {\n  ...         'word-2': 'the',\n  ...         'pos-2': 'DT',\n  ...         'word-1': 'cat',\n  ...         'pos-1': 'NN',\n  ...         'word+1': 'on',\n  ...         'pos+1': 'PP',\n  ...     },\n  ...     # in a real application one would extract many such dictionaries\n  ... ]\n\nThis description can be vectorized into a sparse two-dimensional matrix\nsuitable for feeding into a classifier (maybe after being piped into a\n:class:`~text.TfidfTransformer` for normalization)::\n\n  >>> vec = DictVectorizer()\n  >>> pos_vectorized = vec.fit_transform(pos_window)\n  >>> pos_vectorized\n  <Compressed Sparse...dtype 'float64'\n    with 6 stored elements and shape (1, 6)>\n  >>> pos_vectorized.toarray()\n  array([[1., 1., 1., 1., 1., 1.]])\n  >>> vec.get_feature_names_out()\n  array(['pos+1=PP', 'pos-1=NN', 'pos-2=DT', 'word+1=on', 'word-1=cat',\n         'word-2=the'], ...)\n\nAs you can imagine, if one extracts such a context around each individual\nword of a corpus of documents the resulting matrix will be very wide\n(many one-hot-features) with most of them being valued to zero most\nof the time. So as to make the resulting data structure able to fit in\nmemory the ``DictVectorizer`` class uses a ``scipy.sparse`` matrix by\ndefault instead of a ``numpy.ndarray``.\n\n\n.. _feature_hashing:"
  },
  {
    "filename": "feature_extraction.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\feature_extraction.rst.txt",
    "id": "feature_extraction.rst.txt_chunk_3",
    "header": "Feature hashing",
    "text": "Feature hashing\n===============\n\n.. currentmodule:: sklearn.feature_extraction\n\nThe class :class:`FeatureHasher` is a high-speed, low-memory vectorizer that\nuses a technique known as\n`feature hashing <https://en.wikipedia.org/wiki/Feature_hashing>`_,\nor the \"hashing trick\".\nInstead of building a hash table of the features encountered in training,\nas the vectorizers do, instances of :class:`FeatureHasher`\napply a hash function to the features\nto determine their column index in sample matrices directly.\nThe result is increased speed and reduced memory usage,\nat the expense of inspectability;\nthe hasher does not remember what the input features looked like\nand has no ``inverse_transform`` method.\n\nSince the hash function might cause collisions between (unrelated) features,\na signed hash function is used and the sign of the hash value\ndetermines the sign of the value stored in the output matrix for a feature.\nThis way, collisions are likely to cancel out rather than accumulate error,\nand the expected mean of any output feature's value is zero. This mechanism\nis enabled by default with ``alternate_sign=True`` and is particularly useful\nfor small hash table sizes (``n_features < 10000``). For large hash table\nsizes, it can be disabled, to allow the output to be passed to estimators like\n:class:`~sklearn.naive_bayes.MultinomialNB` or\n:class:`~sklearn.feature_selection.chi2`\nfeature selectors that expect non-negative inputs.\n\n:class:`FeatureHasher` accepts either mappings\n(like Python's ``dict`` and its variants in the ``collections`` module),\n``(feature, value)`` pairs, or strings,\ndepending on the constructor parameter ``input_type``.\nMappings are treated as lists of ``(feature, value)`` pairs,\nwhile single strings have an implicit value of 1,\nso ``['feat1', 'feat2', 'feat3']`` is interpreted as\n``[('feat1', 1), ('feat2', 1), ('feat3', 1)]``.\nIf a single feature occurs multiple times in a sample,\nthe associated values will be summed\n(so ``('feat', 2)`` and ``('feat', 3.5)`` become ``('feat', 5.5)``).\nThe output from :class:`FeatureHasher` is always a ``scipy.sparse`` matrix\nin the CSR format.\n\nFeature hashing can be employed in document classification,\nbut unlike :class:`~text.CountVectorizer`,\n:class:`FeatureHasher` does not do word\nsplitting or any other preprocessing except Unicode-to-UTF-8 encoding;\nsee :ref:`hashing_vectorizer`, below, for a combined tokenizer/hasher.\n\nAs an example, consider a word-level natural language processing task\nthat needs features extracted from ``(token, part_of_speech)`` pairs.\nOne could use a Python generator function to extract features::\n\n  def token_features(token, part_of_speech):\n      if token.isdigit():\n          yield \"numeric\"\n      else:\n          yield \"token={}\".format(token.lower())\n          yield \"token,pos={},{}\".format(token, part_of_speech)\n      if token[0].isupper():\n          yield \"uppercase_initial\"\n      if token.isupper():\n          yield \"all_uppercase\"\n      yield \"pos={}\".format(part_of_speech)\n\nThen, the ``raw_X`` to be fed to ``FeatureHasher.transform``\ncan be constructed using::\n\n  raw_X = (token_features(tok, pos_tagger(tok)) for tok in corpus)\n\nand fed to a hasher with::\n\n  hasher = FeatureHasher(input_type='string')\n  X = hasher.transform(raw_X)\n\nto get a ``scipy.sparse`` matrix ``X``.\n\nNote the use of a generator comprehension,\nwhich introduces laziness into the feature extraction:\ntokens are only processed on demand from the hasher.\n\n.. dropdown:: Implementation details\n\n  :class:`FeatureHasher` uses the signed 32-bit variant of MurmurHash3.\n  As a result (and because of limitations in ``scipy.sparse``),\n  the maximum number of features supported is currently :math:`2^{31} - 1`.\n\n  The original formulation of the hashing trick by Weinberger et al.\n  used two separate hash functions :math:`h` and :math:`\\xi`\n  to determine the column index and sign of a feature, respectively.\n  The present implementation works under the assumption\n  that the sign bit of MurmurHash3 is independent of its other bits.\n\n  Since a simple modulo is used to transform the hash function to a column index,\n  it is advisable to use a power of two as the ``n_features`` parameter;\n  otherwise the features will not be mapped evenly to the columns.\n\n  .. rubric:: References\n\n  * `MurmurHash3 <https://github.com/aappleby/smhasher>`_.\n\n\n.. rubric:: References\n\n* Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola and\n  Josh Attenberg (2009). `Feature hashing for large scale multitask learning\n  <https://alex.smola.org/papers/2009/Weinbergeretal09.pdf>`_. Proc. ICML.\n\n.. _text_feature_extraction:"
  },
  {
    "filename": "feature_extraction.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\feature_extraction.rst.txt",
    "id": "feature_extraction.rst.txt_chunk_4",
    "header": "Text feature extraction",
    "text": "Text feature extraction\n=======================\n\n.. currentmodule:: sklearn.feature_extraction.text"
  },
  {
    "filename": "feature_extraction.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\feature_extraction.rst.txt",
    "id": "feature_extraction.rst.txt_chunk_5",
    "header": "The Bag of Words representation",
    "text": "The Bag of Words representation\n-------------------------------\n\nText Analysis is a major application field for machine learning\nalgorithms. However the raw data, a sequence of symbols, cannot be fed\ndirectly to the algorithms themselves as most of them expect numerical\nfeature vectors with a fixed size rather than the raw text documents\nwith variable length.\n\nIn order to address this, scikit-learn provides utilities for the most\ncommon ways to extract numerical features from text content, namely:\n\n- **tokenizing** strings and giving an integer id for each possible token,\n  for instance by using white-spaces and punctuation as token separators.\n\n- **counting** the occurrences of tokens in each document.\n\n- **normalizing** and weighting with diminishing importance tokens that\n  occur in the majority of samples / documents.\n\nIn this scheme, features and samples are defined as follows:\n\n- each **individual token occurrence frequency** (normalized or not)\n  is treated as a **feature**.\n\n- the vector of all the token frequencies for a given **document** is\n  considered a multivariate **sample**.\n\nA corpus of documents can thus be represented by a matrix with one row\nper document and one column per token (e.g. word) occurring in the corpus.\n\nWe call **vectorization** the general process of turning a collection\nof text documents into numerical feature vectors. This specific strategy\n(tokenization, counting and normalization) is called the **Bag of Words**\nor \"Bag of n-grams\" representation. Documents are described by word\noccurrences while completely ignoring the relative position information\nof the words in the document."
  },
  {
    "filename": "feature_extraction.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\feature_extraction.rst.txt",
    "id": "feature_extraction.rst.txt_chunk_6",
    "header": "Sparsity",
    "text": "Sparsity\n--------\n\nAs most documents will typically use a very small subset of the words used in\nthe corpus, the resulting matrix will have many feature values that are\nzeros (typically more than 99% of them).\n\nFor instance a collection of 10,000 short text documents (such as emails)\nwill use a vocabulary with a size in the order of 100,000 unique words in\ntotal while each document will use 100 to 1000 unique words individually.\n\nIn order to be able to store such a matrix in memory but also to speed\nup algebraic operations matrix / vector, implementations will typically\nuse a sparse representation such as the implementations available in the\n``scipy.sparse`` package."
  },
  {
    "filename": "feature_extraction.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\feature_extraction.rst.txt",
    "id": "feature_extraction.rst.txt_chunk_7",
    "header": "Common Vectorizer usage",
    "text": "Common Vectorizer usage\n-----------------------\n\n:class:`CountVectorizer` implements both tokenization and occurrence\ncounting in a single class::\n\n  >>> from sklearn.feature_extraction.text import CountVectorizer\n\nThis model has many parameters, however the default values are quite\nreasonable (please see  the :ref:`reference documentation\n<feature_extraction_ref-from-text>` for the details)::\n\n  >>> vectorizer = CountVectorizer()\n  >>> vectorizer\n  CountVectorizer()\n\nLet's use it to tokenize and count the word occurrences of a minimalistic\ncorpus of text documents::\n\n  >>> corpus = [\n  ...     'This is the first document.',\n  ...     'This is the second second document.',\n  ...     'And the third one.',\n  ...     'Is this the first document?',\n  ... ]\n  >>> X = vectorizer.fit_transform(corpus)\n  >>> X\n  <Compressed Sparse...dtype 'int64'\n    with 19 stored elements and shape (4, 9)>\n\nThe default configuration tokenizes the string by extracting words of\nat least 2 letters. The specific function that does this step can be\nrequested explicitly::\n\n  >>> analyze = vectorizer.build_analyzer()\n  >>> analyze(\"This is a text document to analyze.\") == (\n  ...     ['this', 'is', 'text', 'document', 'to', 'analyze'])\n  True\n\nEach term found by the analyzer during the fit is assigned a unique\ninteger index corresponding to a column in the resulting matrix. This\ninterpretation of the columns can be retrieved as follows::\n\n  >>> vectorizer.get_feature_names_out()\n  array(['and', 'document', 'first', 'is', 'one', 'second', 'the',\n         'third', 'this'], ...)\n\n  >>> X.toarray()\n  array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n         [0, 1, 0, 1, 0, 2, 1, 0, 1],\n         [1, 0, 0, 0, 1, 0, 1, 1, 0],\n         [0, 1, 1, 1, 0, 0, 1, 0, 1]]...)\n\nThe converse mapping from feature name to column index is stored in the\n``vocabulary_`` attribute of the vectorizer::\n\n  >>> vectorizer.vocabulary_.get('document')\n  1\n\nHence words that were not seen in the training corpus will be completely\nignored in future calls to the transform method::\n\n  >>> vectorizer.transform(['Something completely new.']).toarray()\n  array([[0, 0, 0, 0, 0, 0, 0, 0, 0]]...)\n\nNote that in the previous corpus, the first and the last documents have\nexactly the same words hence are encoded in equal vectors. In particular\nwe lose the information that the last document is an interrogative form. To\npreserve some of the local ordering information we can extract 2-grams\nof words in addition to the 1-grams (individual words)::\n\n  >>> bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),\n  ...                                     token_pattern=r'\\b\\w+\\b', min_df=1)\n  >>> analyze = bigram_vectorizer.build_analyzer()\n  >>> analyze('Bi-grams are cool!') == (\n  ...     ['bi', 'grams', 'are', 'cool', 'bi grams', 'grams are', 'are cool'])\n  True\n\nThe vocabulary extracted by this vectorizer is hence much bigger and\ncan now resolve ambiguities encoded in local positioning patterns::\n\n  >>> X_2 = bigram_vectorizer.fit_transform(corpus).toarray()\n  >>> X_2\n  array([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0],\n         [0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0],\n         [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0],\n         [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]]...)\n\n\nIn particular the interrogative form \"Is this\" is only present in the\nlast document::\n\n  >>> feature_index = bigram_vectorizer.vocabulary_.get('is this')\n  >>> X_2[:, feature_index]\n  array([0, 0, 0, 1]...)\n\n.. _stop_words:"
  },
  {
    "filename": "feature_extraction.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\feature_extraction.rst.txt",
    "id": "feature_extraction.rst.txt_chunk_8",
    "header": "Using stop words",
    "text": "Using stop words\n----------------\n\nStop words are words like \"and\", \"the\", \"him\", which are presumed to be\nuninformative in representing the content of a text, and which may be\nremoved to avoid them being construed as informative for prediction. Sometimes,\nhowever, similar words are useful for prediction, such as in classifying\nwriting style or personality.\n\nThere are several known issues in our provided 'english' stop word list. It\ndoes not aim to be a general, 'one-size-fits-all' solution as some tasks\nmay require a more custom solution. See [NQY18]_ for more details.\n\nPlease take care in choosing a stop word list.\nPopular stop word lists may include words that are highly informative to\nsome tasks, such as *computer*.\n\nYou should also make sure that the stop word list has had the same\npreprocessing and tokenization applied as the one used in the vectorizer.\nThe word *we've* is split into *we* and *ve* by CountVectorizer's default\ntokenizer, so if *we've* is in ``stop_words``, but *ve* is not, *ve* will\nbe retained from *we've* in transformed text.  Our vectorizers will try to\nidentify and warn about some kinds of inconsistencies.\n\n.. rubric:: References\n\n.. [NQY18] J. Nothman, H. Qin and R. Yurchak (2018).\n   `\"Stop Word Lists in Free Open-source Software Packages\"\n   <https://aclweb.org/anthology/W18-2502>`__.\n   In *Proc. Workshop for NLP Open Source Software*.\n\n\n.. _tfidf:"
  },
  {
    "filename": "feature_extraction.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\feature_extraction.rst.txt",
    "id": "feature_extraction.rst.txt_chunk_9",
    "header": "Tf\u2013idf term weighting",
    "text": "Tf\u2013idf term weighting\n---------------------\n\nIn a large text corpus, some words will be very present (e.g. \"the\", \"a\",\n\"is\" in English) hence carrying very little meaningful information about\nthe actual contents of the document. If we were to feed the direct count\ndata directly to a classifier those very frequent terms would shadow\nthe frequencies of rarer yet more interesting terms.\n\nIn order to re-weight the count features into floating point values\nsuitable for usage by a classifier it is very common to use the tf\u2013idf\ntransform.\n\nTf means **term-frequency** while tf\u2013idf means term-frequency times\n**inverse document-frequency**:\n:math:`\\text{tf-idf(t,d)}=\\text{tf(t,d)} \\times \\text{idf(t)}`.\n\nUsing the ``TfidfTransformer``'s default settings,\n``TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)``\nthe term frequency, the number of times a term occurs in a given document,\nis multiplied with idf component, which is computed as\n\n:math:`\\text{idf}(t) = \\log{\\frac{1 + n}{1+\\text{df}(t)}} + 1`,\n\nwhere :math:`n` is the total number of documents in the document set, and\n:math:`\\text{df}(t)` is the number of documents in the document set that\ncontain term :math:`t`. The resulting tf-idf vectors are then normalized by the\nEuclidean norm:\n\n:math:`v_{norm} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v{_1}^2 +\nv{_2}^2 + \\dots + v{_n}^2}}`.\n\nThis was originally a term weighting scheme developed for information retrieval\n(as a ranking function for search engines results) that has also found good\nuse in document classification and clustering.\n\nThe following sections contain further explanations and examples that\nillustrate how the tf-idfs are computed exactly and how the tf-idfs\ncomputed in scikit-learn's :class:`TfidfTransformer`\nand :class:`TfidfVectorizer` differ slightly from the standard textbook\nnotation that defines the idf as\n\n:math:`\\text{idf}(t) = \\log{\\frac{n}{1+\\text{df}(t)}}.`\n\n\nIn the :class:`TfidfTransformer` and :class:`TfidfVectorizer`\nwith ``smooth_idf=False``, the\n\"1\" count is added to the idf instead of the idf's denominator:\n\n:math:`\\text{idf}(t) = \\log{\\frac{n}{\\text{df}(t)}} + 1`\n\nThis normalization is implemented by the :class:`TfidfTransformer`\nclass::\n\n  >>> from sklearn.feature_extraction.text import TfidfTransformer\n  >>> transformer = TfidfTransformer(smooth_idf=False)\n  >>> transformer\n  TfidfTransformer(smooth_idf=False)\n\nAgain please see the :ref:`reference documentation\n<feature_extraction_ref-from-text>` for the details on all the parameters.\n\n.. dropdown:: Numeric example of a tf-idf matrix\n\n  Let's take an example with the following counts. The first term is present\n  100% of the time hence not very interesting. The two other features only\n  in less than 50% of the time hence probably more representative of the\n  content of the documents::\n\n    >>> counts = [[3, 0, 1],\n    ...           [2, 0, 0],\n    ...           [3, 0, 0],\n    ...           [4, 0, 0],\n    ...           [3, 2, 0],\n    ...           [3, 0, 2]]\n    ...\n    >>> tfidf = transformer.fit_transform(counts)\n    >>> tfidf\n    <Compressed Sparse...dtype 'float64'\n      with 9 stored elements and shape (6, 3)>\n\n    >>> tfidf.toarray()\n    array([[0.81940995, 0.        , 0.57320793],\n          [1.        , 0.        , 0.        ],\n          [1.        , 0.        , 0.        ],\n          [1.        , 0.        , 0.        ],\n          [0.47330339, 0.88089948, 0.        ],\n          [0.58149261, 0.        , 0.81355169]])\n\n  Each row is normalized to have unit Euclidean norm:\n\n  :math:`v_{norm} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v{_1}^2 +\n  v{_2}^2 + \\dots + v{_n}^2}}`\n\n  For example, we can compute the tf-idf of the first term in the first\n  document in the `counts` array as follows:\n\n  :math:`n = 6`\n\n  :math:`\\text{df}(t)_{\\text{term1}} = 6`\n\n  :math:`\\text{idf}(t)_{\\text{term1}} =\n  \\log \\frac{n}{\\text{df}(t)} + 1 = \\log(1)+1 = 1`\n\n  :math:`\\text{tf-idf}_{\\text{term1}} = \\text{tf} \\times \\text{idf} = 3 \\times 1 = 3`\n\n  Now, if we repeat this computation for the remaining 2 terms in the document,\n  we get\n\n  :math:`\\text{tf-idf}_{\\text{term2}} = 0 \\times (\\log(6/1)+1) = 0`\n\n  :math:`\\text{tf-idf}_{\\text{term3}} = 1 \\times (\\log(6/2)+1) \\approx 2.0986`\n\n  and the vector of raw tf-idfs:\n\n  :math:`\\text{tf-idf}_{\\text{raw}} = [3, 0, 2.0986].`\n\n\n  Then, applying the Euclidean (L2) norm, we obtain the following tf-idfs\n  for document 1:\n\n  :math:`\\frac{[3, 0, 2.0986]}{\\sqrt{\\big(3^2 + 0^2 + 2.0986^2\\big)}}\n  = [ 0.819,  0,  0.573].`\n\n  Furthermore, the default parameter ``smooth_idf=True`` adds \"1\" to the numerator\n  and  denominator as if an extra document was seen containing every term in the\n  collection exactly once, which prevents zero divisions:\n\n  :math:`\\text{idf}(t) = \\log{\\frac{1 + n}{1+\\text{df}(t)}} + 1`\n\n  Using this modification, the tf-idf of the third term in document 1 changes to\n  1.8473:\n\n  :math:`\\text{tf-idf}_{\\text{term3}} = 1 \\times \\log(7/3)+1 \\approx 1.8473`\n\n  And the L2-normalized tf-idf changes to\n\n  :math:`\\frac{[3, 0, 1.8473]}{\\sqrt{\\big(3^2 + 0^2 + 1.8473^2\\big)}}\n  = [0.8515, 0, 0.5243]`::\n\n    >>> transformer = TfidfTransformer()\n    >>> transformer.fit_transform(counts).toarray()\n    array([[0.85151335, 0.        , 0.52433293],\n          [1.        , 0.        , 0.        ],\n          [1.        , 0.        , 0.        ],\n          [1.        , 0.        , 0.        ],\n          [0.55422893, 0.83236428, 0.        ],\n          [0.63035731, 0.        , 0.77630514]])\n\n  The weights of each\n  feature computed by the ``fit`` method call are stored in a model\n  attribute::\n\n    >>> transformer.idf_\n    array([1., 2.25, 1.84])\n\n  As tf-idf is very often used for text features, there is also another\n  class called :class:`TfidfVectorizer` that combines all the options of\n  :class:`CountVectorizer` and :class:`TfidfTransformer` in a single model::\n\n    >>> from sklearn.feature_extraction.text import TfidfVectorizer\n    >>> vectorizer = TfidfVectorizer()\n    >>> vectorizer.fit_transform(corpus)\n    <Compressed Sparse...dtype 'float64'\n      with 19 stored elements and shape (4, 9)>\n\n  While the tf-idf normalization is often very useful, there might\n  be cases where the binary occurrence markers might offer better\n  features. This can be achieved by using the ``binary`` parameter\n  of :class:`CountVectorizer`. In particular, some estimators such as\n  :ref:`bernoulli_naive_bayes` explicitly model discrete boolean random\n  variables. Also, very short texts are likely to have noisy tf-idf values\n  while the binary occurrence info is more stable.\n\n  As usual the best way to adjust the feature extraction parameters\n  is to use a cross-validated grid search, for instance by pipelining the\n  feature extractor with a classifier:\n\n  * :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_text_feature_extraction.py`"
  },
  {
    "filename": "feature_extraction.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\feature_extraction.rst.txt",
    "id": "feature_extraction.rst.txt_chunk_10",
    "header": "Decoding text files",
    "text": "Decoding text files\n-------------------\nText is made of characters, but files are made of bytes. These bytes represent\ncharacters according to some *encoding*. To work with text files in Python,\ntheir bytes must be *decoded* to a character set called Unicode.\nCommon encodings are ASCII, Latin-1 (Western Europe), KOI8-R (Russian)\nand the universal encodings UTF-8 and UTF-16. Many others exist.\n\n.. note::\n    An encoding can also be called a 'character set',\n    but this term is less accurate: several encodings can exist\n    for a single character set.\n\nThe text feature extractors in scikit-learn know how to decode text files,\nbut only if you tell them what encoding the files are in.\nThe :class:`CountVectorizer` takes an ``encoding`` parameter for this purpose.\nFor modern text files, the correct encoding is probably UTF-8,\nwhich is therefore the default (``encoding=\"utf-8\"``).\n\nIf the text you are loading is not actually encoded with UTF-8, however,\nyou will get a ``UnicodeDecodeError``.\nThe vectorizers can be told to be silent about decoding errors\nby setting the ``decode_error`` parameter to either ``\"ignore\"``\nor ``\"replace\"``. See the documentation for the Python function\n``bytes.decode`` for more details\n(type ``help(bytes.decode)`` at the Python prompt).\n\n.. dropdown:: Troubleshooting decoding text\n\n  If you are having trouble decoding text, here are some things to try:\n\n  - Find out what the actual encoding of the text is. The file might come\n    with a header or README that tells you the encoding, or there might be some\n    standard encoding you can assume based on where the text comes from.\n\n  - You may be able to find out what kind of encoding it is in general\n    using the UNIX command ``file``. The Python ``chardet`` module comes with\n    a script called ``chardetect.py`` that will guess the specific encoding,\n    though you cannot rely on its guess being correct.\n\n  - You could try UTF-8 and disregard the errors. You can decode byte\n    strings with ``bytes.decode(errors='replace')`` to replace all\n    decoding errors with a meaningless character, or set\n    ``decode_error='replace'`` in the vectorizer. This may damage the\n    usefulness of your features.\n\n  - Real text may come from a variety of sources that may have used different\n    encodings, or even be sloppily decoded in a different encoding than the\n    one it was encoded with. This is common in text retrieved from the Web.\n    The Python package `ftfy <https://github.com/LuminosoInsight/python-ftfy>`__\n    can automatically sort out some classes of\n    decoding errors, so you could try decoding the unknown text as ``latin-1``\n    and then using ``ftfy`` to fix errors.\n\n  - If the text is in a mish-mash of encodings that is simply too hard to sort\n    out (which is the case for the 20 Newsgroups dataset), you can fall back on\n    a simple single-byte encoding such as ``latin-1``. Some text may display\n    incorrectly, but at least the same sequence of bytes will always represent\n    the same feature.\n\n  For example, the following snippet uses ``chardet``\n  (not shipped with scikit-learn, must be installed separately)\n  to figure out the encoding of three texts.\n  It then vectorizes the texts and prints the learned vocabulary.\n  The output is not shown here.\n\n    >>> import chardet    # doctest: +SKIP\n    >>> text1 = b\"Sei mir gegr\\xc3\\xbc\\xc3\\x9ft mein Sauerkraut\"\n    >>> text2 = b\"holdselig sind deine Ger\\xfcche\"\n    >>> text3 = b\"\\xff\\xfeA\\x00u\\x00f\\x00 \\x00F\\x00l\\x00\\xfc\\x00g\\x00e\\x00l\\x00n\\x00 \\x00d\\x00e\\x00s\\x00 \\x00G\\x00e\\x00s\\x00a\\x00n\\x00g\\x00e\\x00s\\x00,\\x00 \\x00H\\x00e\\x00r\\x00z\\x00l\\x00i\\x00e\\x00b\\x00c\\x00h\\x00e\\x00n\\x00,\\x00 \\x00t\\x00r\\x00a\\x00g\\x00 \\x00i\\x00c\\x00h\\x00 \\x00d\\x00i\\x00c\\x00h\\x00 \\x00f\\x00o\\x00r\\x00t\\x00\"\n    >>> decoded = [x.decode(chardet.detect(x)['encoding'])\n    ...            for x in (text1, text2, text3)]        # doctest: +SKIP\n    >>> v = CountVectorizer().fit(decoded).vocabulary_    # doctest: +SKIP\n    >>> for term in v: print(v)                           # doctest: +SKIP\n\n  (Depending on the version of ``chardet``, it might get the first one wrong.)\n\n  For an introduction to Unicode and character encodings in general,\n  see Joel Spolsky's `Absolute Minimum Every Software Developer Must Know\n  About Unicode <https://www.joelonsoftware.com/articles/Unicode.html>`_."
  },
  {
    "filename": "feature_extraction.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\feature_extraction.rst.txt",
    "id": "feature_extraction.rst.txt_chunk_11",
    "header": "Applications and examples",
    "text": "Applications and examples\n-------------------------\n\nThe bag of words representation is quite simplistic but surprisingly\nuseful in practice.\n\nIn particular in a **supervised setting** it can be successfully combined\nwith fast and scalable linear models to train **document classifiers**,\nfor instance:\n\n* :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n\nIn an **unsupervised setting** it can be used to group similar documents\ntogether by applying clustering algorithms such as :ref:`k_means`:\n\n* :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`\n\nFinally it is possible to discover the main topics of a corpus by\nrelaxing the hard assignment constraint of clustering, for instance by\nusing :ref:`NMF`:\n\n* :ref:`sphx_glr_auto_examples_applications_plot_topics_extraction_with_nmf_lda.py`"
  },
  {
    "filename": "feature_extraction.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\feature_extraction.rst.txt",
    "id": "feature_extraction.rst.txt_chunk_12",
    "header": "Limitations of the Bag of Words representation",
    "text": "Limitations of the Bag of Words representation\n----------------------------------------------\n\nA collection of unigrams (what bag of words is) cannot capture phrases\nand multi-word expressions, effectively disregarding any word order\ndependence. Additionally, the bag of words model doesn't account for potential\nmisspellings or word derivations.\n\nN-grams to the rescue! Instead of building a simple collection of\nunigrams (n=1), one might prefer a collection of bigrams (n=2), where\noccurrences of pairs of consecutive words are counted.\n\nOne might alternatively consider a collection of character n-grams, a\nrepresentation resilient against misspellings and derivations.\n\nFor example, let's say we're dealing with a corpus of two documents:\n``['words', 'wprds']``. The second document contains a misspelling\nof the word 'words'.\nA simple bag of words representation would consider these two as\nvery distinct documents, differing in both of the two possible features.\nA character 2-gram representation, however, would find the documents\nmatching in 4 out of 8 features, which may help the preferred classifier\ndecide better::\n\n  >>> ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(2, 2))\n  >>> counts = ngram_vectorizer.fit_transform(['words', 'wprds'])\n  >>> ngram_vectorizer.get_feature_names_out()\n  array([' w', 'ds', 'or', 'pr', 'rd', 's ', 'wo', 'wp'], ...)\n  >>> counts.toarray().astype(int)\n  array([[1, 1, 1, 0, 1, 1, 1, 0],\n         [1, 1, 0, 1, 1, 1, 0, 1]])\n\nIn the above example, ``char_wb`` analyzer is used, which creates n-grams\nonly from characters inside word boundaries (padded with space on each\nside). The ``char`` analyzer, alternatively, creates n-grams that\nspan across words::\n\n  >>> ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(5, 5))\n  >>> ngram_vectorizer.fit_transform(['jumpy fox'])\n  <Compressed Sparse...dtype 'int64'\n    with 4 stored elements and shape (1, 4)>\n\n  >>> ngram_vectorizer.get_feature_names_out()\n  array([' fox ', ' jump', 'jumpy', 'umpy '], ...)\n\n  >>> ngram_vectorizer = CountVectorizer(analyzer='char', ngram_range=(5, 5))\n  >>> ngram_vectorizer.fit_transform(['jumpy fox'])\n  <Compressed Sparse...dtype 'int64'\n    with 5 stored elements and shape (1, 5)>\n  >>> ngram_vectorizer.get_feature_names_out()\n  array(['jumpy', 'mpy f', 'py fo', 'umpy ', 'y fox'], ...)\n\nThe word boundaries-aware variant ``char_wb`` is especially interesting\nfor languages that use white-spaces for word separation as it generates\nsignificantly less noisy features than the raw ``char`` variant in\nthat case. For such languages it can increase both the predictive\naccuracy and convergence speed of classifiers trained using such\nfeatures while retaining the robustness with regards to misspellings and\nword derivations.\n\nWhile some local positioning information can be preserved by extracting\nn-grams instead of individual words, bag of words and bag of n-grams\ndestroy most of the inner structure of the document and hence most of\nthe meaning carried by that internal structure.\n\nIn order to address the wider task of Natural Language Understanding,\nthe local structure of sentences and paragraphs should thus be taken\ninto account. Many such models will thus be casted as \"Structured output\"\nproblems which are currently outside of the scope of scikit-learn.\n\n\n.. _hashing_vectorizer:"
  },
  {
    "filename": "feature_extraction.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\feature_extraction.rst.txt",
    "id": "feature_extraction.rst.txt_chunk_13",
    "header": "Vectorizing a large text corpus with the hashing trick",
    "text": "Vectorizing a large text corpus with the hashing trick\n------------------------------------------------------\n\nThe above vectorization scheme is simple but the fact that it holds an\n**in-memory mapping from the string tokens to the integer feature indices**\n(the ``vocabulary_`` attribute) causes several **problems when dealing with large\ndatasets**:\n\n- the larger the corpus, the larger the vocabulary will grow and hence the\n  memory use too,\n\n- fitting requires the allocation of intermediate data structures\n  of size proportional to that of the original dataset.\n\n- building the word-mapping requires a full pass over the dataset hence it is\n  not possible to fit text classifiers in a strictly online manner.\n\n- pickling and un-pickling vectorizers with a large ``vocabulary_`` can be very\n  slow (typically much slower than pickling / un-pickling flat data structures\n  such as a NumPy array of the same size),\n\n- it is not easily possible to split the vectorization work into concurrent sub\n  tasks as the ``vocabulary_`` attribute would have to be a shared state with a\n  fine grained synchronization barrier: the mapping from token string to\n  feature index is dependent on the ordering of the first occurrence of each token\n  hence would have to be shared, potentially harming the concurrent workers'\n  performance to the point of making them slower than the sequential variant.\n\nIt is possible to overcome those limitations by combining the \"hashing trick\"\n(:ref:`Feature_hashing`) implemented by the\n:class:`~sklearn.feature_extraction.FeatureHasher` class and the text\npreprocessing and tokenization features of the :class:`CountVectorizer`.\n\nThis combination is implemented in :class:`HashingVectorizer`,\na transformer class that is mostly API compatible with :class:`CountVectorizer`.\n:class:`HashingVectorizer` is stateless,\nmeaning that you don't have to call ``fit`` on it::\n\n  >>> from sklearn.feature_extraction.text import HashingVectorizer\n  >>> hv = HashingVectorizer(n_features=10)\n  >>> hv.transform(corpus)\n  <Compressed Sparse...dtype 'float64'\n    with 16 stored elements and shape (4, 10)>\n\nYou can see that 16 non-zero feature tokens were extracted in the vector\noutput: this is less than the 19 non-zeros extracted previously by the\n:class:`CountVectorizer` on the same toy corpus. The discrepancy comes from\nhash function collisions because of the low value of the ``n_features`` parameter.\n\nIn a real world setting, the ``n_features`` parameter can be left to its\ndefault value of ``2 ** 20`` (roughly one million possible features). If memory\nor downstream models size is an issue selecting a lower value such as ``2 **\n18`` might help without introducing too many additional collisions on typical\ntext classification tasks.\n\nNote that the dimensionality does not affect the CPU training time of\nalgorithms which operate on CSR matrices (``LinearSVC(dual=True)``,\n``Perceptron``, ``SGDClassifier``, ``PassiveAggressive``) but it does for\nalgorithms that work with CSC matrices (``LinearSVC(dual=False)``, ``Lasso()``,\netc.).\n\nLet's try again with the default setting::\n\n  >>> hv = HashingVectorizer()\n  >>> hv.transform(corpus)\n  <Compressed Sparse...dtype 'float64'\n    with 19 stored elements and shape (4, 1048576)>\n\nWe no longer get the collisions, but this comes at the expense of a much larger\ndimensionality of the output space.\nOf course, other terms than the 19 used here\nmight still collide with each other.\n\nThe :class:`HashingVectorizer` also comes with the following limitations:\n\n- it is not possible to invert the model (no ``inverse_transform`` method),\n  nor to access the original string representation of the features,\n  because of the one-way nature of the hash function that performs the mapping.\n\n- it does not provide IDF weighting as that would introduce statefulness in the\n  model. A :class:`TfidfTransformer` can be appended to it in a pipeline if\n  required.\n\n.. dropdown:: Performing out-of-core scaling with HashingVectorizer\n\n  An interesting development of using a :class:`HashingVectorizer` is the ability\n  to perform `out-of-core`_ scaling. This means that we can learn from data that\n  does not fit into the computer's main memory.\n\n  .. _out-of-core: https://en.wikipedia.org/wiki/Out-of-core_algorithm\n\n  A strategy to implement out-of-core scaling is to stream data to the estimator\n  in mini-batches. Each mini-batch is vectorized using :class:`HashingVectorizer`\n  so as to guarantee that the input space of the estimator has always the same\n  dimensionality. The amount of memory used at any time is thus bounded by the\n  size of a mini-batch. Although there is no limit to the amount of data that can\n  be ingested using such an approach, from a practical point of view the learning\n  time is often limited by the CPU time one wants to spend on the task.\n\n  For a full-fledged example of out-of-core scaling in a text classification\n  task see :ref:`sphx_glr_auto_examples_applications_plot_out_of_core_classification.py`."
  },
  {
    "filename": "feature_extraction.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\feature_extraction.rst.txt",
    "id": "feature_extraction.rst.txt_chunk_14",
    "header": "Customizing the vectorizer classes",
    "text": "Customizing the vectorizer classes\n----------------------------------\n\nIt is possible to customize the behavior by passing a callable\nto the vectorizer constructor::\n\n  >>> def my_tokenizer(s):\n  ...     return s.split()\n  ...\n  >>> vectorizer = CountVectorizer(tokenizer=my_tokenizer)\n  >>> vectorizer.build_analyzer()(u\"Some... punctuation!\") == (\n  ...     ['some...', 'punctuation!'])\n  True\n\nIn particular we name:\n\n* ``preprocessor``: a callable that takes an entire document as input (as a\n  single string), and returns a possibly transformed version of the document,\n  still as an entire string. This can be used to remove HTML tags, lowercase\n  the entire document, etc.\n\n* ``tokenizer``: a callable that takes the output from the preprocessor\n  and splits it into tokens, then returns a list of these.\n\n* ``analyzer``: a callable that replaces the preprocessor and tokenizer.\n  The default analyzers all call the preprocessor and tokenizer, but custom\n  analyzers will skip this. N-gram extraction and stop word filtering take\n  place at the analyzer level, so a custom analyzer may have to reproduce\n  these steps.\n\n(Lucene users might recognize these names, but be aware that scikit-learn\nconcepts may not map one-to-one onto Lucene concepts.)\n\nTo make the preprocessor, tokenizer and analyzers aware of the model\nparameters it is possible to derive from the class and override the\n``build_preprocessor``, ``build_tokenizer`` and ``build_analyzer``\nfactory methods instead of passing custom functions.\n\n.. dropdown:: Tips and tricks\n  :color: success\n\n  * If documents are pre-tokenized by an external package, then store them in\n    files (or strings) with the tokens separated by whitespace and pass\n    ``analyzer=str.split``\n  * Fancy token-level analysis such as stemming, lemmatizing, compound\n    splitting, filtering based on part-of-speech, etc. are not included in the\n    scikit-learn codebase, but can be added by customizing either the\n    tokenizer or the analyzer.\n    Here's a ``CountVectorizer`` with a tokenizer and lemmatizer using\n    `NLTK <https://www.nltk.org/>`_::\n\n        >>> from nltk import word_tokenize          # doctest: +SKIP\n        >>> from nltk.stem import WordNetLemmatizer # doctest: +SKIP\n        >>> class LemmaTokenizer:\n        ...     def __init__(self):\n        ...         self.wnl = WordNetLemmatizer()\n        ...     def __call__(self, doc):\n        ...         return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n        ...\n        >>> vect = CountVectorizer(tokenizer=LemmaTokenizer())  # doctest: +SKIP\n\n    (Note that this will not filter out punctuation.)\n\n    The following example will, for instance, transform some British spelling\n    to American spelling::\n\n        >>> import re\n        >>> def to_british(tokens):\n        ...     for t in tokens:\n        ...         t = re.sub(r\"(...)our$\", r\"\\1or\", t)\n        ...         t = re.sub(r\"([bt])re$\", r\"\\1er\", t)\n        ...         t = re.sub(r\"([iy])s(e$|ing|ation)\", r\"\\1z\\2\", t)\n        ...         t = re.sub(r\"ogue$\", \"og\", t)\n        ...         yield t\n        ...\n        >>> class CustomVectorizer(CountVectorizer):\n        ...     def build_tokenizer(self):\n        ...         tokenize = super().build_tokenizer()\n        ...         return lambda doc: list(to_british(tokenize(doc)))\n        ...\n        >>> print(CustomVectorizer().build_analyzer()(u\"color colour\"))\n        [...'color', ...'color']\n\n    for other styles of preprocessing; examples include stemming, lemmatization,\n    or normalizing numerical tokens, with the latter illustrated in:\n\n    * :ref:`sphx_glr_auto_examples_bicluster_plot_bicluster_newsgroups.py`\n\n  Customizing the vectorizer can also be useful when handling Asian languages\n  that do not use an explicit word separator such as whitespace.\n\n.. _image_feature_extraction:"
  },
  {
    "filename": "feature_extraction.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\feature_extraction.rst.txt",
    "id": "feature_extraction.rst.txt_chunk_15",
    "header": "Image feature extraction",
    "text": "Image feature extraction\n========================\n\n.. currentmodule:: sklearn.feature_extraction.image"
  },
  {
    "filename": "feature_extraction.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\feature_extraction.rst.txt",
    "id": "feature_extraction.rst.txt_chunk_16",
    "header": "Patch extraction",
    "text": "Patch extraction\n----------------\n\nThe :func:`extract_patches_2d` function extracts patches from an image stored\nas a two-dimensional array, or three-dimensional with color information along\nthe third axis. For rebuilding an image from all its patches, use\n:func:`reconstruct_from_patches_2d`. For example let us generate a 4x4 pixel\npicture with 3 color channels (e.g. in RGB format)::\n\n    >>> import numpy as np\n    >>> from sklearn.feature_extraction import image\n\n    >>> one_image = np.arange(4 * 4 * 3).reshape((4, 4, 3))\n    >>> one_image[:, :, 0]  # R channel of a fake RGB picture\n    array([[ 0,  3,  6,  9],\n           [12, 15, 18, 21],\n           [24, 27, 30, 33],\n           [36, 39, 42, 45]])\n\n    >>> patches = image.extract_patches_2d(one_image, (2, 2), max_patches=2,\n    ...     random_state=0)\n    >>> patches.shape\n    (2, 2, 2, 3)\n    >>> patches[:, :, :, 0]\n    array([[[ 0,  3],\n            [12, 15]],\n    <BLANKLINE>\n           [[15, 18],\n            [27, 30]]])\n    >>> patches = image.extract_patches_2d(one_image, (2, 2))\n    >>> patches.shape\n    (9, 2, 2, 3)\n    >>> patches[4, :, :, 0]\n    array([[15, 18],\n           [27, 30]])\n\nLet us now try to reconstruct the original image from the patches by averaging\non overlapping areas::\n\n    >>> reconstructed = image.reconstruct_from_patches_2d(patches, (4, 4, 3))\n    >>> np.testing.assert_array_equal(one_image, reconstructed)\n\nThe :class:`PatchExtractor` class works in the same way as\n:func:`extract_patches_2d`, only it supports multiple images as input. It is\nimplemented as a scikit-learn transformer, so it can be used in pipelines. See::\n\n    >>> five_images = np.arange(5 * 4 * 4 * 3).reshape(5, 4, 4, 3)\n    >>> patches = image.PatchExtractor(patch_size=(2, 2)).transform(five_images)\n    >>> patches.shape\n    (45, 2, 2, 3)\n\n.. _connectivity_graph_image:"
  },
  {
    "filename": "feature_extraction.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\feature_extraction.rst.txt",
    "id": "feature_extraction.rst.txt_chunk_17",
    "header": "Connectivity graph of an image",
    "text": "Connectivity graph of an image\n-------------------------------\n\nSeveral estimators in scikit-learn can use connectivity information between\nfeatures or samples. For instance Ward clustering\n(:ref:`hierarchical_clustering`) can cluster together only neighboring pixels\nof an image, thus forming contiguous patches:\n\n.. figure:: ../auto_examples/cluster/images/sphx_glr_plot_coin_ward_segmentation_001.png\n   :target: ../auto_examples/cluster/plot_coin_ward_segmentation.html\n   :align: center\n   :scale: 40\n\nFor this purpose, the estimators use a 'connectivity' matrix, giving\nwhich samples are connected.\n\nThe function :func:`img_to_graph` returns such a matrix from a 2D or 3D\nimage. Similarly, :func:`grid_to_graph` builds a connectivity matrix for\nimages given the shape of these images.\n\nThese matrices can be used to impose connectivity in estimators that use\nconnectivity information, such as Ward clustering\n(:ref:`hierarchical_clustering`), but also to build precomputed kernels,\nor similarity matrices.\n\n.. note:: **Examples**\n\n   * :ref:`sphx_glr_auto_examples_cluster_plot_coin_ward_segmentation.py`\n\n   * :ref:`sphx_glr_auto_examples_cluster_plot_segmentation_toy.py`\n\n   * :ref:`sphx_glr_auto_examples_cluster_plot_feature_agglomeration_vs_univariate_selection.py`"
  },
  {
    "filename": "feature_selection.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\feature_selection.rst.txt",
    "id": "feature_selection.rst.txt_chunk_0",
    "header": ".. currentmodule:: sklearn.feature_selection",
    "text": ".. currentmodule:: sklearn.feature_selection\n\n.. _feature_selection:\n\n================="
  },
  {
    "filename": "feature_selection.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\feature_selection.rst.txt",
    "id": "feature_selection.rst.txt_chunk_1",
    "header": "Feature selection",
    "text": "Feature selection\n=================\n\n\nThe classes in the :mod:`sklearn.feature_selection` module can be used\nfor feature selection/dimensionality reduction on sample sets, either to\nimprove estimators' accuracy scores or to boost their performance on very\nhigh-dimensional datasets.\n\n\n.. _variance_threshold:"
  },
  {
    "filename": "feature_selection.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\feature_selection.rst.txt",
    "id": "feature_selection.rst.txt_chunk_2",
    "header": "Removing features with low variance",
    "text": "Removing features with low variance\n===================================\n\n:class:`VarianceThreshold` is a simple baseline approach to feature selection.\nIt removes all features whose variance doesn't meet some threshold.\nBy default, it removes all zero-variance features,\ni.e. features that have the same value in all samples.\n\nAs an example, suppose that we have a dataset with boolean features,\nand we want to remove all features that are either one or zero (on or off)\nin more than 80% of the samples.\nBoolean features are Bernoulli random variables,\nand the variance of such variables is given by\n\n.. math:: \\mathrm{Var}[X] = p(1 - p)\n\nso we can select using the threshold ``.8 * (1 - .8)``::\n\n  >>> from sklearn.feature_selection import VarianceThreshold\n  >>> X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]\n  >>> sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n  >>> sel.fit_transform(X)\n  array([[0, 1],\n         [1, 0],\n         [0, 0],\n         [1, 1],\n         [1, 0],\n         [1, 1]])\n\nAs expected, ``VarianceThreshold`` has removed the first column,\nwhich has a probability :math:`p = 5/6 > .8` of containing a zero.\n\n.. _univariate_feature_selection:"
  },
  {
    "filename": "feature_selection.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\feature_selection.rst.txt",
    "id": "feature_selection.rst.txt_chunk_3",
    "header": "Univariate feature selection",
    "text": "Univariate feature selection\n============================\n\nUnivariate feature selection works by selecting the best features based on\nunivariate statistical tests. It can be seen as a preprocessing step\nto an estimator. Scikit-learn exposes feature selection routines\nas objects that implement the ``transform`` method:\n\n* :class:`SelectKBest` removes all but the :math:`k` highest scoring features\n\n* :class:`SelectPercentile` removes all but a user-specified highest scoring\n  percentage of features\n\n* using common univariate statistical tests for each feature:\n  false positive rate :class:`SelectFpr`, false discovery rate\n  :class:`SelectFdr`, or family wise error :class:`SelectFwe`.\n\n* :class:`GenericUnivariateSelect` allows to perform univariate feature\n  selection with a configurable strategy. This allows to select the best\n  univariate selection strategy with hyper-parameter search estimator.\n\nFor instance, we can use a F-test to retrieve the two\nbest features for a dataset as follows:\n\n  >>> from sklearn.datasets import load_iris\n  >>> from sklearn.feature_selection import SelectKBest\n  >>> from sklearn.feature_selection import f_classif\n  >>> X, y = load_iris(return_X_y=True)\n  >>> X.shape\n  (150, 4)\n  >>> X_new = SelectKBest(f_classif, k=2).fit_transform(X, y)\n  >>> X_new.shape\n  (150, 2)\n\nThese objects take as input a scoring function that returns univariate scores\nand p-values (or only scores for :class:`SelectKBest` and\n:class:`SelectPercentile`):\n\n* For regression: :func:`r_regression`, :func:`f_regression`, :func:`mutual_info_regression`\n\n* For classification: :func:`chi2`, :func:`f_classif`, :func:`mutual_info_classif`\n\nThe methods based on F-test estimate the degree of linear dependency between\ntwo random variables. On the other hand, mutual information methods can capture\nany kind of statistical dependency, but being nonparametric, they require more\nsamples for accurate estimation. Note that the :math:`\\chi^2`-test should only be\napplied to non-negative features, such as frequencies.\n\n.. topic:: Feature selection with sparse data\n\n   If you use sparse data (i.e. data represented as sparse matrices),\n   :func:`chi2`, :func:`mutual_info_regression`, :func:`mutual_info_classif`\n   will deal with the data without making it dense.\n\n.. warning::\n\n    Beware not to use a regression scoring function with a classification\n    problem, you will get useless results.\n\n.. note::\n\n    The :class:`SelectPercentile` and :class:`SelectKBest` support unsupervised\n    feature selection as well. One needs to provide a `score_func` where `y=None`.\n    The `score_func` should use internally `X` to compute the scores.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_feature_selection_plot_feature_selection.py`\n\n* :ref:`sphx_glr_auto_examples_feature_selection_plot_f_test_vs_mi.py`\n\n.. _rfe:"
  },
  {
    "filename": "feature_selection.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\feature_selection.rst.txt",
    "id": "feature_selection.rst.txt_chunk_4",
    "header": "Recursive feature elimination",
    "text": "Recursive feature elimination\n=============================\n\nGiven an external estimator that assigns weights to features (e.g., the\ncoefficients of a linear model), the goal of recursive feature elimination (:class:`RFE`)\nis to select features by recursively considering smaller and smaller sets of\nfeatures. First, the estimator is trained on the initial set of features and\nthe importance of each feature is obtained either through any specific attribute\n(such as ``coef_``, ``feature_importances_``) or callable. Then, the least important\nfeatures are pruned from the current set of features. That procedure is recursively\nrepeated on the pruned set until the desired number of features to select is\neventually reached.\n\n:class:`RFECV` performs RFE in a cross-validation loop to find the optimal\nnumber of features. In more details, the number of features selected is tuned\nautomatically by fitting an :class:`RFE` selector on the different\ncross-validation splits (provided by the `cv` parameter). The performance\nof the :class:`RFE` selector is evaluated using `scorer` for different numbers\nof selected features and aggregated together. Finally, the scores are averaged\nacross folds and the number of features selected is set to the number of\nfeatures that maximize the cross-validation score.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_feature_selection_plot_rfe_digits.py`: A recursive feature elimination example\n  showing the relevance of pixels in a digit classification task.\n\n* :ref:`sphx_glr_auto_examples_feature_selection_plot_rfe_with_cross_validation.py`: A recursive feature\n  elimination example with automatic tuning of the number of features\n  selected with cross-validation.\n\n.. _select_from_model:"
  },
  {
    "filename": "feature_selection.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\feature_selection.rst.txt",
    "id": "feature_selection.rst.txt_chunk_5",
    "header": "Feature selection using SelectFromModel",
    "text": "Feature selection using SelectFromModel\n=======================================\n\n:class:`SelectFromModel` is a meta-transformer that can be used alongside any\nestimator that assigns importance to each feature through a specific attribute (such as\n``coef_``, ``feature_importances_``) or via an `importance_getter` callable after fitting.\nThe features are considered unimportant and removed if the corresponding\nimportance of the feature values is below the provided\n``threshold`` parameter. Apart from specifying the threshold numerically,\nthere are built-in heuristics for finding a threshold using a string argument.\nAvailable heuristics are \"mean\", \"median\" and float multiples of these like\n\"0.1*mean\". In combination with the `threshold` criteria, one can use the\n`max_features` parameter to set a limit on the number of features to select.\n\nFor examples on how it is to be used refer to the sections below.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_feature_selection_plot_select_from_model_diabetes.py`\n\n.. _l1_feature_selection:"
  },
  {
    "filename": "feature_selection.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\feature_selection.rst.txt",
    "id": "feature_selection.rst.txt_chunk_6",
    "header": "L1-based feature selection",
    "text": "L1-based feature selection\n--------------------------\n\n.. currentmodule:: sklearn\n\n:ref:`Linear models <linear_model>` penalized with the L1 norm have\nsparse solutions: many of their estimated coefficients are zero. When the goal\nis to reduce the dimensionality of the data to use with another classifier,\nthey can be used along with :class:`~feature_selection.SelectFromModel`\nto select the non-zero coefficients. In particular, sparse estimators useful\nfor this purpose are the :class:`~linear_model.Lasso` for regression, and\nof :class:`~linear_model.LogisticRegression` and :class:`~svm.LinearSVC`\nfor classification::\n\n  >>> from sklearn.svm import LinearSVC\n  >>> from sklearn.datasets import load_iris\n  >>> from sklearn.feature_selection import SelectFromModel\n  >>> X, y = load_iris(return_X_y=True)\n  >>> X.shape\n  (150, 4)\n  >>> lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\n  >>> model = SelectFromModel(lsvc, prefit=True)\n  >>> X_new = model.transform(X)\n  >>> X_new.shape\n  (150, 3)\n\nWith SVMs and logistic regression, the parameter C controls the sparsity:\nthe smaller C the fewer features selected. With Lasso, the higher the\nalpha parameter, the fewer features selected.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_linear_model_plot_lasso_dense_vs_sparse_data.py`.\n\n.. _compressive_sensing:\n\n.. dropdown:: L1-recovery and compressive sensing\n\n  For a good choice of alpha, the :ref:`lasso` can fully recover the\n  exact set of non-zero variables using only few observations, provided\n  certain specific conditions are met. In particular, the number of\n  samples should be \"sufficiently large\", or L1 models will perform at\n  random, where \"sufficiently large\" depends on the number of non-zero\n  coefficients, the logarithm of the number of features, the amount of\n  noise, the smallest absolute value of non-zero coefficients, and the\n  structure of the design matrix X. In addition, the design matrix must\n  display certain specific properties, such as not being too correlated.\n  On the use of Lasso for sparse signal recovery, see this example on\n  compressive sensing:\n  :ref:`sphx_glr_auto_examples_applications_plot_tomography_l1_reconstruction.py`.\n\n  There is no general rule to select an alpha parameter for recovery of\n  non-zero coefficients. It can be set by cross-validation\n  (:class:`~sklearn.linear_model.LassoCV` or\n  :class:`~sklearn.linear_model.LassoLarsCV`), though this may lead to\n  under-penalized models: including a small number of non-relevant variables\n  is not detrimental to prediction score. BIC\n  (:class:`~sklearn.linear_model.LassoLarsIC`) tends, on the opposite, to set\n  high values of alpha.\n\n  .. rubric:: References\n\n  Richard G. Baraniuk \"Compressive Sensing\", IEEE Signal\n  Processing Magazine [120] July 2007\n  http://users.isr.ist.utl.pt/~aguiar/CS_notes.pdf"
  },
  {
    "filename": "feature_selection.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\feature_selection.rst.txt",
    "id": "feature_selection.rst.txt_chunk_7",
    "header": "Tree-based feature selection",
    "text": "Tree-based feature selection\n----------------------------\n\nTree-based estimators (see the :mod:`sklearn.tree` module and forest\nof trees in the :mod:`sklearn.ensemble` module) can be used to compute\nimpurity-based feature importances, which in turn can be used to discard irrelevant\nfeatures (when coupled with the :class:`~feature_selection.SelectFromModel`\nmeta-transformer)::\n\n  >>> from sklearn.ensemble import ExtraTreesClassifier\n  >>> from sklearn.datasets import load_iris\n  >>> from sklearn.feature_selection import SelectFromModel\n  >>> X, y = load_iris(return_X_y=True)\n  >>> X.shape\n  (150, 4)\n  >>> clf = ExtraTreesClassifier(n_estimators=50)\n  >>> clf = clf.fit(X, y)\n  >>> clf.feature_importances_  # doctest: +SKIP\n  array([ 0.04,  0.05,  0.4,  0.4])\n  >>> model = SelectFromModel(clf, prefit=True)\n  >>> X_new = model.transform(X)\n  >>> X_new.shape               # doctest: +SKIP\n  (150, 2)\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_ensemble_plot_forest_importances.py`: example on\n  synthetic data showing the recovery of the actually meaningful features.\n\n* :ref:`sphx_glr_auto_examples_inspection_plot_permutation_importance.py`: example\n  discussing the caveats of using impurity-based feature importances as a proxy for\n  feature relevance.\n\n.. _sequential_feature_selection:"
  },
  {
    "filename": "feature_selection.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\feature_selection.rst.txt",
    "id": "feature_selection.rst.txt_chunk_8",
    "header": "Sequential Feature Selection",
    "text": "Sequential Feature Selection\n============================\n\nSequential Feature Selection [sfs]_ (SFS) is available in the\n:class:`~sklearn.feature_selection.SequentialFeatureSelector` transformer.\nSFS can be either forward or backward:\n\nForward-SFS is a greedy procedure that iteratively finds the best new feature\nto add to the set of selected features. Concretely, we initially start with\nzero features and find the one feature that maximizes a cross-validated score\nwhen an estimator is trained on this single feature. Once that first feature\nis selected, we repeat the procedure by adding a new feature to the set of\nselected features. The procedure stops when the desired number of selected\nfeatures is reached, as determined by the `n_features_to_select` parameter.\n\nBackward-SFS follows the same idea but works in the opposite direction:\ninstead of starting with no features and greedily adding features, we start\nwith *all* the features and greedily *remove* features from the set. The\n`direction` parameter controls whether forward or backward SFS is used.\n\n.. dropdown:: Details on Sequential Feature Selection\n\n  In general, forward and backward selection do not yield equivalent results.\n  Also, one may be much faster than the other depending on the requested number\n  of selected features: if we have 10 features and ask for 7 selected features,\n  forward selection would need to perform 7 iterations while backward selection\n  would only need to perform 3.\n\n  SFS differs from :class:`~sklearn.feature_selection.RFE` and\n  :class:`~sklearn.feature_selection.SelectFromModel` in that it does not\n  require the underlying model to expose a `coef_` or `feature_importances_`\n  attribute. It may however be slower considering that more models need to be\n  evaluated, compared to the other approaches. For example in backward\n  selection, the iteration going from `m` features to `m - 1` features using k-fold\n  cross-validation requires fitting `m * k` models, while\n  :class:`~sklearn.feature_selection.RFE` would require only a single fit, and\n  :class:`~sklearn.feature_selection.SelectFromModel` always just does a single\n  fit and requires no iterations.\n\n  .. rubric:: References\n\n  .. [sfs] Ferri et al, `Comparative study of techniques for\n      large-scale feature selection\n      <https://citeseerx.ist.psu.edu/doc_view/pid/5fedabbb3957bbb442802e012d829ee0629a01b6>`_.\n\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_feature_selection_plot_select_from_model_diabetes.py`"
  },
  {
    "filename": "feature_selection.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\feature_selection.rst.txt",
    "id": "feature_selection.rst.txt_chunk_9",
    "header": "Feature selection as part of a pipeline",
    "text": "Feature selection as part of a pipeline\n=======================================\n\nFeature selection is usually used as a pre-processing step before doing\nthe actual learning. The recommended way to do this in scikit-learn is\nto use a :class:`~pipeline.Pipeline`::\n\n  clf = Pipeline([\n    ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\"))),\n    ('classification', RandomForestClassifier())\n  ])\n  clf.fit(X, y)\n\nIn this snippet we make use of a :class:`~svm.LinearSVC`\ncoupled with :class:`~feature_selection.SelectFromModel`\nto evaluate feature importances and select the most relevant features.\nThen, a :class:`~ensemble.RandomForestClassifier` is trained on the\ntransformed output, i.e. using only relevant features. You can perform\nsimilar operations with the other feature selection methods and also\nclassifiers that provide a way to evaluate feature importances of course.\nSee the :class:`~pipeline.Pipeline` examples for more details."
  },
  {
    "filename": "gaussian_process.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\gaussian_process.rst.txt",
    "id": "gaussian_process.rst.txt_chunk_0",
    "header": ".. _gaussian_process:",
    "text": ".. _gaussian_process:\n\n=================="
  },
  {
    "filename": "gaussian_process.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\gaussian_process.rst.txt",
    "id": "gaussian_process.rst.txt_chunk_1",
    "header": "Gaussian Processes",
    "text": "Gaussian Processes\n==================\n\n.. currentmodule:: sklearn.gaussian_process\n\n**Gaussian Processes (GP)** are a nonparametric supervised learning method used\nto solve *regression* and *probabilistic classification* problems.\n\nThe advantages of Gaussian processes are:\n\n- The prediction interpolates the observations (at least for regular\n  kernels).\n\n- The prediction is probabilistic (Gaussian) so that one can compute\n  empirical confidence intervals and decide based on those if one should\n  refit (online fitting, adaptive fitting) the prediction in some\n  region of interest.\n\n- Versatile: different :ref:`kernels\n  <gp_kernels>` can be specified. Common kernels are provided, but\n  it is also possible to specify custom kernels.\n\nThe disadvantages of Gaussian processes include:\n\n- Our implementation is not sparse, i.e., they use the whole samples/features\n  information to perform the prediction.\n\n- They lose efficiency in high dimensional spaces -- namely when the number\n  of features exceeds a few dozens.\n\n\n.. _gpr:"
  },
  {
    "filename": "gaussian_process.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\gaussian_process.rst.txt",
    "id": "gaussian_process.rst.txt_chunk_2",
    "header": "Gaussian Process Regression (GPR)",
    "text": "Gaussian Process Regression (GPR)\n=================================\n\n.. currentmodule:: sklearn.gaussian_process\n\nThe :class:`GaussianProcessRegressor` implements Gaussian processes (GP) for\nregression purposes. For this, the prior of the GP needs to be specified. GP\nwill combine this prior and the likelihood function based on training samples.\nIt allows to give a probabilistic approach to prediction by giving the mean and\nstandard deviation as output when predicting.\n\n.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_noisy_targets_002.png\n   :target: ../auto_examples/gaussian_process/plot_gpr_noisy_targets.html\n   :align: center\n\nThe prior mean is assumed to be constant and zero (for `normalize_y=False`) or\nthe training data's mean (for `normalize_y=True`). The prior's covariance is\nspecified by passing a :ref:`kernel <gp_kernels>` object. The hyperparameters\nof the kernel are optimized when fitting the :class:`GaussianProcessRegressor`\nby maximizing the log-marginal-likelihood (LML) based on the passed\n`optimizer`. As the LML may have multiple local optima, the optimizer can be\nstarted repeatedly by specifying `n_restarts_optimizer`. The first run is\nalways conducted starting from the initial hyperparameter values of the kernel;\nsubsequent runs are conducted from hyperparameter values that have been chosen\nrandomly from the range of allowed values. If the initial hyperparameters\nshould be kept fixed, `None` can be passed as optimizer.\n\nThe noise level in the targets can be specified by passing it via the parameter\n`alpha`, either globally as a scalar or per datapoint. Note that a moderate\nnoise level can also be helpful for dealing with numeric instabilities during\nfitting as it is effectively implemented as Tikhonov regularization, i.e., by\nadding it to the diagonal of the kernel matrix. An alternative to specifying\nthe noise level explicitly is to include a\n:class:`~sklearn.gaussian_process.kernels.WhiteKernel` component into the\nkernel, which can estimate the global noise level from the data (see example\nbelow). The figure below shows the effect of noisy target handled by setting\nthe parameter `alpha`.\n\n.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_noisy_targets_003.png\n   :target: ../auto_examples/gaussian_process/plot_gpr_noisy_targets.html\n   :align: center\n\nThe implementation is based on Algorithm 2.1 of [RW2006]_. In addition to\nthe API of standard scikit-learn estimators, :class:`GaussianProcessRegressor`:\n\n* allows prediction without prior fitting (based on the GP prior)\n\n* provides an additional method ``sample_y(X)``, which evaluates samples\n  drawn from the GPR (prior or posterior) at given inputs\n\n* exposes a method ``log_marginal_likelihood(theta)``, which can be used\n  externally for other ways of selecting hyperparameters, e.g., via\n  Markov chain Monte Carlo.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_gaussian_process_plot_gpr_noisy_targets.py`\n* :ref:`sphx_glr_auto_examples_gaussian_process_plot_gpr_noisy.py`\n* :ref:`sphx_glr_auto_examples_gaussian_process_plot_compare_gpr_krr.py`\n* :ref:`sphx_glr_auto_examples_gaussian_process_plot_gpr_co2.py`\n\n.. _gpc:"
  },
  {
    "filename": "gaussian_process.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\gaussian_process.rst.txt",
    "id": "gaussian_process.rst.txt_chunk_3",
    "header": "Gaussian Process Classification (GPC)",
    "text": "Gaussian Process Classification (GPC)\n=====================================\n\n.. currentmodule:: sklearn.gaussian_process\n\nThe :class:`GaussianProcessClassifier` implements Gaussian processes (GP) for\nclassification purposes, more specifically for probabilistic classification,\nwhere test predictions take the form of class probabilities.\nGaussianProcessClassifier places a GP prior on a latent function :math:`f`,\nwhich is then squashed through a link function :math:`\\pi` to obtain the probabilistic\nclassification. The latent function :math:`f` is a so-called nuisance function,\nwhose values are not observed and are not relevant by themselves.\nIts purpose is to allow a convenient formulation of the model, and :math:`f`\nis removed (integrated out) during prediction. :class:`GaussianProcessClassifier`\nimplements the logistic link function, for which the integral cannot be\ncomputed analytically but is easily approximated in the binary case.\n\nIn contrast to the regression setting, the posterior of the latent function\n:math:`f` is not Gaussian even for a GP prior since a Gaussian likelihood is\ninappropriate for discrete class labels. Rather, a non-Gaussian likelihood\ncorresponding to the logistic link function (logit) is used.\nGaussianProcessClassifier approximates the non-Gaussian posterior with a\nGaussian based on the Laplace approximation. More details can be found in\nChapter 3 of [RW2006]_.\n\nThe GP prior mean is assumed to be zero. The prior's\ncovariance is specified by passing a :ref:`kernel <gp_kernels>` object. The\nhyperparameters of the kernel are optimized during fitting of\nGaussianProcessRegressor by maximizing the log-marginal-likelihood (LML) based\non the passed ``optimizer``. As the LML may have multiple local optima, the\noptimizer can be started repeatedly by specifying ``n_restarts_optimizer``. The\nfirst run is always conducted starting from the initial hyperparameter values\nof the kernel; subsequent runs are conducted from hyperparameter values\nthat have been chosen randomly from the range of allowed values.\nIf the initial hyperparameters should be kept fixed, `None` can be passed as\noptimizer.\n\nIn some scenarios, information about the latent function :math:`f` is desired\n(i.e. the mean :math:`\\bar{f_*}` and the variance :math:`\\text{Var}[f_*]` described\nin Eqs. (3.21) and (3.24) of [RW2006]_). The :class:`GaussianProcessClassifier`\nprovides access to these quantities via the `latent_mean_and_variance` method.\n\n:class:`GaussianProcessClassifier` supports multi-class classification\nby performing either one-versus-rest or one-versus-one based training and\nprediction.  In one-versus-rest, one binary Gaussian process classifier is\nfitted for each class, which is trained to separate this class from the rest.\nIn \"one_vs_one\", one binary Gaussian process classifier is fitted for each pair\nof classes, which is trained to separate these two classes. The predictions of\nthese binary predictors are combined into multi-class predictions. See the\nsection on :ref:`multi-class classification <multiclass>` for more details.\n\nIn the case of Gaussian process classification, \"one_vs_one\" might be\ncomputationally  cheaper since it has to solve many problems involving only a\nsubset of the whole training set rather than fewer problems on the whole\ndataset. Since Gaussian process classification scales cubically with the size\nof the dataset, this might be considerably faster. However, note that\n\"one_vs_one\" does not support predicting probability estimates but only plain\npredictions. Moreover, note that :class:`GaussianProcessClassifier` does not\n(yet) implement a true multi-class Laplace approximation internally, but\nas discussed above is based on solving several binary classification tasks\ninternally, which are combined using one-versus-rest or one-versus-one."
  },
  {
    "filename": "gaussian_process.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\gaussian_process.rst.txt",
    "id": "gaussian_process.rst.txt_chunk_4",
    "header": "GPC examples",
    "text": "GPC examples\n============"
  },
  {
    "filename": "gaussian_process.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\gaussian_process.rst.txt",
    "id": "gaussian_process.rst.txt_chunk_5",
    "header": "Probabilistic predictions with GPC",
    "text": "Probabilistic predictions with GPC\n----------------------------------\n\nThis example illustrates the predicted probability of GPC for an RBF kernel\nwith different choices of the hyperparameters. The first figure shows the\npredicted probability of GPC with arbitrarily chosen hyperparameters and with\nthe hyperparameters corresponding to the maximum log-marginal-likelihood (LML).\n\nWhile the hyperparameters chosen by optimizing LML have a considerably larger\nLML, they perform slightly worse according to the log-loss on test data. The\nfigure shows that this is because they exhibit a steep change of the class\nprobabilities at the class boundaries (which is good) but have predicted\nprobabilities close to 0.5 far away from the class boundaries (which is bad).\nThis undesirable effect is caused by the Laplace approximation used\ninternally by GPC.\n\nThe second figure shows the log-marginal-likelihood for different choices of\nthe kernel's hyperparameters, highlighting the two choices of the\nhyperparameters used in the first figure by black dots.\n\n.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpc_001.png\n   :target: ../auto_examples/gaussian_process/plot_gpc.html\n   :align: center\n\n.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpc_002.png\n   :target: ../auto_examples/gaussian_process/plot_gpc.html\n   :align: center"
  },
  {
    "filename": "gaussian_process.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\gaussian_process.rst.txt",
    "id": "gaussian_process.rst.txt_chunk_6",
    "header": "Illustration of GPC on the XOR dataset",
    "text": "Illustration of GPC on the XOR dataset\n--------------------------------------\n\n.. currentmodule:: sklearn.gaussian_process.kernels\n\nThis example illustrates GPC on XOR data. Compared are a stationary, isotropic\nkernel (:class:`RBF`) and a non-stationary kernel (:class:`DotProduct`). On\nthis particular dataset, the :class:`DotProduct` kernel obtains considerably\nbetter results because the class-boundaries are linear and coincide with the\ncoordinate axes. In practice, however, stationary kernels such as :class:`RBF`\noften obtain better results.\n\n.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpc_xor_001.png\n   :target: ../auto_examples/gaussian_process/plot_gpc_xor.html\n   :align: center\n\n.. currentmodule:: sklearn.gaussian_process"
  },
  {
    "filename": "gaussian_process.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\gaussian_process.rst.txt",
    "id": "gaussian_process.rst.txt_chunk_7",
    "header": "Gaussian process classification (GPC) on iris dataset",
    "text": "Gaussian process classification (GPC) on iris dataset\n-----------------------------------------------------\n\nThis example illustrates the predicted probability of GPC for an isotropic\nand anisotropic RBF kernel on a two-dimensional version for the iris dataset.\nThis illustrates the applicability of GPC to non-binary classification.\nThe anisotropic RBF kernel obtains slightly higher log-marginal-likelihood by\nassigning different length-scales to the two feature dimensions.\n\n.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpc_iris_001.png\n   :target: ../auto_examples/gaussian_process/plot_gpc_iris.html\n   :align: center\n\n\n.. _gp_kernels:"
  },
  {
    "filename": "gaussian_process.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\gaussian_process.rst.txt",
    "id": "gaussian_process.rst.txt_chunk_8",
    "header": "Kernels for Gaussian Processes",
    "text": "Kernels for Gaussian Processes\n==============================\n.. currentmodule:: sklearn.gaussian_process.kernels\n\nKernels (also called \"covariance functions\" in the context of GPs) are a crucial\ningredient of GPs which determine the shape of prior and posterior of the GP.\nThey encode the assumptions on the function being learned by defining the \"similarity\"\nof two datapoints combined with the assumption that similar datapoints should\nhave similar target values. Two categories of kernels can be distinguished:\nstationary kernels depend only on the distance of two datapoints and not on their\nabsolute values :math:`k(x_i, x_j)= k(d(x_i, x_j))` and are thus invariant to\ntranslations in the input space, while non-stationary kernels\ndepend also on the specific values of the datapoints. Stationary kernels can further\nbe subdivided into isotropic and anisotropic kernels, where isotropic kernels are\nalso invariant to rotations in the input space. For more details, we refer to\nChapter 4 of [RW2006]_. :ref:`This example\n<sphx_glr_auto_examples_gaussian_process_plot_gpr_on_structured_data.py>`\nshows how to define a custom kernel over discrete data. For guidance on how to best\ncombine different kernels, we refer to [Duv2014]_.\n\n.. dropdown:: Gaussian Process Kernel API\n\n   The main usage of a :class:`Kernel` is to compute the GP's covariance between\n   datapoints. For this, the method ``__call__`` of the kernel can be called. This\n   method can either be used to compute the \"auto-covariance\" of all pairs of\n   datapoints in a 2d array X, or the \"cross-covariance\" of all combinations\n   of datapoints of a 2d array X with datapoints in a 2d array Y. The following\n   identity holds true for all kernels k (except for the :class:`WhiteKernel`):\n   ``k(X) == K(X, Y=X)``\n\n   If only the diagonal of the auto-covariance is being used, the method ``diag()``\n   of a kernel can be called, which is more computationally efficient than the\n   equivalent call to ``__call__``: ``np.diag(k(X, X)) == k.diag(X)``\n\n   Kernels are parameterized by a vector :math:`\\theta` of hyperparameters. These\n   hyperparameters can for instance control length-scales or periodicity of a\n   kernel (see below). All kernels support computing analytic gradients\n   of the kernel's auto-covariance with respect to :math:`log(\\theta)` via setting\n   ``eval_gradient=True`` in the ``__call__`` method.\n   That is, a ``(len(X), len(X), len(theta))`` array is returned where the entry\n   ``[i, j, l]`` contains :math:`\\frac{\\partial k_\\theta(x_i, x_j)}{\\partial log(\\theta_l)}`.\n   This gradient is used by the Gaussian process (both regressor and classifier)\n   in computing the gradient of the log-marginal-likelihood, which in turn is used\n   to determine the value of :math:`\\theta`, which maximizes the log-marginal-likelihood,\n   via gradient ascent. For each hyperparameter, the initial value and the\n   bounds need to be specified when creating an instance of the kernel. The\n   current value of :math:`\\theta` can be get and set via the property\n   ``theta`` of the kernel object. Moreover, the bounds of the hyperparameters can be\n   accessed by the property ``bounds`` of the kernel. Note that both properties\n   (theta and bounds) return log-transformed values of the internally used values\n   since those are typically more amenable to gradient-based optimization.\n   The specification of each hyperparameter is stored in the form of an instance of\n   :class:`Hyperparameter` in the respective kernel. Note that a kernel using a\n   hyperparameter with name \"x\" must have the attributes self.x and self.x_bounds.\n\n   The abstract base class for all kernels is :class:`Kernel`. Kernel implements a\n   similar interface as :class:`~sklearn.base.BaseEstimator`, providing the\n   methods ``get_params()``, ``set_params()``, and ``clone()``. This allows\n   setting kernel values also via meta-estimators such as\n   :class:`~sklearn.pipeline.Pipeline` or\n   :class:`~sklearn.model_selection.GridSearchCV`. Note that due to the nested\n   structure of kernels (by applying kernel operators, see below), the names of\n   kernel parameters might become relatively complicated. In general, for a binary\n   kernel operator, parameters of the left operand are prefixed with ``k1__`` and\n   parameters of the right operand with ``k2__``. An additional convenience method\n   is ``clone_with_theta(theta)``, which returns a cloned version of the kernel\n   but with the hyperparameters set to ``theta``. An illustrative example:\n\n      >>> from sklearn.gaussian_process.kernels import ConstantKernel, RBF\n      >>> kernel = ConstantKernel(constant_value=1.0, constant_value_bounds=(0.0, 10.0)) * RBF(length_scale=0.5, length_scale_bounds=(0.0, 10.0)) + RBF(length_scale=2.0, length_scale_bounds=(0.0, 10.0))\n      >>> for hyperparameter in kernel.hyperparameters: print(hyperparameter)\n      Hyperparameter(name='k1__k1__constant_value', value_type='numeric', bounds=array([[ 0., 10.]]), n_elements=1, fixed=False)\n      Hyperparameter(name='k1__k2__length_scale', value_type='numeric', bounds=array([[ 0., 10.]]), n_elements=1, fixed=False)\n      Hyperparameter(name='k2__length_scale', value_type='numeric', bounds=array([[ 0., 10.]]), n_elements=1, fixed=False)\n      >>> params = kernel.get_params()\n      >>> for key in sorted(params): print(\"%s : %s\" % (key, params[key]))\n      k1 : 1**2 * RBF(length_scale=0.5)\n      k1__k1 : 1**2\n      k1__k1__constant_value : 1.0\n      k1__k1__constant_value_bounds : (0.0, 10.0)\n      k1__k2 : RBF(length_scale=0.5)\n      k1__k2__length_scale : 0.5\n      k1__k2__length_scale_bounds : (0.0, 10.0)\n      k2 : RBF(length_scale=2)\n      k2__length_scale : 2.0\n      k2__length_scale_bounds : (0.0, 10.0)\n      >>> print(kernel.theta)  # Note: log-transformed\n      [ 0.         -0.69314718  0.69314718]\n      >>> print(kernel.bounds)  # Note: log-transformed\n      [[      -inf 2.30258509]\n      [      -inf 2.30258509]\n      [      -inf 2.30258509]]\n\n   All Gaussian process kernels are interoperable with :mod:`sklearn.metrics.pairwise`\n   and vice versa: instances of subclasses of :class:`Kernel` can be passed as\n   ``metric`` to ``pairwise_kernels`` from :mod:`sklearn.metrics.pairwise`. Moreover,\n   kernel functions from pairwise can be used as GP kernels by using the wrapper\n   class :class:`PairwiseKernel`. The only caveat is that the gradient of\n   the hyperparameters is not analytic but numeric and all those kernels support\n   only isotropic distances. The parameter ``gamma`` is considered to be a\n   hyperparameter and may be optimized. The other kernel parameters are set\n   directly at initialization and are kept fixed."
  },
  {
    "filename": "gaussian_process.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\gaussian_process.rst.txt",
    "id": "gaussian_process.rst.txt_chunk_9",
    "header": "Basic kernels",
    "text": "Basic kernels\n-------------\nThe :class:`ConstantKernel` kernel can be used as part of a :class:`Product`\nkernel where it scales the magnitude of the other factor (kernel) or as part\nof a :class:`Sum` kernel, where it modifies the mean of the Gaussian process.\nIt depends on a parameter :math:`constant\\_value`. It is defined as:\n\n.. math::\n   k(x_i, x_j) = constant\\_value \\;\\forall\\; x_i, x_j\n\nThe main use-case of the :class:`WhiteKernel` kernel is as part of a\nsum-kernel where it explains the noise-component of the signal. Tuning its\nparameter :math:`noise\\_level` corresponds to estimating the noise-level.\nIt is defined as:\n\n.. math::\n    k(x_i, x_j) = noise\\_level \\text{ if } x_i == x_j \\text{ else } 0"
  },
  {
    "filename": "gaussian_process.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\gaussian_process.rst.txt",
    "id": "gaussian_process.rst.txt_chunk_10",
    "header": "Kernel operators",
    "text": "Kernel operators\n----------------\nKernel operators take one or two base kernels and combine them into a new\nkernel. The :class:`Sum` kernel takes two kernels :math:`k_1` and :math:`k_2`\nand combines them via :math:`k_{sum}(X, Y) = k_1(X, Y) + k_2(X, Y)`.\nThe  :class:`Product` kernel takes two kernels :math:`k_1` and :math:`k_2`\nand combines them via :math:`k_{product}(X, Y) = k_1(X, Y) * k_2(X, Y)`.\nThe :class:`Exponentiation` kernel takes one base kernel and a scalar parameter\n:math:`p` and combines them via\n:math:`k_{exp}(X, Y) = k(X, Y)^p`.\nNote that magic methods ``__add__``, ``__mul___`` and ``__pow__`` are\noverridden on the Kernel objects, so one can use e.g. ``RBF() + RBF()`` as\na shortcut for ``Sum(RBF(), RBF())``."
  },
  {
    "filename": "gaussian_process.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\gaussian_process.rst.txt",
    "id": "gaussian_process.rst.txt_chunk_11",
    "header": "Radial basis function (RBF) kernel",
    "text": "Radial basis function (RBF) kernel\n----------------------------------\nThe :class:`RBF` kernel is a stationary kernel. It is also known as the \"squared\nexponential\" kernel. It is parameterized by a length-scale parameter :math:`l>0`, which\ncan either be a scalar (isotropic variant of the kernel) or a vector with the same\nnumber of dimensions as the inputs :math:`x` (anisotropic variant of the kernel).\nThe kernel is given by:\n\n.. math::\n   k(x_i, x_j) = \\text{exp}\\left(- \\frac{d(x_i, x_j)^2}{2l^2} \\right)\n\nwhere :math:`d(\\cdot, \\cdot)` is the Euclidean distance.\nThis kernel is infinitely differentiable, which implies that GPs with this\nkernel as covariance function have mean square derivatives of all orders, and are thus\nvery smooth. The prior and posterior of a GP resulting from an RBF kernel are shown in\nthe following figure:\n\n.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_prior_posterior_001.png\n   :target: ../auto_examples/gaussian_process/plot_gpr_prior_posterior.html\n   :align: center"
  },
  {
    "filename": "gaussian_process.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\gaussian_process.rst.txt",
    "id": "gaussian_process.rst.txt_chunk_12",
    "header": "Mat\u00e9rn kernel",
    "text": "Mat\u00e9rn kernel\n-------------\nThe :class:`Matern` kernel is a stationary kernel and a generalization of the\n:class:`RBF` kernel. It has an additional parameter :math:`\\nu` which controls\nthe smoothness of the resulting function. It is parameterized by a length-scale parameter :math:`l>0`, which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs :math:`x` (anisotropic variant of the kernel).\n\n.. dropdown:: Mathematical implementation of Mat\u00e9rn kernel\n\n   The kernel is given by:\n\n   .. math::\n\n      k(x_i, x_j) = \\frac{1}{\\Gamma(\\nu)2^{\\nu-1}}\\Bigg(\\frac{\\sqrt{2\\nu}}{l} d(x_i , x_j )\\Bigg)^\\nu K_\\nu\\Bigg(\\frac{\\sqrt{2\\nu}}{l} d(x_i , x_j )\\Bigg),\n\n   where :math:`d(\\cdot,\\cdot)` is the Euclidean distance, :math:`K_\\nu(\\cdot)` is a modified Bessel function and :math:`\\Gamma(\\cdot)` is the gamma function.\n   As :math:`\\nu\\rightarrow\\infty`, the Mat\u00e9rn kernel converges to the RBF kernel.\n   When :math:`\\nu = 1/2`, the Mat\u00e9rn kernel becomes identical to the absolute\n   exponential kernel, i.e.,\n\n   .. math::\n      k(x_i, x_j) = \\exp \\Bigg(- \\frac{1}{l} d(x_i , x_j ) \\Bigg) \\quad \\quad \\nu= \\tfrac{1}{2}\n\n   In particular, :math:`\\nu = 3/2`:\n\n   .. math::\n      k(x_i, x_j) =  \\Bigg(1 + \\frac{\\sqrt{3}}{l} d(x_i , x_j )\\Bigg) \\exp \\Bigg(-\\frac{\\sqrt{3}}{l} d(x_i , x_j ) \\Bigg) \\quad \\quad \\nu= \\tfrac{3}{2}\n\n   and :math:`\\nu = 5/2`:\n\n   .. math::\n      k(x_i, x_j) = \\Bigg(1 + \\frac{\\sqrt{5}}{l} d(x_i , x_j ) +\\frac{5}{3l} d(x_i , x_j )^2 \\Bigg) \\exp \\Bigg(-\\frac{\\sqrt{5}}{l} d(x_i , x_j ) \\Bigg) \\quad \\quad \\nu= \\tfrac{5}{2}\n\n   are popular choices for learning functions that are not infinitely\n   differentiable (as assumed by the RBF kernel) but at least once (:math:`\\nu =\n   3/2`) or twice differentiable (:math:`\\nu = 5/2`).\n\n   The flexibility of controlling the smoothness of the learned function via :math:`\\nu`\n   allows adapting to the properties of the true underlying functional relation.\n\nThe prior and posterior of a GP resulting from a Mat\u00e9rn kernel are shown in\nthe following figure:\n\n.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_prior_posterior_005.png\n   :target: ../auto_examples/gaussian_process/plot_gpr_prior_posterior.html\n   :align: center\n\nSee [RW2006]_, pp84 for further details regarding the\ndifferent variants of the Mat\u00e9rn kernel."
  },
  {
    "filename": "gaussian_process.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\gaussian_process.rst.txt",
    "id": "gaussian_process.rst.txt_chunk_13",
    "header": "Rational quadratic kernel",
    "text": "Rational quadratic kernel\n-------------------------\n\nThe :class:`RationalQuadratic` kernel can be seen as a scale mixture (an infinite sum)\nof :class:`RBF` kernels with different characteristic length-scales. It is parameterized\nby a length-scale parameter :math:`l>0` and a scale mixture parameter  :math:`\\alpha>0`\nOnly the isotropic variant where :math:`l` is a scalar is supported at the moment.\nThe kernel is given by:\n\n.. math::\n   k(x_i, x_j) = \\left(1 + \\frac{d(x_i, x_j)^2}{2\\alpha l^2}\\right)^{-\\alpha}\n\nThe prior and posterior of a GP resulting from a :class:`RationalQuadratic` kernel are shown in\nthe following figure:\n\n.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_prior_posterior_002.png\n   :target: ../auto_examples/gaussian_process/plot_gpr_prior_posterior.html\n   :align: center"
  },
  {
    "filename": "gaussian_process.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\gaussian_process.rst.txt",
    "id": "gaussian_process.rst.txt_chunk_14",
    "header": "Exp-Sine-Squared kernel",
    "text": "Exp-Sine-Squared kernel\n-----------------------\n\nThe :class:`ExpSineSquared` kernel allows modeling periodic functions.\nIt is parameterized by a length-scale parameter :math:`l>0` and a periodicity parameter\n:math:`p>0`. Only the isotropic variant where :math:`l` is a scalar is supported at the moment.\nThe kernel is given by:\n\n.. math::\n   k(x_i, x_j) = \\text{exp}\\left(- \\frac{ 2\\sin^2(\\pi d(x_i, x_j) / p) }{ l^ 2} \\right)\n\nThe prior and posterior of a GP resulting from an ExpSineSquared kernel are shown in\nthe following figure:\n\n.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_prior_posterior_003.png\n   :target: ../auto_examples/gaussian_process/plot_gpr_prior_posterior.html\n   :align: center"
  },
  {
    "filename": "gaussian_process.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\gaussian_process.rst.txt",
    "id": "gaussian_process.rst.txt_chunk_15",
    "header": "Dot-Product kernel",
    "text": "Dot-Product kernel\n------------------\n\nThe :class:`DotProduct` kernel is non-stationary and can be obtained from linear regression\nby putting :math:`N(0, 1)` priors on the coefficients of :math:`x_d (d = 1, . . . , D)` and\na prior of :math:`N(0, \\sigma_0^2)` on the bias. The :class:`DotProduct` kernel is invariant to a rotation\nof the coordinates about the origin, but not translations.\nIt is parameterized by a parameter :math:`\\sigma_0^2`. For :math:`\\sigma_0^2 = 0`, the kernel\nis called the homogeneous linear kernel, otherwise it is inhomogeneous. The kernel is given by\n\n.. math::\n   k(x_i, x_j) = \\sigma_0 ^ 2 + x_i \\cdot x_j\n\nThe :class:`DotProduct` kernel is commonly combined with exponentiation. An example with exponent 2 is\nshown in the following figure:\n\n.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_prior_posterior_004.png\n   :target: ../auto_examples/gaussian_process/plot_gpr_prior_posterior.html\n   :align: center"
  },
  {
    "filename": "gaussian_process.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\gaussian_process.rst.txt",
    "id": "gaussian_process.rst.txt_chunk_16",
    "header": "References",
    "text": "References\n----------\n\n.. [RW2006] `Carl E. Rasmussen and Christopher K.I. Williams,\n   \"Gaussian Processes for Machine Learning\",\n   MIT Press 2006 <https://www.gaussianprocess.org/gpml/chapters/RW.pdf>`_\n\n.. [Duv2014] `David Duvenaud, \"The Kernel Cookbook: Advice on Covariance functions\", 2014\n   <https://www.cs.toronto.edu/~duvenaud/cookbook/>`_\n\n.. currentmodule:: sklearn.gaussian_process"
  },
  {
    "filename": "grid_search.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\grid_search.rst.txt",
    "id": "grid_search.rst.txt_chunk_0",
    "header": "",
    "text": ".. currentmodule:: sklearn.model_selection\n\n.. _grid_search:\n\n==========================================="
  },
  {
    "filename": "grid_search.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\grid_search.rst.txt",
    "id": "grid_search.rst.txt_chunk_1",
    "header": "Tuning the hyper-parameters of an estimator",
    "text": "Tuning the hyper-parameters of an estimator\n===========================================\n\nHyper-parameters are parameters that are not directly learnt within estimators.\nIn scikit-learn they are passed as arguments to the constructor of the\nestimator classes. Typical examples include ``C``, ``kernel`` and ``gamma``\nfor Support Vector Classifier, ``alpha`` for Lasso, etc.\n\nIt is possible and recommended to search the hyper-parameter space for the\nbest :ref:`cross validation <cross_validation>` score.\n\nAny parameter provided when constructing an estimator may be optimized in this\nmanner. Specifically, to find the names and current values for all parameters\nfor a given estimator, use::\n\n  estimator.get_params()\n\nA search consists of:\n\n- an estimator (regressor or classifier such as ``sklearn.svm.SVC()``);\n- a parameter space;\n- a method for searching or sampling candidates;\n- a cross-validation scheme; and\n- a :ref:`score function <gridsearch_scoring>`.\n\nTwo generic approaches to parameter search are provided in\nscikit-learn: for given values, :class:`GridSearchCV` exhaustively considers\nall parameter combinations, while :class:`RandomizedSearchCV` can sample a\ngiven number of candidates from a parameter space with a specified\ndistribution. Both these tools have successive halving counterparts\n:class:`HalvingGridSearchCV` and :class:`HalvingRandomSearchCV`, which can be\nmuch faster at finding a good parameter combination.\n\nAfter describing these tools we detail :ref:`best practices\n<grid_search_tips>` applicable to these approaches. Some models allow for\nspecialized, efficient parameter search strategies, outlined in\n:ref:`alternative_cv`.\n\nNote that it is common that a small subset of those parameters can have a large\nimpact on the predictive or computation performance of the model while others\ncan be left to their default values. It is recommended to read the docstring of\nthe estimator class to get a finer understanding of their expected behavior,\npossibly by reading the enclosed reference to the literature."
  },
  {
    "filename": "grid_search.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\grid_search.rst.txt",
    "id": "grid_search.rst.txt_chunk_2",
    "header": "Exhaustive Grid Search",
    "text": "Exhaustive Grid Search\n======================\n\nThe grid search provided by :class:`GridSearchCV` exhaustively generates\ncandidates from a grid of parameter values specified with the ``param_grid``\nparameter. For instance, the following ``param_grid``::\n\n  param_grid = [\n    {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n    {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n   ]\n\nspecifies that two grids should be explored: one with a linear kernel and\nC values in [1, 10, 100, 1000], and the second one with an RBF kernel,\nand the cross-product of C values ranging in [1, 10, 100, 1000] and gamma\nvalues in [0.001, 0.0001].\n\nThe :class:`GridSearchCV` instance implements the usual estimator API: when\n\"fitting\" it on a dataset all the possible combinations of parameter values are\nevaluated and the best combination is retained.\n\n.. currentmodule:: sklearn.model_selection\n\n.. rubric:: Examples\n\n- See :ref:`sphx_glr_auto_examples_model_selection_plot_nested_cross_validation_iris.py`\n  for an example of Grid Search within a cross validation loop on the iris\n  dataset. This is the best practice for evaluating the performance of a\n  model with grid search.\n\n- See :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_text_feature_extraction.py` for an example\n  of Grid Search coupling parameters from a text documents feature\n  extractor (n-gram count vectorizer and TF-IDF transformer) with a\n  classifier (here a linear SVM trained with SGD with either elastic\n  net or L2 penalty) using a :class:`~sklearn.pipeline.Pipeline` instance.\n\n\n.. dropdown:: Advanced examples\n\n  - See :ref:`sphx_glr_auto_examples_model_selection_plot_nested_cross_validation_iris.py`\n    for an example of Grid Search within a cross validation loop on the iris\n    dataset. This is the best practice for evaluating the performance of a\n    model with grid search.\n\n  - See :ref:`sphx_glr_auto_examples_model_selection_plot_multi_metric_evaluation.py`\n    for an example of :class:`GridSearchCV` being used to evaluate multiple\n    metrics simultaneously.\n\n  - See :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_refit_callable.py`\n    for an example of using ``refit=callable`` interface in\n    :class:`GridSearchCV`. The example shows how this interface adds a certain\n    amount of flexibility in identifying the \"best\" estimator. This interface\n    can also be used in multiple metrics evaluation.\n\n  - See :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_stats.py`\n    for an example of how to do a statistical comparison on the outputs of\n    :class:`GridSearchCV`.\n\n\n.. _randomized_parameter_search:"
  },
  {
    "filename": "grid_search.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\grid_search.rst.txt",
    "id": "grid_search.rst.txt_chunk_3",
    "header": "Randomized Parameter Optimization",
    "text": "Randomized Parameter Optimization\n=================================\nWhile using a grid of parameter settings is currently the most widely used\nmethod for parameter optimization, other search methods have more\nfavorable properties.\n:class:`RandomizedSearchCV` implements a randomized search over parameters,\nwhere each setting is sampled from a distribution over possible parameter values.\nThis has two main benefits over an exhaustive search:\n\n* A budget can be chosen independent of the number of parameters and possible values.\n* Adding parameters that do not influence the performance does not decrease efficiency.\n\nSpecifying how parameters should be sampled is done using a dictionary, very\nsimilar to specifying parameters for :class:`GridSearchCV`. Additionally,\na computation budget, being the number of sampled candidates or sampling\niterations, is specified using the ``n_iter`` parameter.\nFor each parameter, either a distribution over possible values or a list of\ndiscrete choices (which will be sampled uniformly) can be specified::\n\n  {'C': scipy.stats.expon(scale=100), 'gamma': scipy.stats.expon(scale=.1),\n    'kernel': ['rbf'], 'class_weight':['balanced', None]}\n\nThis example uses the ``scipy.stats`` module, which contains many useful\ndistributions for sampling parameters, such as ``expon``, ``gamma``,\n``uniform``, ``loguniform`` or ``randint``.\n\nIn principle, any function can be passed that provides a ``rvs`` (random\nvariate sample) method to sample a value. A call to the ``rvs`` function should\nprovide independent random samples from possible parameter values on\nconsecutive calls.\n\n.. warning::\n\n    The distributions in ``scipy.stats`` prior to version scipy 0.16\n    do not allow specifying a random state. Instead, they use the global\n    numpy random state, that can be seeded via ``np.random.seed`` or set\n    using ``np.random.set_state``. However, beginning scikit-learn 0.18,\n    the :mod:`sklearn.model_selection` module sets the random state provided\n    by the user if scipy >= 0.16 is also available.\n\nFor continuous parameters, such as ``C`` above, it is important to specify\na continuous distribution to take full advantage of the randomization. This way,\nincreasing ``n_iter`` will always lead to a finer search.\n\nA continuous log-uniform random variable is the continuous version of\na log-spaced parameter. For example to specify the equivalent of ``C`` from above,\n``loguniform(1, 100)`` can be used instead of ``[1, 10, 100]``.\n\nMirroring the example above in grid search, we can specify a continuous random\nvariable that is log-uniformly distributed between ``1e0`` and ``1e3``::\n\n  from sklearn.utils.fixes import loguniform\n  {'C': loguniform(1e0, 1e3),\n   'gamma': loguniform(1e-4, 1e-3),\n   'kernel': ['rbf'],\n   'class_weight':['balanced', None]}\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_model_selection_plot_randomized_search.py` compares the usage and efficiency\n  of randomized search and grid search.\n\n.. rubric:: References\n\n* Bergstra, J. and Bengio, Y.,\n  Random search for hyper-parameter optimization,\n  The Journal of Machine Learning Research (2012)\n\n.. _successive_halving_user_guide:"
  },
  {
    "filename": "grid_search.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\grid_search.rst.txt",
    "id": "grid_search.rst.txt_chunk_4",
    "header": "Searching for optimal parameters with successive halving",
    "text": "Searching for optimal parameters with successive halving\n========================================================\n\nScikit-learn also provides the :class:`HalvingGridSearchCV` and\n:class:`HalvingRandomSearchCV` estimators that can be used to\nsearch a parameter space using successive halving [1]_ [2]_. Successive\nhalving (SH) is like a tournament among candidate parameter combinations.\nSH is an iterative selection process where all candidates (the\nparameter combinations) are evaluated with a small amount of resources at\nthe first iteration. Only some of these candidates are selected for the next\niteration, which will be allocated more resources. For parameter tuning, the\nresource is typically the number of training samples, but it can also be an\narbitrary numeric parameter such as `n_estimators` in a random forest.\n\n.. note::\n\n    The resource increase chosen should be large enough so that a large improvement\n    in scores is obtained when taking into account statistical significance.\n\nAs illustrated in the figure below, only a subset of candidates\n'survive' until the last iteration. These are the candidates that have\nconsistently ranked among the top-scoring candidates across all iterations.\nEach iteration is allocated an increasing amount of resources per candidate,\nhere the number of samples.\n\n.. figure:: ../auto_examples/model_selection/images/sphx_glr_plot_successive_halving_iterations_001.png\n   :target: ../auto_examples/model_selection/plot_successive_halving_iterations.html\n   :align: center\n\nWe here briefly describe the main parameters, but each parameter and their\ninteractions are described more in detail in the dropdown section below. The\n``factor`` (> 1) parameter controls the rate at which the resources grow, and\nthe rate at which the number of candidates decreases. In each iteration, the\nnumber of resources per candidate is multiplied by ``factor`` and the number\nof candidates is divided by the same factor. Along with ``resource`` and\n``min_resources``, ``factor`` is the most important parameter to control the\nsearch in our implementation, though a value of 3 usually works well.\n``factor`` effectively controls the number of iterations in\n:class:`HalvingGridSearchCV` and the number of candidates (by default) and\niterations in :class:`HalvingRandomSearchCV`. ``aggressive_elimination=True``\ncan also be used if the number of available resources is small. More control\nis available through tuning the ``min_resources`` parameter.\n\nThese estimators are still **experimental**: their predictions\nand their API might change without any deprecation cycle. To use them, you\nneed to explicitly import ``enable_halving_search_cv``::\n\n  >>> from sklearn.experimental import enable_halving_search_cv  # noqa\n  >>> from sklearn.model_selection import HalvingGridSearchCV\n  >>> from sklearn.model_selection import HalvingRandomSearchCV\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_model_selection_plot_successive_halving_heatmap.py`\n* :ref:`sphx_glr_auto_examples_model_selection_plot_successive_halving_iterations.py`\n\nThe sections below dive into technical aspects of successive halving.\n\n.. dropdown:: Choosing ``min_resources`` and the number of candidates\n\n  Beside ``factor``, the two main parameters that influence the behaviour of a\n  successive halving search are the ``min_resources`` parameter, and the\n  number of candidates (or parameter combinations) that are evaluated.\n  ``min_resources`` is the amount of resources allocated at the first\n  iteration for each candidate. The number of candidates is specified directly\n  in :class:`HalvingRandomSearchCV`, and is determined from the ``param_grid``\n  parameter of :class:`HalvingGridSearchCV`.\n\n  Consider a case where the resource is the number of samples, and where we\n  have 1000 samples. In theory, with ``min_resources=10`` and ``factor=2``, we\n  are able to run **at most** 7 iterations with the following number of\n  samples: ``[10, 20, 40, 80, 160, 320, 640]``.\n\n  But depending on the number of candidates, we might run less than 7\n  iterations: if we start with a **small** number of candidates, the last\n  iteration might use less than 640 samples, which means not using all the\n  available resources (samples). For example if we start with 5 candidates, we\n  only need 2 iterations: 5 candidates for the first iteration, then\n  `5 // 2 = 2` candidates at the second iteration, after which we know which\n  candidate performs the best (so we don't need a third one). We would only be\n  using at most 20 samples which is a waste since we have 1000 samples at our\n  disposal. On the other hand, if we start with a **high** number of\n  candidates, we might end up with a lot of candidates at the last iteration,\n  which may not always be ideal: it means that many candidates will run with\n  the full resources, basically reducing the procedure to standard search.\n\n  In the case of :class:`HalvingRandomSearchCV`, the number of candidates is set\n  by default such that the last iteration uses as much of the available\n  resources as possible. For :class:`HalvingGridSearchCV`, the number of\n  candidates is determined by the `param_grid` parameter. Changing the value of\n  ``min_resources`` will impact the number of possible iterations, and as a\n  result will also have an effect on the ideal number of candidates.\n\n  Another consideration when choosing ``min_resources`` is whether or not it\n  is easy to discriminate between good and bad candidates with a small amount\n  of resources. For example, if you need a lot of samples to distinguish\n  between good and bad parameters, a high ``min_resources`` is recommended. On\n  the other hand if the distinction is clear even with a small amount of\n  samples, then a small ``min_resources`` may be preferable since it would\n  speed up the computation.\n\n  Notice in the example above that the last iteration does not use the maximum\n  amount of resources available: 1000 samples are available, yet only 640 are\n  used, at most. By default, both :class:`HalvingRandomSearchCV` and\n  :class:`HalvingGridSearchCV` try to use as many resources as possible in the\n  last iteration, with the constraint that this amount of resources must be a\n  multiple of both `min_resources` and `factor` (this constraint will be clear\n  in the next section). :class:`HalvingRandomSearchCV` achieves this by\n  sampling the right amount of candidates, while :class:`HalvingGridSearchCV`\n  achieves this by properly setting `min_resources`.\n\n\n.. dropdown:: Amount of resource and number of candidates at each iteration\n\n  At any iteration `i`, each candidate is allocated a given amount of resources\n  which we denote `n_resources_i`. This quantity is controlled by the\n  parameters ``factor`` and ``min_resources`` as follows (`factor` is strictly\n  greater than 1)::\n\n      n_resources_i = factor**i * min_resources,\n\n  or equivalently::\n\n      n_resources_{i+1} = n_resources_i * factor\n\n  where ``min_resources == n_resources_0`` is the amount of resources used at\n  the first iteration. ``factor`` also defines the proportions of candidates\n  that will be selected for the next iteration::\n\n      n_candidates_i = n_candidates // (factor ** i)\n\n  or equivalently::\n\n      n_candidates_0 = n_candidates\n      n_candidates_{i+1} = n_candidates_i // factor\n\n  So in the first iteration, we use ``min_resources`` resources\n  ``n_candidates`` times. In the second iteration, we use ``min_resources *\n  factor`` resources ``n_candidates // factor`` times. The third again\n  multiplies the resources per candidate and divides the number of candidates.\n  This process stops when the maximum amount of resource per candidate is\n  reached, or when we have identified the best candidate. The best candidate\n  is identified at the iteration that is evaluating `factor` or less candidates\n  (see just below for an explanation).\n\n  Here is an example with ``min_resources=3`` and ``factor=2``, starting with\n  70 candidates:\n\n  +-----------------------+-----------------------+\n  | ``n_resources_i``     | ``n_candidates_i``    |\n  +=======================+=======================+\n  | 3 (=min_resources)    | 70 (=n_candidates)    |\n  +-----------------------+-----------------------+\n  | 3 * 2 = 6             | 70 // 2 = 35          |\n  +-----------------------+-----------------------+\n  | 6 * 2 = 12            | 35 // 2 = 17          |\n  +-----------------------+-----------------------+\n  | 12 * 2 = 24           | 17 // 2 = 8           |\n  +-----------------------+-----------------------+\n  | 24 * 2 = 48           | 8 // 2 = 4            |\n  +-----------------------+-----------------------+\n  | 48 * 2 = 96           | 4 // 2 = 2            |\n  +-----------------------+-----------------------+\n\n  We can note that:\n\n  - the process stops at the first iteration which evaluates `factor=2`\n    candidates: the best candidate is the best out of these 2 candidates. It\n    is not necessary to run an additional iteration, since it would only\n    evaluate one candidate (namely the best one, which we have already\n    identified). For this reason, in general, we want the last iteration to\n    run at most ``factor`` candidates. If the last iteration evaluates more\n    than `factor` candidates, then this last iteration reduces to a regular\n    search (as in :class:`RandomizedSearchCV` or :class:`GridSearchCV`).\n  - each ``n_resources_i`` is a multiple of both ``factor`` and\n    ``min_resources`` (which is confirmed by its definition above).\n\n  The amount of resources that is used at each iteration can be found in the\n  `n_resources_` attribute.\n\n.. dropdown:: Choosing a resource\n\n  By default, the resource is defined in terms of number of samples. That is,\n  each iteration will use an increasing amount of samples to train on. You can\n  however manually specify a parameter to use as the resource with the\n  ``resource`` parameter. Here is an example where the resource is defined in\n  terms of the number of estimators of a random forest::\n\n      >>> from sklearn.datasets import make_classification\n      >>> from sklearn.ensemble import RandomForestClassifier\n      >>> from sklearn.experimental import enable_halving_search_cv  # noqa\n      >>> from sklearn.model_selection import HalvingGridSearchCV\n      >>> import pandas as pd\n      >>> param_grid = {'max_depth': [3, 5, 10],\n      ...               'min_samples_split': [2, 5, 10]}\n      >>> base_estimator = RandomForestClassifier(random_state=0)\n      >>> X, y = make_classification(n_samples=1000, random_state=0)\n      >>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,\n      ...                          factor=2, resource='n_estimators',\n      ...                          max_resources=30).fit(X, y)\n      >>> sh.best_estimator_\n      RandomForestClassifier(max_depth=5, n_estimators=24, random_state=0)\n\n  Note that it is not possible to budget on a parameter that is part of the\n  parameter grid.\n\n\n.. dropdown:: Exhausting the available resources\n\n  As mentioned above, the number of resources that is used at each iteration\n  depends on the `min_resources` parameter.\n  If you have a lot of resources available but start with a low number of\n  resources, some of them might be wasted (i.e. not used)::\n\n      >>> from sklearn.datasets import make_classification\n      >>> from sklearn.svm import SVC\n      >>> from sklearn.experimental import enable_halving_search_cv  # noqa\n      >>> from sklearn.model_selection import HalvingGridSearchCV\n      >>> import pandas as pd\n      >>> param_grid= {'kernel': ('linear', 'rbf'),\n      ...              'C': [1, 10, 100]}\n      >>> base_estimator = SVC(gamma='scale')\n      >>> X, y = make_classification(n_samples=1000)\n      >>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,\n      ...                          factor=2, min_resources=20).fit(X, y)\n      >>> sh.n_resources_\n      [20, 40, 80]\n\n  The search process will only use 80 resources at most, while our maximum\n  amount of available resources is ``n_samples=1000``. Here, we have\n  ``min_resources = r_0 = 20``.\n\n  For :class:`HalvingGridSearchCV`, by default, the `min_resources` parameter\n  is set to 'exhaust'. This means that `min_resources` is automatically set\n  such that the last iteration can use as many resources as possible, within\n  the `max_resources` limit::\n\n      >>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,\n      ...                          factor=2, min_resources='exhaust').fit(X, y)\n      >>> sh.n_resources_\n      [250, 500, 1000]\n\n  `min_resources` was here automatically set to 250, which results in the last\n  iteration using all the resources. The exact value that is used depends on\n  the number of candidate parameters, on `max_resources` and on `factor`.\n\n  For :class:`HalvingRandomSearchCV`, exhausting the resources can be done in 2\n  ways:\n\n  - by setting `min_resources='exhaust'`, just like for\n    :class:`HalvingGridSearchCV`;\n  - by setting `n_candidates='exhaust'`.\n\n  Both options are mutually exclusive: using `min_resources='exhaust'` requires\n  knowing the number of candidates, and symmetrically `n_candidates='exhaust'`\n  requires knowing `min_resources`.\n\n  In general, exhausting the total number of resources leads to a better final\n  candidate parameter, and is slightly more time-intensive.\n\n.. _aggressive_elimination:"
  },
  {
    "filename": "grid_search.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\grid_search.rst.txt",
    "id": "grid_search.rst.txt_chunk_5",
    "header": "Aggressive elimination of candidates",
    "text": "Aggressive elimination of candidates\n------------------------------------\n\nUsing the ``aggressive_elimination`` parameter, you can force the search\nprocess to end up with less than ``factor`` candidates at the last\niteration.\n\n.. dropdown:: Code example of aggressive elimination\n\n  Ideally, we want the last iteration to evaluate ``factor`` candidates. We\n  then just have to pick the best one. When the number of available resources is\n  small with respect to the number of candidates, the last iteration may have to\n  evaluate more than ``factor`` candidates::\n\n      >>> from sklearn.datasets import make_classification\n      >>> from sklearn.svm import SVC\n      >>> from sklearn.experimental import enable_halving_search_cv  # noqa\n      >>> from sklearn.model_selection import HalvingGridSearchCV\n      >>> import pandas as pd\n      >>> param_grid = {'kernel': ('linear', 'rbf'),\n      ...               'C': [1, 10, 100]}\n      >>> base_estimator = SVC(gamma='scale')\n      >>> X, y = make_classification(n_samples=1000)\n      >>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,\n      ...                          factor=2, max_resources=40,\n      ...                          aggressive_elimination=False).fit(X, y)\n      >>> sh.n_resources_\n      [20, 40]\n      >>> sh.n_candidates_\n      [6, 3]\n\n  Since we cannot use more than ``max_resources=40`` resources, the process\n  has to stop at the second iteration which evaluates more than ``factor=2``\n  candidates.\n\n  When using ``aggressive_elimination``, the process will eliminate as many\n  candidates as necessary using ``min_resources`` resources::\n\n      >>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,\n      ...                            factor=2,\n      ...                            max_resources=40,\n      ...                            aggressive_elimination=True,\n      ...                            ).fit(X, y)\n      >>> sh.n_resources_\n      [20, 20, 40]\n      >>> sh.n_candidates_\n      [6, 3, 2]\n\n  Notice that we end with 2 candidates at the last iteration since we have\n  eliminated enough candidates during the first iterations, using ``n_resources =\n  min_resources = 20``.\n\n.. _successive_halving_cv_results:"
  },
  {
    "filename": "grid_search.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\grid_search.rst.txt",
    "id": "grid_search.rst.txt_chunk_6",
    "header": "Analyzing results with the `cv_results_` attribute",
    "text": "Analyzing results with the `cv_results_` attribute\n--------------------------------------------------\n\nThe ``cv_results_`` attribute contains useful information for analyzing the\nresults of a search. It can be converted to a pandas dataframe with ``df =\npd.DataFrame(est.cv_results_)``. The ``cv_results_`` attribute of\n:class:`HalvingGridSearchCV` and :class:`HalvingRandomSearchCV` is similar\nto that of :class:`GridSearchCV` and :class:`RandomizedSearchCV`, with\nadditional information related to the successive halving process.\n\n.. dropdown:: Example of a (truncated) output dataframe:\n\n  ====  ======  ===============  =================  ========================================================================================\n    ..    iter      n_resources    mean_test_score  params\n  ====  ======  ===============  =================  ========================================================================================\n     0       0              125           0.983667  {'criterion': 'log_loss', 'max_depth': None, 'max_features': 9, 'min_samples_split': 5}\n     1       0              125           0.983667  {'criterion': 'gini', 'max_depth': None, 'max_features': 8, 'min_samples_split': 7}\n     2       0              125           0.983667  {'criterion': 'gini', 'max_depth': None, 'max_features': 10, 'min_samples_split': 10}\n     3       0              125           0.983667  {'criterion': 'log_loss', 'max_depth': None, 'max_features': 6, 'min_samples_split': 6}\n   ...     ...              ...                ...  ...\n    15       2              500           0.951958  {'criterion': 'log_loss', 'max_depth': None, 'max_features': 9, 'min_samples_split': 10}\n    16       2              500           0.947958  {'criterion': 'gini', 'max_depth': None, 'max_features': 10, 'min_samples_split': 10}\n    17       2              500           0.951958  {'criterion': 'gini', 'max_depth': None, 'max_features': 10, 'min_samples_split': 4}\n    18       3             1000           0.961009  {'criterion': 'log_loss', 'max_depth': None, 'max_features': 9, 'min_samples_split': 10}\n    19       3             1000           0.955989  {'criterion': 'gini', 'max_depth': None, 'max_features': 10, 'min_samples_split': 4}\n  ====  ======  ===============  =================  ========================================================================================\n\n  Each row corresponds to a given parameter combination (a candidate) and a given\n  iteration. The iteration is given by the ``iter`` column. The ``n_resources``\n  column tells you how many resources were used.\n\n  In the example above, the best parameter combination is ``{'criterion':\n  'log_loss', 'max_depth': None, 'max_features': 9, 'min_samples_split': 10}``\n  since it has reached the last iteration (3) with the highest score:\n  0.96.\n\n  .. rubric:: References\n\n  .. [1] K. Jamieson, A. Talwalkar,\n     `Non-stochastic Best Arm Identification and Hyperparameter\n     Optimization <http://proceedings.mlr.press/v51/jamieson16.html>`_, in\n     proc. of Machine Learning Research, 2016.\n\n  .. [2] L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, A. Talwalkar,\n     :arxiv:`Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization\n     <1603.06560>`, in Machine Learning Research 18, 2018.\n\n\n\n.. _grid_search_tips:"
  },
  {
    "filename": "grid_search.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\grid_search.rst.txt",
    "id": "grid_search.rst.txt_chunk_7",
    "header": "Tips for parameter search",
    "text": "Tips for parameter search\n=========================\n\n.. _gridsearch_scoring:"
  },
  {
    "filename": "grid_search.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\grid_search.rst.txt",
    "id": "grid_search.rst.txt_chunk_8",
    "header": "Specifying an objective metric",
    "text": "Specifying an objective metric\n------------------------------\n\nBy default, parameter search uses the ``score`` function of the estimator to\nevaluate a parameter setting. These are the\n:func:`sklearn.metrics.accuracy_score` for classification and\n:func:`sklearn.metrics.r2_score` for regression.  For some applications, other\nscoring functions are better suited (for example in unbalanced classification,\nthe accuracy score is often uninformative), see :ref:`which_scoring_function`\nfor some guidance. An alternative scoring function can be specified via the\n``scoring`` parameter of most parameter search tools, see\n:ref:`scoring_parameter` for more details.\n\n.. _multimetric_grid_search:"
  },
  {
    "filename": "grid_search.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\grid_search.rst.txt",
    "id": "grid_search.rst.txt_chunk_9",
    "header": "Specifying multiple metrics for evaluation",
    "text": "Specifying multiple metrics for evaluation\n------------------------------------------\n\n:class:`GridSearchCV` and :class:`RandomizedSearchCV` allow specifying\nmultiple metrics for the ``scoring`` parameter.\n\nMultimetric scoring can either be specified as a list of strings of predefined\nscores names or a dict mapping the scorer name to the scorer function and/or\nthe predefined scorer name(s). See :ref:`multimetric_scoring` for more details.\n\nWhen specifying multiple metrics, the ``refit`` parameter must be set to the\nmetric (string) for which the ``best_params_`` will be found and used to build\nthe ``best_estimator_`` on the whole dataset. If the search should not be\nrefit, set ``refit=False``. Leaving refit to the default value ``None`` will\nresult in an error when using multiple metrics.\n\nSee :ref:`sphx_glr_auto_examples_model_selection_plot_multi_metric_evaluation.py`\nfor an example usage.\n\n:class:`HalvingRandomSearchCV` and :class:`HalvingGridSearchCV` do not support\nmultimetric scoring.\n\n.. _composite_grid_search:"
  },
  {
    "filename": "grid_search.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\grid_search.rst.txt",
    "id": "grid_search.rst.txt_chunk_10",
    "header": "Composite estimators and parameter spaces",
    "text": "Composite estimators and parameter spaces\n-----------------------------------------\n:class:`GridSearchCV` and :class:`RandomizedSearchCV` allow searching over\nparameters of composite or nested estimators such as\n:class:`~sklearn.pipeline.Pipeline`,\n:class:`~sklearn.compose.ColumnTransformer`,\n:class:`~sklearn.ensemble.VotingClassifier` or\n:class:`~sklearn.calibration.CalibratedClassifierCV` using a dedicated\n``<estimator>__<parameter>`` syntax::\n\n  >>> from sklearn.model_selection import GridSearchCV\n  >>> from sklearn.calibration import CalibratedClassifierCV\n  >>> from sklearn.ensemble import RandomForestClassifier\n  >>> from sklearn.datasets import make_moons\n  >>> X, y = make_moons()\n  >>> calibrated_forest = CalibratedClassifierCV(\n  ...    estimator=RandomForestClassifier(n_estimators=10))\n  >>> param_grid = {\n  ...    'estimator__max_depth': [2, 4, 6, 8]}\n  >>> search = GridSearchCV(calibrated_forest, param_grid, cv=5)\n  >>> search.fit(X, y)\n  GridSearchCV(cv=5,\n               estimator=CalibratedClassifierCV(estimator=RandomForestClassifier(n_estimators=10)),\n               param_grid={'estimator__max_depth': [2, 4, 6, 8]})\n\nHere, ``<estimator>`` is the parameter name of the nested estimator,\nin this case ``estimator``.\nIf the meta-estimator is constructed as a collection of estimators as in\n`pipeline.Pipeline`, then ``<estimator>`` refers to the name of the estimator,\nsee :ref:`pipeline_nested_parameters`. In practice, there can be several\nlevels of nesting::\n\n  >>> from sklearn.pipeline import Pipeline\n  >>> from sklearn.feature_selection import SelectKBest\n  >>> pipe = Pipeline([\n  ...    ('select', SelectKBest()),\n  ...    ('model', calibrated_forest)])\n  >>> param_grid = {\n  ...    'select__k': [1, 2],\n  ...    'model__estimator__max_depth': [2, 4, 6, 8]}\n  >>> search = GridSearchCV(pipe, param_grid, cv=5).fit(X, y)\n\nPlease refer to :ref:`pipeline` for performing parameter searches over\npipelines."
  },
  {
    "filename": "grid_search.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\grid_search.rst.txt",
    "id": "grid_search.rst.txt_chunk_11",
    "header": "Model selection: development and evaluation",
    "text": "Model selection: development and evaluation\n-------------------------------------------\n\nModel selection by evaluating various parameter settings can be seen as a way\nto use the labeled data to \"train\" the parameters of the grid.\n\nWhen evaluating the resulting model it is important to do it on\nheld-out samples that were not seen during the grid search process:\nit is recommended to split the data into a **development set** (to\nbe fed to the :class:`GridSearchCV` instance) and an **evaluation set**\nto compute performance metrics.\n\nThis can be done by using the :func:`train_test_split`\nutility function."
  },
  {
    "filename": "grid_search.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\grid_search.rst.txt",
    "id": "grid_search.rst.txt_chunk_12",
    "header": "Parallelism",
    "text": "Parallelism\n-----------\n\nThe parameter search tools evaluate each parameter combination on each data\nfold independently. Computations can be run in parallel by using the keyword\n``n_jobs=-1``. See function signature for more details, and also the Glossary\nentry for :term:`n_jobs`."
  },
  {
    "filename": "grid_search.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\grid_search.rst.txt",
    "id": "grid_search.rst.txt_chunk_13",
    "header": "Robustness to failure",
    "text": "Robustness to failure\n---------------------\n\nSome parameter settings may result in a failure to ``fit`` one or more folds of\nthe data. By default, the score for those settings will be `np.nan`. This can\nbe controlled by setting `error_score=\"raise\"` to raise an exception if one fit\nfails, or for example `error_score=0` to set another value for the score of\nfailing parameter combinations.\n\n.. _alternative_cv:"
  },
  {
    "filename": "grid_search.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\grid_search.rst.txt",
    "id": "grid_search.rst.txt_chunk_14",
    "header": "Alternatives to brute force parameter search",
    "text": "Alternatives to brute force parameter search\n============================================"
  },
  {
    "filename": "grid_search.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\grid_search.rst.txt",
    "id": "grid_search.rst.txt_chunk_15",
    "header": "Model specific cross-validation",
    "text": "Model specific cross-validation\n-------------------------------\n\n\nSome models can fit data for a range of values of some parameter almost\nas efficiently as fitting the estimator for a single value of the\nparameter. This feature can be leveraged to perform a more efficient\ncross-validation used for model selection of this parameter.\n\nThe most common parameter amenable to this strategy is the parameter\nencoding the strength of the regularizer. In this case we say that we\ncompute the **regularization path** of the estimator.\n\nHere is the list of such models:\n\n.. currentmodule:: sklearn\n\n.. autosummary::\n\n   linear_model.ElasticNetCV\n   linear_model.LarsCV\n   linear_model.LassoCV\n   linear_model.LassoLarsCV\n   linear_model.LogisticRegressionCV\n   linear_model.MultiTaskElasticNetCV\n   linear_model.MultiTaskLassoCV\n   linear_model.OrthogonalMatchingPursuitCV\n   linear_model.RidgeCV\n   linear_model.RidgeClassifierCV"
  },
  {
    "filename": "grid_search.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\grid_search.rst.txt",
    "id": "grid_search.rst.txt_chunk_16",
    "header": "Information Criterion",
    "text": "Information Criterion\n---------------------\n\nSome models can offer an information-theoretic closed-form formula of the\noptimal estimate of the regularization parameter by computing a single\nregularization path (instead of several when using cross-validation).\n\nHere is the list of models benefiting from the Akaike Information\nCriterion (AIC) or the Bayesian Information Criterion (BIC) for automated\nmodel selection:\n\n.. autosummary::\n\n   linear_model.LassoLarsIC\n\n\n.. _out_of_bag:"
  },
  {
    "filename": "grid_search.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\grid_search.rst.txt",
    "id": "grid_search.rst.txt_chunk_17",
    "header": "Out of Bag Estimates",
    "text": "Out of Bag Estimates\n--------------------\n\nWhen using ensemble methods based upon bagging, i.e. generating new\ntraining sets using sampling with replacement, part of the training set\nremains unused.  For each classifier in the ensemble, a different part\nof the training set is left out.\n\nThis left out portion can be used to estimate the generalization error\nwithout having to rely on a separate validation set.  This estimate\ncomes \"for free\" as no additional data is needed and can be used for\nmodel selection.\n\nThis is currently implemented in the following classes:\n\n.. autosummary::\n\n    ensemble.RandomForestClassifier\n    ensemble.RandomForestRegressor\n    ensemble.ExtraTreesClassifier\n    ensemble.ExtraTreesRegressor\n    ensemble.GradientBoostingClassifier\n    ensemble.GradientBoostingRegressor"
  },
  {
    "filename": "impute.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\impute.rst.txt",
    "id": "impute.rst.txt_chunk_0",
    "header": ".. _impute:",
    "text": ".. _impute:\n\n============================"
  },
  {
    "filename": "impute.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\impute.rst.txt",
    "id": "impute.rst.txt_chunk_1",
    "header": "Imputation of missing values",
    "text": "Imputation of missing values\n============================\n\n.. currentmodule:: sklearn.impute\n\nFor various reasons, many real world datasets contain missing values, often\nencoded as blanks, NaNs or other placeholders. Such datasets however are\nincompatible with scikit-learn estimators which assume that all values in an\narray are numerical, and that all have and hold meaning. A basic strategy to\nuse incomplete datasets is to discard entire rows and/or columns containing\nmissing values. However, this comes at the price of losing data which may be\nvaluable (even though incomplete). A better strategy is to impute the missing\nvalues, i.e., to infer them from the known part of the data. See the\nglossary entry on :term:`imputation`."
  },
  {
    "filename": "impute.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\impute.rst.txt",
    "id": "impute.rst.txt_chunk_2",
    "header": "Univariate vs. Multivariate Imputation",
    "text": "Univariate vs. Multivariate Imputation\n======================================\n\nOne type of imputation algorithm is univariate, which imputes values in the\ni-th feature dimension using only non-missing values in that feature dimension\n(e.g. :class:`SimpleImputer`). By contrast, multivariate imputation\nalgorithms use the entire set of available feature dimensions to estimate the\nmissing values (e.g. :class:`IterativeImputer`).\n\n\n.. _single_imputer:"
  },
  {
    "filename": "impute.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\impute.rst.txt",
    "id": "impute.rst.txt_chunk_3",
    "header": "Univariate feature imputation",
    "text": "Univariate feature imputation\n=============================\n\nThe :class:`SimpleImputer` class provides basic strategies for imputing missing\nvalues. Missing values can be imputed with a provided constant value, or using\nthe statistics (mean, median or most frequent) of each column in which the\nmissing values are located. This class also allows for different missing values\nencodings.\n\nThe following snippet demonstrates how to replace missing values,\nencoded as ``np.nan``, using the mean value of the columns (axis 0)\nthat contain the missing values::\n\n    >>> import numpy as np\n    >>> from sklearn.impute import SimpleImputer\n    >>> imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n    >>> imp.fit([[1, 2], [np.nan, 3], [7, 6]])\n    SimpleImputer()\n    >>> X = [[np.nan, 2], [6, np.nan], [7, 6]]\n    >>> print(imp.transform(X))\n    [[4.          2.        ]\n     [6.          3.666]\n     [7.          6.        ]]\n\nThe :class:`SimpleImputer` class also supports sparse matrices::\n\n    >>> import scipy.sparse as sp\n    >>> X = sp.csc_matrix([[1, 2], [0, -1], [8, 4]])\n    >>> imp = SimpleImputer(missing_values=-1, strategy='mean')\n    >>> imp.fit(X)\n    SimpleImputer(missing_values=-1)\n    >>> X_test = sp.csc_matrix([[-1, 2], [6, -1], [7, 6]])\n    >>> print(imp.transform(X_test).toarray())\n    [[3. 2.]\n     [6. 3.]\n     [7. 6.]]\n\nNote that this format is not meant to be used to implicitly store missing\nvalues in the matrix because it would densify it at transform time. Missing\nvalues encoded by 0 must be used with dense input.\n\nThe :class:`SimpleImputer` class also supports categorical data represented as\nstring values or pandas categoricals when using the ``'most_frequent'`` or\n``'constant'`` strategy::\n\n    >>> import pandas as pd\n    >>> df = pd.DataFrame([[\"a\", \"x\"],\n    ...                    [np.nan, \"y\"],\n    ...                    [\"a\", np.nan],\n    ...                    [\"b\", \"y\"]], dtype=\"category\")\n    ...\n    >>> imp = SimpleImputer(strategy=\"most_frequent\")\n    >>> print(imp.fit_transform(df))\n    [['a' 'x']\n     ['a' 'y']\n     ['a' 'y']\n     ['b' 'y']]\n\nFor another example on usage, see :ref:`sphx_glr_auto_examples_impute_plot_missing_values.py`.\n\n.. _iterative_imputer:"
  },
  {
    "filename": "impute.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\impute.rst.txt",
    "id": "impute.rst.txt_chunk_4",
    "header": "Multivariate feature imputation",
    "text": "Multivariate feature imputation\n===============================\n\nA more sophisticated approach is to use the :class:`IterativeImputer` class,\nwhich models each feature with missing values as a function of other features,\nand uses that estimate for imputation. It does so in an iterated round-robin\nfashion: at each step, a feature column is designated as output ``y`` and the\nother feature columns are treated as inputs ``X``. A regressor is fit on ``(X,\ny)`` for known ``y``. Then, the regressor is used to predict the missing values\nof ``y``.  This is done for each feature in an iterative fashion, and then is\nrepeated for ``max_iter`` imputation rounds. The results of the final\nimputation round are returned.\n\n.. note::\n\n   This estimator is still **experimental** for now: default parameters or\n   details of behaviour might change without any deprecation cycle. Resolving\n   the following issues would help stabilize :class:`IterativeImputer`:\n   convergence criteria (:issue:`14338`) and default estimators\n   (:issue:`13286`). To use it, you need to explicitly import\n   ``enable_iterative_imputer``.\n\n::\n\n    >>> import numpy as np\n    >>> from sklearn.experimental import enable_iterative_imputer\n    >>> from sklearn.impute import IterativeImputer\n    >>> imp = IterativeImputer(max_iter=10, random_state=0)\n    >>> imp.fit([[1, 2], [3, 6], [4, 8], [np.nan, 3], [7, np.nan]])\n    IterativeImputer(random_state=0)\n    >>> X_test = [[np.nan, 2], [6, np.nan], [np.nan, 6]]\n    >>> # the model learns that the second feature is double the first\n    >>> print(np.round(imp.transform(X_test)))\n    [[ 1.  2.]\n     [ 6. 12.]\n     [ 3.  6.]]\n\nBoth :class:`SimpleImputer` and :class:`IterativeImputer` can be used in a\nPipeline as a way to build a composite estimator that supports imputation.\nSee :ref:`sphx_glr_auto_examples_impute_plot_missing_values.py`."
  },
  {
    "filename": "impute.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\impute.rst.txt",
    "id": "impute.rst.txt_chunk_5",
    "header": "Flexibility of IterativeImputer",
    "text": "Flexibility of IterativeImputer\n-------------------------------\n\nThere are many well-established imputation packages in the R data science\necosystem: Amelia, mi, mice, missForest, etc. missForest is popular, and turns\nout to be a particular instance of different sequential imputation algorithms\nthat can all be implemented with :class:`IterativeImputer` by passing in\ndifferent regressors to be used for predicting missing feature values. In the\ncase of missForest, this regressor is a Random Forest.\nSee :ref:`sphx_glr_auto_examples_impute_plot_iterative_imputer_variants_comparison.py`.\n\n\n.. _multiple_imputation:"
  },
  {
    "filename": "impute.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\impute.rst.txt",
    "id": "impute.rst.txt_chunk_6",
    "header": "Multiple vs. Single Imputation",
    "text": "Multiple vs. Single Imputation\n------------------------------\n\nIn the statistics community, it is common practice to perform multiple\nimputations, generating, for example, ``m`` separate imputations for a single\nfeature matrix. Each of these ``m`` imputations is then put through the\nsubsequent analysis pipeline (e.g. feature engineering, clustering, regression,\nclassification). The ``m`` final analysis results (e.g. held-out validation\nerrors) allow the data scientist to obtain understanding of how analytic\nresults may differ as a consequence of the inherent uncertainty caused by the\nmissing values. The above practice is called multiple imputation.\n\nOur implementation of :class:`IterativeImputer` was inspired by the R MICE\npackage (Multivariate Imputation by Chained Equations) [1]_, but differs from\nit by returning a single imputation instead of multiple imputations.  However,\n:class:`IterativeImputer` can also be used for multiple imputations by applying\nit repeatedly to the same dataset with different random seeds when\n``sample_posterior=True``. See [2]_, chapter 4 for more discussion on multiple\nvs. single imputations.\n\nIt is still an open problem as to how useful single vs. multiple imputation is\nin the context of prediction and classification when the user is not\ninterested in measuring uncertainty due to missing values.\n\nNote that a call to the ``transform`` method of :class:`IterativeImputer` is\nnot allowed to change the number of samples. Therefore multiple imputations\ncannot be achieved by a single call to ``transform``.\n\n.. rubric:: References\n\n.. [1] `Stef van Buuren, Karin Groothuis-Oudshoorn (2011). \"mice: Multivariate\n   Imputation by Chained Equations in R\". Journal of Statistical Software 45:\n   1-67. <https://www.jstatsoft.org/article/view/v045i03>`_\n\n.. [2] Roderick J A Little and Donald B Rubin (1986). \"Statistical Analysis\n   with Missing Data\". John Wiley & Sons, Inc., New York, NY, USA.\n\n.. _knnimpute:"
  },
  {
    "filename": "impute.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\impute.rst.txt",
    "id": "impute.rst.txt_chunk_7",
    "header": "Nearest neighbors imputation",
    "text": "Nearest neighbors imputation\n============================\n\nThe :class:`KNNImputer` class provides imputation for filling in missing values\nusing the k-Nearest Neighbors approach. By default, a euclidean distance metric\nthat supports missing values,\n:func:`~sklearn.metrics.pairwise.nan_euclidean_distances`, is used to find the\nnearest neighbors. Each missing feature is imputed using values from\n``n_neighbors`` nearest neighbors that have a value for the feature. The\nfeature of the neighbors are averaged uniformly or weighted by distance to each\nneighbor. If a sample has more than one feature missing, then the neighbors for\nthat sample can be different depending on the particular feature being imputed.\nWhen the number of available neighbors is less than `n_neighbors` and there are\nno defined distances to the training set, the training set average for that\nfeature is used during imputation. If there is at least one neighbor with a\ndefined distance, the weighted or unweighted average of the remaining neighbors\nwill be used during imputation. If a feature is always missing in training, it\nis removed during `transform`. For more information on the methodology, see\nref. [OL2001]_.\n\nThe following snippet demonstrates how to replace missing values,\nencoded as ``np.nan``, using the mean feature value of the two nearest\nneighbors of samples with missing values::\n\n    >>> import numpy as np\n    >>> from sklearn.impute import KNNImputer\n    >>> nan = np.nan\n    >>> X = [[1, 2, nan], [3, 4, 3], [nan, 6, 5], [8, 8, 7]]\n    >>> imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n    >>> imputer.fit_transform(X)\n    array([[1. , 2. , 4. ],\n           [3. , 4. , 3. ],\n           [5.5, 6. , 5. ],\n           [8. , 8. , 7. ]])\n\nFor another example on usage, see :ref:`sphx_glr_auto_examples_impute_plot_missing_values.py`.\n\n.. rubric:: References\n\n.. [OL2001] `Olga Troyanskaya, Michael Cantor, Gavin Sherlock, Pat Brown,\n    Trevor Hastie, Robert Tibshirani, David Botstein and Russ B. Altman,\n    Missing value estimation methods for DNA microarrays, BIOINFORMATICS\n    Vol. 17 no. 6, 2001 Pages 520-525.\n    <https://academic.oup.com/bioinformatics/article/17/6/520/272365>`_"
  },
  {
    "filename": "impute.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\impute.rst.txt",
    "id": "impute.rst.txt_chunk_8",
    "header": "Keeping the number of features constant",
    "text": "Keeping the number of features constant\n=======================================\n\nBy default, the scikit-learn imputers will drop fully empty features, i.e.\ncolumns containing only missing values. For instance::\n\n  >>> imputer = SimpleImputer()\n  >>> X = np.array([[np.nan, 1], [np.nan, 2], [np.nan, 3]])\n  >>> imputer.fit_transform(X)\n  array([[1.],\n         [2.],\n         [3.]])\n\nThe first feature in `X` containing only `np.nan` was dropped after the\nimputation. While this feature will not help in predictive setting, dropping\nthe columns will change the shape of `X` which could be problematic when using\nimputers in a more complex machine-learning pipeline. The parameter\n`keep_empty_features` offers the option to keep the empty features by imputing\nwith a constant value. In most of the cases, this constant value is zero::\n\n  >>> imputer.set_params(keep_empty_features=True)\n  SimpleImputer(keep_empty_features=True)\n  >>> imputer.fit_transform(X)\n  array([[0., 1.],\n         [0., 2.],\n         [0., 3.]])\n\n.. _missing_indicator:"
  },
  {
    "filename": "impute.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\impute.rst.txt",
    "id": "impute.rst.txt_chunk_9",
    "header": "Marking imputed values",
    "text": "Marking imputed values\n======================\n\nThe :class:`MissingIndicator` transformer is useful to transform a dataset into\ncorresponding binary matrix indicating the presence of missing values in the\ndataset. This transformation is useful in conjunction with imputation. When\nusing imputation, preserving the information about which values had been\nmissing can be informative. Note that both the :class:`SimpleImputer` and\n:class:`IterativeImputer` have the boolean parameter ``add_indicator``\n(``False`` by default) which when set to ``True`` provides a convenient way of\nstacking the output of the :class:`MissingIndicator` transformer with the\noutput of the imputer.\n\n``NaN`` is usually used as the placeholder for missing values. However, it\nenforces the data type to be float. The parameter ``missing_values`` allows to\nspecify other placeholder such as integer. In the following example, we will\nuse ``-1`` as missing values::\n\n  >>> from sklearn.impute import MissingIndicator\n  >>> X = np.array([[-1, -1, 1, 3],\n  ...               [4, -1, 0, -1],\n  ...               [8, -1, 1, 0]])\n  >>> indicator = MissingIndicator(missing_values=-1)\n  >>> mask_missing_values_only = indicator.fit_transform(X)\n  >>> mask_missing_values_only\n  array([[ True,  True, False],\n         [False,  True,  True],\n         [False,  True, False]])\n\nThe ``features`` parameter is used to choose the features for which the mask is\nconstructed. By default, it is ``'missing-only'`` which returns the imputer\nmask of the features containing missing values at ``fit`` time::\n\n  >>> indicator.features_\n  array([0, 1, 3])\n\nThe ``features`` parameter can be set to ``'all'`` to return all features\nwhether or not they contain missing values::\n\n  >>> indicator = MissingIndicator(missing_values=-1, features=\"all\")\n  >>> mask_all = indicator.fit_transform(X)\n  >>> mask_all\n  array([[ True,  True, False, False],\n         [False,  True, False,  True],\n         [False,  True, False, False]])\n  >>> indicator.features_\n  array([0, 1, 2, 3])\n\nWhen using the :class:`MissingIndicator` in a\n:class:`~sklearn.pipeline.Pipeline`, be sure to use the\n:class:`~sklearn.pipeline.FeatureUnion` or\n:class:`~sklearn.compose.ColumnTransformer` to add the indicator features to\nthe regular features. First we obtain the `iris` dataset, and add some missing\nvalues to it.\n\n  >>> from sklearn.datasets import load_iris\n  >>> from sklearn.impute import SimpleImputer, MissingIndicator\n  >>> from sklearn.model_selection import train_test_split\n  >>> from sklearn.pipeline import FeatureUnion, make_pipeline\n  >>> from sklearn.tree import DecisionTreeClassifier\n  >>> X, y = load_iris(return_X_y=True)\n  >>> mask = np.random.randint(0, 2, size=X.shape).astype(bool)\n  >>> X[mask] = np.nan\n  >>> X_train, X_test, y_train, _ = train_test_split(X, y, test_size=100,\n  ...                                                random_state=0)\n\nNow we create a :class:`~sklearn.pipeline.FeatureUnion`. All features will be\nimputed using :class:`SimpleImputer`, in order to enable classifiers to work\nwith this data. Additionally, it adds the indicator variables from\n:class:`MissingIndicator`.\n\n  >>> transformer = FeatureUnion(\n  ...     transformer_list=[\n  ...         ('features', SimpleImputer(strategy='mean')),\n  ...         ('indicators', MissingIndicator())])\n  >>> transformer = transformer.fit(X_train, y_train)\n  >>> results = transformer.transform(X_test)\n  >>> results.shape\n  (100, 8)\n\nOf course, we cannot use the transformer to make any predictions. We should\nwrap this in a :class:`~sklearn.pipeline.Pipeline` with a classifier (e.g., a\n:class:`~sklearn.tree.DecisionTreeClassifier`) to be able to make predictions.\n\n  >>> clf = make_pipeline(transformer, DecisionTreeClassifier())\n  >>> clf = clf.fit(X_train, y_train)\n  >>> results = clf.predict(X_test)\n  >>> results.shape\n  (100,)"
  },
  {
    "filename": "impute.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\impute.rst.txt",
    "id": "impute.rst.txt_chunk_10",
    "header": "Estimators that handle NaN values",
    "text": "Estimators that handle NaN values\n=================================\n\nSome estimators are designed to handle NaN values without preprocessing.\nBelow is the list of these estimators, classified by type\n(cluster, regressor, classifier, transform):\n\n.. allow_nan_estimators::"
  },
  {
    "filename": "isotonic.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\isotonic.rst.txt",
    "id": "isotonic.rst.txt_chunk_0",
    "header": ".. _isotonic:",
    "text": ".. _isotonic:\n\n==================="
  },
  {
    "filename": "isotonic.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\isotonic.rst.txt",
    "id": "isotonic.rst.txt_chunk_1",
    "header": "Isotonic regression",
    "text": "Isotonic regression\n===================\n\n.. currentmodule:: sklearn.isotonic\n\nThe class :class:`IsotonicRegression` fits a non-decreasing real function to\n1-dimensional data. It solves the following problem:\n\n.. math::\n    \\min \\sum_i w_i (y_i - \\hat{y}_i)^2\n\nsubject to :math:`\\hat{y}_i \\le \\hat{y}_j` whenever :math:`X_i \\le X_j`,\nwhere the weights :math:`w_i` are strictly positive, and both `X` and `y` are\narbitrary real quantities.\n\nThe `increasing` parameter changes the constraint to\n:math:`\\hat{y}_i \\ge \\hat{y}_j` whenever :math:`X_i \\le X_j`. Setting it to\n'auto' will automatically choose the constraint based on `Spearman's rank\ncorrelation coefficient\n<https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient>`_.\n\n:class:`IsotonicRegression` produces a series of predictions\n:math:`\\hat{y}_i` for the training data which are the closest to the targets\n:math:`y` in terms of mean squared error. These predictions are interpolated\nfor predicting to unseen data. The predictions of :class:`IsotonicRegression`\nthus form a function that is piecewise linear:\n\n.. figure:: ../auto_examples/miscellaneous/images/sphx_glr_plot_isotonic_regression_001.png\n   :target: ../auto_examples/miscellaneous/plot_isotonic_regression.html\n   :align: center\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_miscellaneous_plot_isotonic_regression.py`"
  },
  {
    "filename": "kernel_approximation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\kernel_approximation.rst.txt",
    "id": "kernel_approximation.rst.txt_chunk_0",
    "header": ".. _kernel_approximation:",
    "text": ".. _kernel_approximation:"
  },
  {
    "filename": "kernel_approximation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\kernel_approximation.rst.txt",
    "id": "kernel_approximation.rst.txt_chunk_1",
    "header": "Kernel Approximation",
    "text": "Kernel Approximation\n====================\n\nThis submodule contains functions that approximate the feature mappings that\ncorrespond to certain kernels, as they are used for example in support vector\nmachines (see :ref:`svm`).\nThe following feature functions perform non-linear transformations of the\ninput, which can serve as a basis for linear classification or other\nalgorithms.\n\n.. currentmodule:: sklearn.linear_model\n\nThe advantage of using approximate explicit feature maps compared to the\n`kernel trick <https://en.wikipedia.org/wiki/Kernel_trick>`_,\nwhich makes use of feature maps implicitly, is that explicit mappings\ncan be better suited for online learning and can significantly reduce the cost\nof learning with very large datasets.\nStandard kernelized SVMs do not scale well to large datasets, but using an\napproximate kernel map it is possible to use much more efficient linear SVMs.\nIn particular, the combination of kernel map approximations with\n:class:`SGDClassifier` can make non-linear learning on large datasets possible.\n\nSince there has not been much empirical work using approximate embeddings, it\nis advisable to compare results against exact kernel methods when possible.\n\n.. seealso::\n\n   :ref:`polynomial_regression` for an exact polynomial transformation.\n\n.. currentmodule:: sklearn.kernel_approximation\n\n.. _nystroem_kernel_approx:"
  },
  {
    "filename": "kernel_approximation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\kernel_approximation.rst.txt",
    "id": "kernel_approximation.rst.txt_chunk_2",
    "header": "Nystroem Method for Kernel Approximation",
    "text": "Nystroem Method for Kernel Approximation\n----------------------------------------\nThe Nystroem method, as implemented in :class:`Nystroem` is a general method for\nreduced rank approximations of kernels. It achieves this by subsampling without\nreplacement rows/columns of the data on which the kernel is evaluated. While the\ncomputational complexity of the exact method is\n:math:`\\mathcal{O}(n^3_{\\text{samples}})`, the complexity of the approximation\nis :math:`\\mathcal{O}(n^2_{\\text{components}} \\cdot n_{\\text{samples}})`, where\none can set :math:`n_{\\text{components}} \\ll n_{\\text{samples}}` without a\nsignificant decrease in performance [WS2001]_.\n\nWe can construct the eigendecomposition of the kernel matrix :math:`K`, based\non the features of the data, and then split it into sampled and unsampled data\npoints.\n\n.. math::\n\n        K = U \\Lambda U^T\n        = \\begin{bmatrix} U_1 \\\\ U_2\\end{bmatrix} \\Lambda \\begin{bmatrix} U_1 \\\\ U_2 \\end{bmatrix}^T\n        = \\begin{bmatrix} U_1 \\Lambda U_1^T & U_1 \\Lambda U_2^T \\\\ U_2 \\Lambda U_1^T & U_2 \\Lambda U_2^T \\end{bmatrix}\n        \\equiv \\begin{bmatrix} K_{11} & K_{12} \\\\ K_{21} & K_{22} \\end{bmatrix}\n\nwhere:\n\n* :math:`U` is orthonormal\n* :math:`\\Lambda` is diagonal matrix of eigenvalues\n* :math:`U_1` is orthonormal matrix of samples that were chosen\n* :math:`U_2` is orthonormal matrix of samples that were not chosen\n\nGiven that :math:`U_1 \\Lambda U_1^T` can be obtained by orthonormalization of\nthe matrix :math:`K_{11}`, and :math:`U_2 \\Lambda U_1^T` can be evaluated (as\nwell as its transpose), the only remaining term to elucidate is\n:math:`U_2 \\Lambda U_2^T`. To do this we can express it in terms of the already\nevaluated matrices:\n\n.. math::\n\n         \\begin{align} U_2 \\Lambda U_2^T &= \\left(K_{21} U_1 \\Lambda^{-1}\\right) \\Lambda \\left(K_{21} U_1 \\Lambda^{-1}\\right)^T\n         \\\\&= K_{21} U_1 (\\Lambda^{-1} \\Lambda) \\Lambda^{-1} U_1^T K_{21}^T\n         \\\\&= K_{21} U_1 \\Lambda^{-1} U_1^T K_{21}^T\n         \\\\&= K_{21} K_{11}^{-1} K_{21}^T\n         \\\\&= \\left( K_{21} K_{11}^{-\\frac12} \\right) \\left( K_{21} K_{11}^{-\\frac12} \\right)^T\n         .\\end{align}\n\nDuring ``fit``, the class :class:`Nystroem` evaluates the basis :math:`U_1`, and\ncomputes the normalization constant, :math:`K_{11}^{-\\frac12}`. Later, during\n``transform``, the kernel matrix is determined between the basis (given by the\n`components_` attribute) and the new data points, ``X``. This matrix is then\nmultiplied by the ``normalization_`` matrix for the final result.\n\nBy default :class:`Nystroem` uses the ``rbf`` kernel, but it can use any kernel\nfunction or a precomputed kernel matrix. The number of samples used - which is\nalso the dimensionality of the features computed - is given by the parameter\n``n_components``.\n\n.. rubric:: Examples\n\n* See the example entitled\n  :ref:`sphx_glr_auto_examples_applications_plot_cyclical_feature_engineering.py`,\n  that shows an efficient machine learning pipeline that uses a\n  :class:`Nystroem` kernel.\n* See :ref:`sphx_glr_auto_examples_miscellaneous_plot_kernel_approximation.py`\n  for a comparison of :class:`Nystroem` kernel with :class:`RBFSampler`.\n\n.. _rbf_kernel_approx:"
  },
  {
    "filename": "kernel_approximation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\kernel_approximation.rst.txt",
    "id": "kernel_approximation.rst.txt_chunk_3",
    "header": "Radial Basis Function Kernel",
    "text": "Radial Basis Function Kernel\n----------------------------\n\nThe :class:`RBFSampler` constructs an approximate mapping for the radial basis\nfunction kernel, also known as *Random Kitchen Sinks* [RR2007]_. This\ntransformation can be used to explicitly model a kernel map, prior to applying\na linear algorithm, for example a linear SVM::\n\n    >>> from sklearn.kernel_approximation import RBFSampler\n    >>> from sklearn.linear_model import SGDClassifier\n    >>> X = [[0, 0], [1, 1], [1, 0], [0, 1]]\n    >>> y = [0, 0, 1, 1]\n    >>> rbf_feature = RBFSampler(gamma=1, random_state=1)\n    >>> X_features = rbf_feature.fit_transform(X)\n    >>> clf = SGDClassifier(max_iter=5)\n    >>> clf.fit(X_features, y)\n    SGDClassifier(max_iter=5)\n    >>> clf.score(X_features, y)\n    1.0\n\nThe mapping relies on a Monte Carlo approximation to the\nkernel values. The ``fit`` function performs the Monte Carlo sampling, whereas\nthe ``transform`` method performs the mapping of the data.  Because of the\ninherent randomness of the process, results may vary between different calls to\nthe ``fit`` function.\n\nThe ``fit`` function takes two arguments:\n``n_components``, which is the target dimensionality of the feature transform,\nand ``gamma``, the parameter of the RBF-kernel.  A higher ``n_components`` will\nresult in a better approximation of the kernel and will yield results more\nsimilar to those produced by a kernel SVM. Note that \"fitting\" the feature\nfunction does not actually depend on the data given to the ``fit`` function.\nOnly the dimensionality of the data is used.\nDetails on the method can be found in [RR2007]_.\n\nFor a given value of ``n_components`` :class:`RBFSampler` is often less accurate\nas :class:`Nystroem`. :class:`RBFSampler` is cheaper to compute, though, making\nuse of larger feature spaces more efficient.\n\n.. figure:: ../auto_examples/miscellaneous/images/sphx_glr_plot_kernel_approximation_002.png\n    :target: ../auto_examples/miscellaneous/plot_kernel_approximation.html\n    :scale: 50%\n    :align: center\n\n    Comparing an exact RBF kernel (left) with the approximation (right)\n\n.. rubric:: Examples\n\n* See :ref:`sphx_glr_auto_examples_miscellaneous_plot_kernel_approximation.py` for a\n  comparison of :class:`Nystroem` kernel with :class:`RBFSampler`.\n\n\n.. _additive_chi_kernel_approx:"
  },
  {
    "filename": "kernel_approximation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\kernel_approximation.rst.txt",
    "id": "kernel_approximation.rst.txt_chunk_4",
    "header": "Additive Chi Squared Kernel",
    "text": "Additive Chi Squared Kernel\n---------------------------\n\nThe additive chi squared kernel is a kernel on histograms, often used in computer vision.\n\nThe additive chi squared kernel as used here is given by\n\n.. math::\n\n        k(x, y) = \\sum_i \\frac{2x_iy_i}{x_i+y_i}\n\nThis is not exactly the same as :func:`sklearn.metrics.pairwise.additive_chi2_kernel`.\nThe authors of [VZ2010]_ prefer the version above as it is always positive\ndefinite.\nSince the kernel is additive, it is possible to treat all components\n:math:`x_i` separately for embedding. This makes it possible to sample\nthe Fourier transform in regular intervals, instead of approximating\nusing Monte Carlo sampling.\n\nThe class :class:`AdditiveChi2Sampler` implements this component wise\ndeterministic sampling. Each component is sampled :math:`n` times, yielding\n:math:`2n+1` dimensions per input dimension (the multiple of two stems\nfrom the real and complex part of the Fourier transform).\nIn the literature, :math:`n` is usually chosen to be 1 or 2, transforming\nthe dataset to size ``n_samples * 5 * n_features`` (in the case of :math:`n=2`).\n\nThe approximate feature map provided by :class:`AdditiveChi2Sampler` can be combined\nwith the approximate feature map provided by :class:`RBFSampler` to yield an approximate\nfeature map for the exponentiated chi squared kernel.\nSee the [VZ2010]_ for details and [VVZ2010]_ for combination with the :class:`RBFSampler`.\n\n.. _skewed_chi_kernel_approx:"
  },
  {
    "filename": "kernel_approximation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\kernel_approximation.rst.txt",
    "id": "kernel_approximation.rst.txt_chunk_5",
    "header": "Skewed Chi Squared Kernel",
    "text": "Skewed Chi Squared Kernel\n-------------------------\n\nThe skewed chi squared kernel is given by:\n\n.. math::\n\n        k(x,y) = \\prod_i \\frac{2\\sqrt{x_i+c}\\sqrt{y_i+c}}{x_i + y_i + 2c}\n\n\nIt has properties that are similar to the exponentiated chi squared kernel\noften used in computer vision, but allows for a simple Monte Carlo\napproximation of the feature map.\n\nThe usage of the :class:`SkewedChi2Sampler` is the same as the usage described\nabove for the :class:`RBFSampler`. The only difference is in the free\nparameter, that is called :math:`c`.\nFor a motivation for this mapping and the mathematical details see [LS2010]_.\n\n.. _polynomial_kernel_approx:"
  },
  {
    "filename": "kernel_approximation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\kernel_approximation.rst.txt",
    "id": "kernel_approximation.rst.txt_chunk_6",
    "header": "Polynomial Kernel Approximation via Tensor Sketch",
    "text": "Polynomial Kernel Approximation via Tensor Sketch\n-------------------------------------------------\n\nThe :ref:`polynomial kernel <polynomial_kernel>` is a popular type of kernel\nfunction given by:\n\n.. math::\n\n        k(x, y) = (\\gamma x^\\top y +c_0)^d\n\nwhere:\n\n* ``x``, ``y`` are the input vectors\n* ``d`` is the kernel degree\n\nIntuitively, the feature space of the polynomial kernel of degree `d`\nconsists of all possible degree-`d` products among input features, which enables\nlearning algorithms using this kernel to account for interactions between features.\n\nThe TensorSketch [PP2013]_ method, as implemented in :class:`PolynomialCountSketch`, is a\nscalable, input data independent method for polynomial kernel approximation.\nIt is based on the concept of Count sketch [WIKICS]_ [CCF2002]_ , a dimensionality\nreduction technique similar to feature hashing, which instead uses several\nindependent hash functions. TensorSketch obtains a Count Sketch of the outer product\nof two vectors (or a vector with itself), which can be used as an approximation of the\npolynomial kernel feature space. In particular, instead of explicitly computing\nthe outer product, TensorSketch computes the Count Sketch of the vectors and then\nuses polynomial multiplication via the Fast Fourier Transform to compute the\nCount Sketch of their outer product.\n\nConveniently, the training phase of TensorSketch simply consists of initializing\nsome random variables. It is thus independent of the input data, i.e. it only\ndepends on the number of input features, but not the data values.\nIn addition, this method can transform samples in\n:math:`\\mathcal{O}(n_{\\text{samples}}(n_{\\text{features}} + n_{\\text{components}} \\log(n_{\\text{components}})))`\ntime, where :math:`n_{\\text{components}}` is the desired output dimension,\ndetermined by ``n_components``.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_kernel_approximation_plot_scalable_poly_kernels.py`\n\n.. _tensor_sketch_kernel_approx:"
  },
  {
    "filename": "kernel_approximation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\kernel_approximation.rst.txt",
    "id": "kernel_approximation.rst.txt_chunk_7",
    "header": "Mathematical Details",
    "text": "Mathematical Details\n--------------------\n\nKernel methods like support vector machines or kernelized\nPCA rely on a property of reproducing kernel Hilbert spaces.\nFor any positive definite kernel function :math:`k` (a so called Mercer kernel),\nit is guaranteed that there exists a mapping :math:`\\phi`\ninto a Hilbert space :math:`\\mathcal{H}`, such that\n\n.. math::\n\n        k(x,y) = \\langle \\phi(x), \\phi(y) \\rangle\n\nWhere :math:`\\langle \\cdot, \\cdot \\rangle` denotes the inner product in the\nHilbert space.\n\nIf an algorithm, such as a linear support vector machine or PCA,\nrelies only on the scalar product of data points :math:`x_i`, one may use\nthe value of :math:`k(x_i, x_j)`, which corresponds to applying the algorithm\nto the mapped data points :math:`\\phi(x_i)`.\nThe advantage of using :math:`k` is that the mapping :math:`\\phi` never has\nto be calculated explicitly, allowing for arbitrary large\nfeatures (even infinite).\n\nOne drawback of kernel methods is, that it might be necessary\nto store many kernel values :math:`k(x_i, x_j)` during optimization.\nIf a kernelized classifier is applied to new data :math:`y_j`,\n:math:`k(x_i, y_j)` needs to be computed to make predictions,\npossibly for many different :math:`x_i` in the training set.\n\nThe classes in this submodule allow to approximate the embedding\n:math:`\\phi`, thereby working explicitly with the representations\n:math:`\\phi(x_i)`, which obviates the need to apply the kernel\nor store training examples.\n\n\n.. rubric:: References\n\n.. [WS2001] `\"Using the Nystr\u00f6m method to speed up kernel machines\"\n  <https://papers.nips.cc/paper_files/paper/2000/hash/19de10adbaa1b2ee13f77f679fa1483a-Abstract.html>`_\n  Williams, C.K.I.; Seeger, M. - 2001.\n.. [RR2007] `\"Random features for large-scale kernel machines\"\n  <https://papers.nips.cc/paper/2007/hash/013a006f03dbc5392effeb8f18fda755-Abstract.html>`_\n  Rahimi, A. and Recht, B. - Advances in neural information processing 2007,\n.. [LS2010] `\"Random Fourier approximations for skewed multiplicative histogram kernels\"\n  <https://www.researchgate.net/publication/221114584_Random_Fourier_Approximations_for_Skewed_Multiplicative_Histogram_Kernels>`_\n  Li, F., Ionescu, C., and Sminchisescu, C.\n  - Pattern Recognition,  DAGM 2010, Lecture Notes in Computer Science.\n.. [VZ2010] `\"Efficient additive kernels via explicit feature maps\"\n  <https://www.robots.ox.ac.uk/~vgg/publications/2011/Vedaldi11/vedaldi11.pdf>`_\n  Vedaldi, A. and Zisserman, A. - Computer Vision and Pattern Recognition 2010\n.. [VVZ2010] `\"Generalized RBF feature maps for Efficient Detection\"\n  <https://www.robots.ox.ac.uk/~vgg/publications/2010/Sreekanth10/sreekanth10.pdf>`_\n  Vempati, S. and Vedaldi, A. and Zisserman, A. and Jawahar, CV - 2010\n.. [PP2013] :doi:`\"Fast and scalable polynomial kernels via explicit feature maps\"\n  <10.1145/2487575.2487591>`\n  Pham, N., & Pagh, R. - 2013\n.. [CCF2002] `\"Finding frequent items in data streams\"\n  <https://www.cs.princeton.edu/courses/archive/spring04/cos598B/bib/CharikarCF.pdf>`_\n  Charikar, M., Chen, K., & Farach-Colton - 2002\n.. [WIKICS] `\"Wikipedia: Count sketch\"\n  <https://en.wikipedia.org/wiki/Count_sketch>`_"
  },
  {
    "filename": "kernel_ridge.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\kernel_ridge.rst.txt",
    "id": "kernel_ridge.rst.txt_chunk_0",
    "header": ".. _kernel_ridge:",
    "text": ".. _kernel_ridge:\n\n==========================="
  },
  {
    "filename": "kernel_ridge.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\kernel_ridge.rst.txt",
    "id": "kernel_ridge.rst.txt_chunk_1",
    "header": "Kernel ridge regression",
    "text": "Kernel ridge regression\n===========================\n\n.. currentmodule:: sklearn.kernel_ridge\n\nKernel ridge regression (KRR) [M2012]_ combines :ref:`ridge_regression`\n(linear least squares with :math:`L_2`-norm regularization) with the `kernel trick\n<https://en.wikipedia.org/wiki/Kernel_method>`_. It thus learns a linear\nfunction in the space induced by the respective kernel and the data. For\nnon-linear kernels, this corresponds to a non-linear function in the original\nspace.\n\nThe form of the model learned by :class:`KernelRidge` is identical to support\nvector regression (:class:`~sklearn.svm.SVR`). However, different loss\nfunctions are used: KRR uses squared error loss while support vector\nregression uses :math:`\\epsilon`-insensitive loss, both combined with :math:`L_2`\nregularization. In contrast to :class:`~sklearn.svm.SVR`, fitting\n:class:`KernelRidge` can be done in closed-form and is typically faster for\nmedium-sized datasets. On the other hand, the learned model is non-sparse and\nthus slower than :class:`~sklearn.svm.SVR`, which learns a sparse model for\n:math:`\\epsilon > 0`, at prediction-time.\n\nThe following figure compares :class:`KernelRidge` and\n:class:`~sklearn.svm.SVR` on an artificial dataset, which consists of a\nsinusoidal target function and strong noise added to every fifth datapoint.\nThe learned model of :class:`KernelRidge` and :class:`~sklearn.svm.SVR` is\nplotted, where both complexity/regularization and bandwidth of the RBF kernel\nhave been optimized using grid-search. The learned functions are very\nsimilar; however, fitting :class:`KernelRidge` is approximately seven times\nfaster than fitting :class:`~sklearn.svm.SVR` (both with grid-search).\nHowever, prediction of 100,000 target values is more than three times faster\nwith :class:`~sklearn.svm.SVR` since it has learned a sparse model using only\napproximately 1/3 of the 100 training datapoints as support vectors.\n\n.. figure:: ../auto_examples/miscellaneous/images/sphx_glr_plot_kernel_ridge_regression_001.png\n   :target: ../auto_examples/miscellaneous/plot_kernel_ridge_regression.html\n   :align: center\n\nThe next figure compares the time for fitting and prediction of\n:class:`KernelRidge` and :class:`~sklearn.svm.SVR` for different sizes of the\ntraining set. Fitting :class:`KernelRidge` is faster than\n:class:`~sklearn.svm.SVR` for medium-sized training sets (less than 1000\nsamples); however, for larger training sets :class:`~sklearn.svm.SVR` scales\nbetter. With regard to prediction time, :class:`~sklearn.svm.SVR` is faster\nthan :class:`KernelRidge` for all sizes of the training set because of the\nlearned sparse solution. Note that the degree of sparsity and thus the\nprediction time depends on the parameters :math:`\\epsilon` and :math:`C` of\nthe :class:`~sklearn.svm.SVR`; :math:`\\epsilon = 0` would correspond to a\ndense model.\n\n.. figure:: ../auto_examples/miscellaneous/images/sphx_glr_plot_kernel_ridge_regression_002.png\n   :target: ../auto_examples/miscellaneous/plot_kernel_ridge_regression.html\n   :align: center\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_miscellaneous_plot_kernel_ridge_regression.py`\n\n.. rubric:: References\n\n.. [M2012] \"Machine Learning: A Probabilistic Perspective\"\n   Murphy, K. P. - chapter 14.4.3, pp. 492-493, The MIT Press, 2012"
  },
  {
    "filename": "lda_qda.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\lda_qda.rst.txt",
    "id": "lda_qda.rst.txt_chunk_0",
    "header": ".. _lda_qda:",
    "text": ".. _lda_qda:\n\n=========================================="
  },
  {
    "filename": "lda_qda.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\lda_qda.rst.txt",
    "id": "lda_qda.rst.txt_chunk_1",
    "header": "Linear and Quadratic Discriminant Analysis",
    "text": "Linear and Quadratic Discriminant Analysis\n==========================================\n\n.. currentmodule:: sklearn\n\nLinear Discriminant Analysis\n(:class:`~discriminant_analysis.LinearDiscriminantAnalysis`) and Quadratic\nDiscriminant Analysis\n(:class:`~discriminant_analysis.QuadraticDiscriminantAnalysis`) are two classic\nclassifiers, with, as their names suggest, a linear and a quadratic decision\nsurface, respectively.\n\nThese classifiers are attractive because they have closed-form solutions that\ncan be easily computed, are inherently multiclass, have proven to work well in\npractice, and have no hyperparameters to tune.\n\n.. |ldaqda| image:: ../auto_examples/classification/images/sphx_glr_plot_lda_qda_001.png\n        :target: ../auto_examples/classification/plot_lda_qda.html\n        :scale: 80\n\n.. centered:: |ldaqda|\n\nThe plot shows decision boundaries for Linear Discriminant Analysis and\nQuadratic Discriminant Analysis. The bottom row demonstrates that Linear\nDiscriminant Analysis can only learn linear boundaries, while Quadratic\nDiscriminant Analysis can learn quadratic boundaries and is therefore more\nflexible.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_classification_plot_lda_qda.py`: Comparison of LDA and\n  QDA on synthetic data."
  },
  {
    "filename": "lda_qda.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\lda_qda.rst.txt",
    "id": "lda_qda.rst.txt_chunk_2",
    "header": "Dimensionality reduction using Linear Discriminant Analysis",
    "text": "Dimensionality reduction using Linear Discriminant Analysis\n===========================================================\n\n:class:`~discriminant_analysis.LinearDiscriminantAnalysis` can be used to\nperform supervised dimensionality reduction, by projecting the input data to a\nlinear subspace consisting of the directions which maximize the separation\nbetween classes (in a precise sense discussed in the mathematics section\nbelow). The dimension of the output is necessarily less than the number of\nclasses, so this is in general a rather strong dimensionality reduction, and\nonly makes sense in a multiclass setting.\n\nThis is implemented in the `transform` method. The desired dimensionality can\nbe set using the ``n_components`` parameter. This parameter has no influence\non the `fit` and `predict` methods.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_decomposition_plot_pca_vs_lda.py`: Comparison of LDA and\n  PCA for dimensionality reduction of the Iris dataset\n\n.. _lda_qda_math:"
  },
  {
    "filename": "lda_qda.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\lda_qda.rst.txt",
    "id": "lda_qda.rst.txt_chunk_3",
    "header": "Mathematical formulation of the LDA and QDA classifiers",
    "text": "Mathematical formulation of the LDA and QDA classifiers\n=======================================================\n\nBoth LDA and QDA can be derived from simple probabilistic models which model\nthe class conditional distribution of the data :math:`P(X|y=k)` for each class\n:math:`k`. Predictions can then be obtained by using Bayes' rule, for each\ntraining sample :math:`x \\in \\mathcal{R}^d`:\n\n.. math::\n    P(y=k | x) = \\frac{P(x | y=k) P(y=k)}{P(x)} = \\frac{P(x | y=k) P(y = k)}{ \\sum_{l} P(x | y=l) \\cdot P(y=l)}\n\nand we select the class :math:`k` which maximizes this posterior probability.\n\nMore specifically, for linear and quadratic discriminant analysis,\n:math:`P(x|y)` is modeled as a multivariate Gaussian distribution with\ndensity:\n\n.. math:: P(x | y=k) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_k|^{1/2}}\\exp\\left(-\\frac{1}{2} (x-\\mu_k)^t \\Sigma_k^{-1} (x-\\mu_k)\\right)\n\nwhere :math:`d` is the number of features."
  },
  {
    "filename": "lda_qda.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\lda_qda.rst.txt",
    "id": "lda_qda.rst.txt_chunk_4",
    "header": "QDA",
    "text": "QDA\n---\n\nAccording to the model above, the log of the posterior is:\n\n.. math::\n\n    \\log P(y=k | x) &= \\log P(x | y=k) + \\log P(y = k) + Cst \\\\\n    &= -\\frac{1}{2} \\log |\\Sigma_k| -\\frac{1}{2} (x-\\mu_k)^t \\Sigma_k^{-1} (x-\\mu_k) + \\log P(y = k) + Cst,\n\nwhere the constant term :math:`Cst` corresponds to the denominator\n:math:`P(x)`, in addition to other constant terms from the Gaussian. The\npredicted class is the one that maximises this log-posterior.\n\n.. note:: **Relation with Gaussian Naive Bayes**\n\n    If in the QDA model one assumes that the covariance matrices are diagonal,\n    then the inputs are assumed to be conditionally independent in each class,\n    and the resulting classifier is equivalent to the Gaussian Naive Bayes\n    classifier :class:`naive_bayes.GaussianNB`."
  },
  {
    "filename": "lda_qda.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\lda_qda.rst.txt",
    "id": "lda_qda.rst.txt_chunk_5",
    "header": "LDA",
    "text": "LDA\n---\n\nLDA is a special case of QDA, where the Gaussians for each class are assumed\nto share the same covariance matrix: :math:`\\Sigma_k = \\Sigma` for all\n:math:`k`. This reduces the log posterior to:\n\n.. math:: \\log P(y=k | x) = -\\frac{1}{2} (x-\\mu_k)^t \\Sigma^{-1} (x-\\mu_k) + \\log P(y = k) + Cst.\n\nThe term :math:`(x-\\mu_k)^t \\Sigma^{-1} (x-\\mu_k)` corresponds to the\n`Mahalanobis Distance <https://en.wikipedia.org/wiki/Mahalanobis_distance>`_\nbetween the sample :math:`x` and the mean :math:`\\mu_k`. The Mahalanobis\ndistance tells how close :math:`x` is from :math:`\\mu_k`, while also\naccounting for the variance of each feature. We can thus interpret LDA as\nassigning :math:`x` to the class whose mean is the closest in terms of\nMahalanobis distance, while also accounting for the class prior\nprobabilities.\n\nThe log-posterior of LDA can also be written [3]_ as:\n\n.. math::\n\n    \\log P(y=k | x) = \\omega_k^t x + \\omega_{k0} + Cst.\n\nwhere :math:`\\omega_k = \\Sigma^{-1} \\mu_k` and :math:`\\omega_{k0} =\n-\\frac{1}{2} \\mu_k^t\\Sigma^{-1}\\mu_k + \\log P (y = k)`. These quantities\ncorrespond to the `coef_` and `intercept_` attributes, respectively.\n\nFrom the above formula, it is clear that LDA has a linear decision surface.\nIn the case of QDA, there are no assumptions on the covariance matrices\n:math:`\\Sigma_k` of the Gaussians, leading to quadratic decision surfaces.\nSee [1]_ for more details."
  },
  {
    "filename": "lda_qda.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\lda_qda.rst.txt",
    "id": "lda_qda.rst.txt_chunk_6",
    "header": "Mathematical formulation of LDA dimensionality reduction",
    "text": "Mathematical formulation of LDA dimensionality reduction\n========================================================\n\nFirst note that the K means :math:`\\mu_k` are vectors in\n:math:`\\mathcal{R}^d`, and they lie in an affine subspace :math:`H` of\ndimension at most :math:`K - 1` (2 points lie on a line, 3 points lie on a\nplane, etc.).\n\nAs mentioned above, we can interpret LDA as assigning :math:`x` to the class\nwhose mean :math:`\\mu_k` is the closest in terms of Mahalanobis distance,\nwhile also accounting for the class prior probabilities. Alternatively, LDA\nis equivalent to first *sphering* the data so that the covariance matrix is\nthe identity, and then assigning :math:`x` to the closest mean in terms of\nEuclidean distance (still accounting for the class priors).\n\nComputing Euclidean distances in this d-dimensional space is equivalent to\nfirst projecting the data points into :math:`H`, and computing the distances\nthere (since the other dimensions will contribute equally to each class in\nterms of distance). In other words, if :math:`x` is closest to :math:`\\mu_k`\nin the original space, it will also be the case in :math:`H`.\nThis shows that, implicit in the LDA\nclassifier, there is a dimensionality reduction by linear projection onto a\n:math:`K-1` dimensional space.\n\nWe can reduce the dimension even more, to a chosen :math:`L`, by projecting\nonto the linear subspace :math:`H_L` which maximizes the variance of the\n:math:`\\mu^*_k` after projection (in effect, we are doing a form of PCA for the\ntransformed class means :math:`\\mu^*_k`). This :math:`L` corresponds to the\n``n_components`` parameter used in the\n:func:`~discriminant_analysis.LinearDiscriminantAnalysis.transform` method. See\n[1]_ for more details."
  },
  {
    "filename": "lda_qda.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\lda_qda.rst.txt",
    "id": "lda_qda.rst.txt_chunk_7",
    "header": "Shrinkage and Covariance Estimator",
    "text": "Shrinkage and Covariance Estimator\n==================================\n\nShrinkage is a form of regularization used to improve the estimation of\ncovariance matrices in situations where the number of training samples is\nsmall compared to the number of features.\nIn this scenario, the empirical sample covariance is a poor\nestimator, and shrinkage helps improving the generalization performance of\nthe classifier.\nShrinkage LDA can be used by setting the ``shrinkage`` parameter of\nthe :class:`~discriminant_analysis.LinearDiscriminantAnalysis` class to `'auto'`.\nThis automatically determines the optimal shrinkage parameter in an analytic\nway following the lemma introduced by Ledoit and Wolf [2]_. Note that\ncurrently shrinkage only works when setting the ``solver`` parameter to `'lsqr'`\nor `'eigen'`.\n\nThe ``shrinkage`` parameter can also be manually set between 0 and 1. In\nparticular, a value of 0 corresponds to no shrinkage (which means the empirical\ncovariance matrix will be used) and a value of 1 corresponds to complete\nshrinkage (which means that the diagonal matrix of variances will be used as\nan estimate for the covariance matrix). Setting this parameter to a value\nbetween these two extrema will estimate a shrunk version of the covariance\nmatrix.\n\nThe shrunk Ledoit and Wolf estimator of covariance may not always be the\nbest choice. For example if the distribution of the data\nis normally distributed, the\nOracle Approximating Shrinkage estimator :class:`sklearn.covariance.OAS`\nyields a smaller Mean Squared Error than the one given by Ledoit and Wolf's\nformula used with `shrinkage=\"auto\"`. In LDA, the data are assumed to be gaussian\nconditionally to the class. If these assumptions hold, using LDA with\nthe OAS estimator of covariance will yield a better classification\naccuracy than if Ledoit and Wolf or the empirical covariance estimator is used.\n\nThe covariance estimator can be chosen using the ``covariance_estimator``\nparameter of the :class:`discriminant_analysis.LinearDiscriminantAnalysis`\nclass. A covariance estimator should have a :term:`fit` method and a\n``covariance_`` attribute like all covariance estimators in the\n:mod:`sklearn.covariance` module.\n\n\n.. |shrinkage| image:: ../auto_examples/classification/images/sphx_glr_plot_lda_001.png\n        :target: ../auto_examples/classification/plot_lda.html\n        :scale: 75\n\n.. centered:: |shrinkage|\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_classification_plot_lda.py`: Comparison of LDA classifiers\n  with Empirical, Ledoit Wolf and OAS covariance estimator."
  },
  {
    "filename": "lda_qda.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\lda_qda.rst.txt",
    "id": "lda_qda.rst.txt_chunk_8",
    "header": "Estimation algorithms",
    "text": "Estimation algorithms\n=====================\n\nUsing LDA and QDA requires computing the log-posterior which depends on the\nclass priors :math:`P(y=k)`, the class means :math:`\\mu_k`, and the\ncovariance matrices.\n\nThe 'svd' solver is the default solver used for\n:class:`~sklearn.discriminant_analysis.LinearDiscriminantAnalysis`, and it is\nthe only available solver for\n:class:`~sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis`.\nIt can perform both classification and transform (for LDA).\nAs it does not rely on the calculation of the covariance matrix, the 'svd'\nsolver may be preferable in situations where the number of features is large.\nThe 'svd' solver cannot be used with shrinkage.\nFor QDA, the use of the SVD solver relies on the fact that the covariance\nmatrix :math:`\\Sigma_k` is, by definition, equal to :math:`\\frac{1}{n - 1}\nX_k^tX_k = \\frac{1}{n - 1} V S^2 V^t` where :math:`V` comes from the SVD of the (centered)\nmatrix: :math:`X_k = U S V^t`. It turns out that we can compute the\nlog-posterior above without having to explicitly compute :math:`\\Sigma`:\ncomputing :math:`S` and :math:`V` via the SVD of :math:`X` is enough. For\nLDA, two SVDs are computed: the SVD of the centered input matrix :math:`X`\nand the SVD of the class-wise mean vectors.\n\nThe `'lsqr'` solver is an efficient algorithm that only works for\nclassification. It needs to explicitly compute the covariance matrix\n:math:`\\Sigma`, and supports shrinkage and custom covariance estimators.\nThis solver computes the coefficients\n:math:`\\omega_k = \\Sigma^{-1}\\mu_k` by solving for :math:`\\Sigma \\omega =\n\\mu_k`, thus avoiding the explicit computation of the inverse\n:math:`\\Sigma^{-1}`.\n\nThe `'eigen'` solver is based on the optimization of the between class scatter to\nwithin class scatter ratio. It can be used for both classification and\ntransform, and it supports shrinkage. However, the `'eigen'` solver needs to\ncompute the covariance matrix, so it might not be suitable for situations with\na high number of features.\n\n.. rubric:: References\n\n.. [1] \"The Elements of Statistical Learning\", Hastie T., Tibshirani R.,\n    Friedman J., Section 4.3, p.106-119, 2008.\n\n.. [2] Ledoit O, Wolf M. Honey, I Shrunk the Sample Covariance Matrix.\n    The Journal of Portfolio Management 30(4), 110-119, 2004.\n\n.. [3] R. O. Duda, P. E. Hart, D. G. Stork. Pattern Classification\n    (Second Edition), section 2.6.2."
  },
  {
    "filename": "learning_curve.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\learning_curve.rst.txt",
    "id": "learning_curve.rst.txt_chunk_0",
    "header": ".. _learning_curves:",
    "text": ".. _learning_curves:\n\n====================================================="
  },
  {
    "filename": "learning_curve.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\learning_curve.rst.txt",
    "id": "learning_curve.rst.txt_chunk_1",
    "header": "Validation curves: plotting scores to evaluate models",
    "text": "Validation curves: plotting scores to evaluate models\n=====================================================\n\n.. currentmodule:: sklearn.model_selection\n\nEvery estimator has its advantages and drawbacks. Its generalization error\ncan be decomposed in terms of bias, variance and noise. The **bias** of an\nestimator is its average error for different training sets. The **variance**\nof an estimator indicates how sensitive it is to varying training sets. Noise\nis a property of the data.\n\nIn the following plot, we see a function :math:`f(x) = \\cos (\\frac{3}{2} \\pi x)`\nand some noisy samples from that function. We use three different estimators\nto fit the function: linear regression with polynomial features of degree 1,\n4 and 15. We see that the first estimator can at best provide only a poor fit\nto the samples and the true function because it is too simple (high bias),\nthe second estimator approximates it almost perfectly and the last estimator\napproximates the training data perfectly but does not fit the true function\nvery well, i.e. it is very sensitive to varying training data (high variance).\n\n.. figure:: ../auto_examples/model_selection/images/sphx_glr_plot_underfitting_overfitting_001.png\n   :target: ../auto_examples/model_selection/plot_underfitting_overfitting.html\n   :align: center\n   :scale: 50%\n\nBias and variance are inherent properties of estimators and we usually have to\nselect learning algorithms and hyperparameters so that both bias and variance\nare as low as possible (see `Bias-variance dilemma\n<https://en.wikipedia.org/wiki/Bias-variance_dilemma>`_). Another way to reduce\nthe variance of a model is to use more training data. However, you should only\ncollect more training data if the true function is too complex to be\napproximated by an estimator with a lower variance.\n\nIn the simple one-dimensional problem that we have seen in the example it is\neasy to see whether the estimator suffers from bias or variance. However, in\nhigh-dimensional spaces, models can become very difficult to visualize. For\nthis reason, it is often helpful to use the tools described below.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_model_selection_plot_underfitting_overfitting.py`\n* :ref:`sphx_glr_auto_examples_model_selection_plot_train_error_vs_test_error.py`\n* :ref:`sphx_glr_auto_examples_model_selection_plot_learning_curve.py`\n\n\n.. _validation_curve:"
  },
  {
    "filename": "learning_curve.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\learning_curve.rst.txt",
    "id": "learning_curve.rst.txt_chunk_2",
    "header": "Validation curve",
    "text": "Validation curve\n================\n\nTo validate a model we need a scoring function (see :ref:`model_evaluation`),\nfor example accuracy for classifiers. The proper way of choosing multiple\nhyperparameters of an estimator is of course grid search or similar methods\n(see :ref:`grid_search`) that select the hyperparameter with the maximum score\non a validation set or multiple validation sets. Note that if we optimize\nthe hyperparameters based on a validation score the validation score is biased\nand not a good estimate of the generalization any longer. To get a proper\nestimate of the generalization we have to compute the score on another test\nset.\n\nHowever, it is sometimes helpful to plot the influence of a single\nhyperparameter on the training score and the validation score to find out\nwhether the estimator is overfitting or underfitting for some hyperparameter\nvalues.\n\nThe function :func:`validation_curve` can help in this case::\n\n  >>> import numpy as np\n  >>> from sklearn.model_selection import validation_curve\n  >>> from sklearn.datasets import load_iris\n  >>> from sklearn.svm import SVC\n\n  >>> np.random.seed(0)\n  >>> X, y = load_iris(return_X_y=True)\n  >>> indices = np.arange(y.shape[0])\n  >>> np.random.shuffle(indices)\n  >>> X, y = X[indices], y[indices]\n\n  >>> train_scores, valid_scores = validation_curve(\n  ...     SVC(kernel=\"linear\"), X, y, param_name=\"C\", param_range=np.logspace(-7, 3, 3),\n  ... )\n  >>> train_scores\n  array([[0.90, 0.94, 0.91, 0.89, 0.92],\n         [0.9 , 0.92, 0.93, 0.92, 0.93],\n         [0.97, 1   , 0.98, 0.97, 0.99]])\n  >>> valid_scores\n  array([[0.9, 0.9 , 0.9 , 0.96, 0.9 ],\n         [0.9, 0.83, 0.96, 0.96, 0.93],\n         [1. , 0.93, 1   , 1   , 0.9 ]])\n\nIf you intend to plot the validation curves only, the class\n:class:`~sklearn.model_selection.ValidationCurveDisplay` is more direct than\nusing matplotlib manually on the results of a call to :func:`validation_curve`.\nYou can use the method\n:meth:`~sklearn.model_selection.ValidationCurveDisplay.from_estimator` similarly\nto :func:`validation_curve` to generate and plot the validation curve:\n\n.. plot::\n   :context: close-figs\n   :align: center\n\n      from sklearn.datasets import load_iris\n      from sklearn.model_selection import ValidationCurveDisplay\n      from sklearn.svm import SVC\n      from sklearn.utils import shuffle\n      X, y = load_iris(return_X_y=True)\n      X, y = shuffle(X, y, random_state=0)\n      ValidationCurveDisplay.from_estimator(\n         SVC(kernel=\"linear\"), X, y, param_name=\"C\", param_range=np.logspace(-7, 3, 10)\n      )\n\nIf the training score and the validation score are both low, the estimator will\nbe underfitting. If the training score is high and the validation score is low,\nthe estimator is overfitting and otherwise it is working very well. A low\ntraining score and a high validation score is usually not possible.\n\n.. _learning_curve:"
  },
  {
    "filename": "learning_curve.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\learning_curve.rst.txt",
    "id": "learning_curve.rst.txt_chunk_3",
    "header": "Learning curve",
    "text": "Learning curve\n==============\n\nA learning curve shows the validation and training score of an estimator\nfor varying numbers of training samples. It is a tool to find out how much\nwe benefit from adding more training data and whether the estimator suffers\nmore from a variance error or a bias error. Consider the following example\nwhere we plot the learning curve of a naive Bayes classifier and an SVM.\n\nFor the naive Bayes, both the validation score and the training score\nconverge to a value that is quite low with increasing size of the training\nset. Thus, we will probably not benefit much from more training data.\n\nIn contrast, for small amounts of data, the training score of the SVM is\nmuch greater than the validation score. Adding more training samples will\nmost likely increase generalization.\n\n.. figure:: ../auto_examples/model_selection/images/sphx_glr_plot_learning_curve_001.png\n   :target: ../auto_examples/model_selection/plot_learning_curve.html\n   :align: center\n   :scale: 50%\n\nWe can use the function :func:`learning_curve` to generate the values\nthat are required to plot such a learning curve (number of samples\nthat have been used, the average scores on the training sets and the\naverage scores on the validation sets)::\n\n  >>> from sklearn.model_selection import learning_curve\n  >>> from sklearn.svm import SVC\n\n  >>> train_sizes, train_scores, valid_scores = learning_curve(\n  ...     SVC(kernel='linear'), X, y, train_sizes=[50, 80, 110], cv=5)\n  >>> train_sizes\n  array([ 50, 80, 110])\n  >>> train_scores\n  array([[0.98, 0.98 , 0.98, 0.98, 0.98],\n         [0.98, 1.   , 0.98, 0.98, 0.98],\n         [0.98, 1.   , 0.98, 0.98, 0.99]])\n  >>> valid_scores\n  array([[1. ,  0.93,  1. ,  1. ,  0.96],\n         [1. ,  0.96,  1. ,  1. ,  0.96],\n         [1. ,  0.96,  1. ,  1. ,  0.96]])\n\nIf you intend to plot the learning curves only, the class\n:class:`~sklearn.model_selection.LearningCurveDisplay` will be easier to use.\nYou can use the method\n:meth:`~sklearn.model_selection.LearningCurveDisplay.from_estimator` similarly\nto :func:`learning_curve` to generate and plot the learning curve:\n\n.. plot::\n   :context: close-figs\n   :align: center\n\n      from sklearn.datasets import load_iris\n      from sklearn.model_selection import LearningCurveDisplay\n      from sklearn.svm import SVC\n      from sklearn.utils import shuffle\n      X, y = load_iris(return_X_y=True)\n      X, y = shuffle(X, y, random_state=0)\n      LearningCurveDisplay.from_estimator(\n         SVC(kernel=\"linear\"), X, y, train_sizes=[50, 80, 110], cv=5)\n\n.. rubric:: Examples\n\n* See :ref:`sphx_glr_auto_examples_model_selection_plot_learning_curve.py` for an\n  example of using learning curves to check the scalability of a predictive model."
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_0",
    "header": ".. _linear_model:",
    "text": ".. _linear_model:\n\n============="
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_1",
    "header": "Linear Models",
    "text": "Linear Models\n=============\n\n.. currentmodule:: sklearn.linear_model\n\nThe following are a set of methods intended for regression in which\nthe target value is expected to be a linear combination of the features.\nIn mathematical notation, if :math:`\\hat{y}` is the predicted\nvalue.\n\n.. math::    \\hat{y}(w, x) = w_0 + w_1 x_1 + ... + w_p x_p\n\nAcross the module, we designate the vector :math:`w = (w_1,\n..., w_p)` as ``coef_`` and :math:`w_0` as ``intercept_``.\n\nTo perform classification with generalized linear models, see\n:ref:`Logistic_regression`.\n\n.. _ordinary_least_squares:"
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_2",
    "header": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\n=======================\n\n:class:`LinearRegression` fits a linear model with coefficients\n:math:`w = (w_1, ..., w_p)` to minimize the residual sum\nof squares between the observed targets in the dataset, and the\ntargets predicted by the linear approximation. Mathematically it\nsolves a problem of the form:\n\n.. math:: \\min_{w} || X w - y||_2^2\n\n.. figure:: ../auto_examples/linear_model/images/sphx_glr_plot_ols_ridge_001.png\n   :target: ../auto_examples/linear_model/plot_ols_ridge.html\n   :align: center\n   :scale: 50%\n\n:class:`LinearRegression` takes in its ``fit`` method arguments ``X``, ``y``,\n``sample_weight`` and stores the coefficients :math:`w` of the linear model in its\n``coef_`` and ``intercept_`` attributes::\n\n    >>> from sklearn import linear_model\n    >>> reg = linear_model.LinearRegression()\n    >>> reg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])\n    LinearRegression()\n    >>> reg.coef_\n    array([0.5, 0.5])\n    >>> reg.intercept_\n    0.0\n\nThe coefficient estimates for Ordinary Least Squares rely on the\nindependence of the features. When features are correlated and some\ncolumns of the design matrix :math:`X` have an approximately linear\ndependence, the design matrix becomes close to singular\nand as a result, the least-squares estimate becomes highly sensitive\nto random errors in the observed target, producing a large\nvariance. This situation of *multicollinearity* can arise, for\nexample, when data are collected without an experimental design.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_linear_model_plot_ols_ridge.py`"
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_3",
    "header": "Non-Negative Least Squares",
    "text": "Non-Negative Least Squares\n--------------------------\n\nIt is possible to constrain all the coefficients to be non-negative, which may\nbe useful when they represent some physical or naturally non-negative\nquantities (e.g., frequency counts or prices of goods).\n:class:`LinearRegression` accepts a boolean ``positive``\nparameter: when set to `True` `Non-Negative Least Squares\n<https://en.wikipedia.org/wiki/Non-negative_least_squares>`_ are then applied.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_linear_model_plot_nnls.py`"
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_4",
    "header": "Ordinary Least Squares Complexity",
    "text": "Ordinary Least Squares Complexity\n---------------------------------\n\nThe least squares solution is computed using the singular value\ndecomposition of :math:`X`. If :math:`X` is a matrix of shape `(n_samples, n_features)`\nthis method has a cost of\n:math:`O(n_{\\text{samples}} n_{\\text{features}}^2)`, assuming that\n:math:`n_{\\text{samples}} \\geq n_{\\text{features}}`.\n\n.. _ridge_regression:"
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_5",
    "header": "Ridge regression and classification",
    "text": "Ridge regression and classification\n==================================="
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_6",
    "header": "Regression",
    "text": "Regression\n----------\n\n:class:`Ridge` regression addresses some of the problems of\n:ref:`ordinary_least_squares` by imposing a penalty on the size of the\ncoefficients. The ridge coefficients minimize a penalized residual sum\nof squares:\n\n\n.. math::\n\n   \\min_{w} || X w - y||_2^2 + \\alpha ||w||_2^2\n\n\nThe complexity parameter :math:`\\alpha \\geq 0` controls the amount\nof shrinkage: the larger the value of :math:`\\alpha`, the greater the amount\nof shrinkage and thus the coefficients become more robust to collinearity.\n\n.. figure:: ../auto_examples/linear_model/images/sphx_glr_plot_ridge_path_001.png\n   :target: ../auto_examples/linear_model/plot_ridge_path.html\n   :align: center\n   :scale: 50%\n\n\nAs with other linear models, :class:`Ridge` will take in its ``fit`` method\narrays ``X``, ``y`` and will store the coefficients :math:`w` of the linear model in\nits ``coef_`` member::\n\n    >>> from sklearn import linear_model\n    >>> reg = linear_model.Ridge(alpha=.5)\n    >>> reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])\n    Ridge(alpha=0.5)\n    >>> reg.coef_\n    array([0.34545455, 0.34545455])\n    >>> reg.intercept_\n    np.float64(0.13636)\n\nNote that the class :class:`Ridge` allows for the user to specify that the\nsolver be automatically chosen by setting `solver=\"auto\"`. When this option\nis specified, :class:`Ridge` will choose between the `\"lbfgs\"`, `\"cholesky\"`,\nand `\"sparse_cg\"` solvers. :class:`Ridge` will begin checking the conditions\nshown in the following table from top to bottom. If the condition is true,\nthe corresponding solver is chosen.\n\n+-------------+----------------------------------------------------+\n| **Solver**  | **Condition**                                      |\n+-------------+----------------------------------------------------+\n| 'lbfgs'     | The ``positive=True`` option is specified.         |\n+-------------+----------------------------------------------------+\n| 'cholesky'  | The input array X is not sparse.                   |\n+-------------+----------------------------------------------------+\n| 'sparse_cg' | None of the above conditions are fulfilled.        |\n+-------------+----------------------------------------------------+\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_linear_model_plot_ols_ridge.py`\n* :ref:`sphx_glr_auto_examples_linear_model_plot_ridge_path.py`\n* :ref:`sphx_glr_auto_examples_inspection_plot_linear_model_coefficient_interpretation.py`"
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_7",
    "header": "Classification",
    "text": "Classification\n--------------\n\nThe :class:`Ridge` regressor has a classifier variant:\n:class:`RidgeClassifier`. This classifier first converts binary targets to\n``{-1, 1}`` and then treats the problem as a regression task, optimizing the\nsame objective as above. The predicted class corresponds to the sign of the\nregressor's prediction. For multiclass classification, the problem is\ntreated as multi-output regression, and the predicted class corresponds to\nthe output with the highest value.\n\nIt might seem questionable to use a (penalized) Least Squares loss to fit a\nclassification model instead of the more traditional logistic or hinge\nlosses. However, in practice, all those models can lead to similar\ncross-validation scores in terms of accuracy or precision/recall, while the\npenalized least squares loss used by the :class:`RidgeClassifier` allows for\na very different choice of the numerical solvers with distinct computational\nperformance profiles.\n\nThe :class:`RidgeClassifier` can be significantly faster than e.g.\n:class:`LogisticRegression` with a high number of classes because it can\ncompute the projection matrix :math:`(X^T X)^{-1} X^T` only once.\n\nThis classifier is sometimes referred to as a `Least Squares Support Vector\nMachine\n<https://en.wikipedia.org/wiki/Least-squares_support-vector_machine>`_ with\na linear kernel.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`"
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_8",
    "header": "Ridge Complexity",
    "text": "Ridge Complexity\n----------------\n\nThis method has the same order of complexity as\n:ref:`ordinary_least_squares`.\n\n.. FIXME:\n.. Not completely true: OLS is solved by an SVD, while Ridge is solved by\n.. the method of normal equations (Cholesky), there is a big flop difference\n.. between these"
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_9",
    "header": "Setting the regularization parameter: leave-one-out Cross-Validation",
    "text": "Setting the regularization parameter: leave-one-out Cross-Validation\n--------------------------------------------------------------------\n\n:class:`RidgeCV` and :class:`RidgeClassifierCV` implement ridge\nregression/classification with built-in cross-validation of the alpha parameter.\nThey work in the same way as :class:`~sklearn.model_selection.GridSearchCV` except\nthat it defaults to efficient Leave-One-Out :term:`cross-validation`.\nWhen using the default :term:`cross-validation`, alpha cannot be 0 due to the\nformulation used to calculate Leave-One-Out error. See [RL2007]_ for details.\n\nUsage example::\n\n    >>> import numpy as np\n    >>> from sklearn import linear_model\n    >>> reg = linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13))\n    >>> reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])\n    RidgeCV(alphas=array([1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01,\n          1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06]))\n    >>> reg.alpha_\n    np.float64(0.01)\n\nSpecifying the value of the :term:`cv` attribute will trigger the use of\ncross-validation with :class:`~sklearn.model_selection.GridSearchCV`, for\nexample `cv=10` for 10-fold cross-validation, rather than Leave-One-Out\nCross-Validation.\n\n.. dropdown:: References\n\n  .. [RL2007] \"Notes on Regularized Least Squares\", Rifkin & Lippert (`technical report\n    <http://cbcl.mit.edu/publications/ps/MIT-CSAIL-TR-2007-025.pdf>`_,\n    `course slides <https://www.mit.edu/~9.520/spring07/Classes/rlsslides.pdf>`_).\n\n.. _lasso:"
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_10",
    "header": "Lasso",
    "text": "Lasso\n=====\n\nThe :class:`Lasso` is a linear model that estimates sparse coefficients.\nIt is useful in some contexts due to its tendency to prefer solutions\nwith fewer non-zero coefficients, effectively reducing the number of\nfeatures upon which the given solution is dependent. For this reason,\nLasso and its variants are fundamental to the field of compressed sensing.\nUnder certain conditions, it can recover the exact set of non-zero\ncoefficients (see\n:ref:`sphx_glr_auto_examples_applications_plot_tomography_l1_reconstruction.py`).\n\nMathematically, it consists of a linear model with an added regularization term.\nThe objective function to minimize is:\n\n.. math::  \\min_{w} { \\frac{1}{2n_{\\text{samples}}} ||X w - y||_2 ^ 2 + \\alpha ||w||_1}\n\nThe lasso estimate thus solves the minimization of the\nleast-squares penalty with :math:`\\alpha ||w||_1` added, where\n:math:`\\alpha` is a constant and :math:`||w||_1` is the :math:`\\ell_1`-norm of\nthe coefficient vector.\n\nThe implementation in the class :class:`Lasso` uses coordinate descent as\nthe algorithm to fit the coefficients. See :ref:`least_angle_regression`\nfor another implementation::\n\n    >>> from sklearn import linear_model\n    >>> reg = linear_model.Lasso(alpha=0.1)\n    >>> reg.fit([[0, 0], [1, 1]], [0, 1])\n    Lasso(alpha=0.1)\n    >>> reg.predict([[1, 1]])\n    array([0.8])\n\nThe function :func:`lasso_path` is useful for lower-level tasks, as it\ncomputes the coefficients along the full path of possible values.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_linear_model_plot_lasso_and_elasticnet.py`\n* :ref:`sphx_glr_auto_examples_applications_plot_tomography_l1_reconstruction.py`\n* :ref:`sphx_glr_auto_examples_inspection_plot_linear_model_coefficient_interpretation.py`\n\n\n.. note:: **Feature selection with Lasso**\n\n      As the Lasso regression yields sparse models, it can\n      thus be used to perform feature selection, as detailed in\n      :ref:`l1_feature_selection`.\n\n.. dropdown:: References\n\n  The following two references explain the iterations\n  used in the coordinate descent solver of scikit-learn, as well as\n  the duality gap computation used for convergence control.\n\n  * \"Regularization Path For Generalized linear Models by Coordinate Descent\",\n    Friedman, Hastie & Tibshirani, J Stat Softw, 2010 (`Paper\n    <https://www.jstatsoft.org/article/view/v033i01/v33i01.pdf>`__).\n  * \"An Interior-Point Method for Large-Scale L1-Regularized Least Squares,\"\n    S. J. Kim, K. Koh, M. Lustig, S. Boyd and D. Gorinevsky,\n    in IEEE Journal of Selected Topics in Signal Processing, 2007\n    (`Paper <https://web.stanford.edu/~boyd/papers/pdf/l1_ls.pdf>`__)"
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_11",
    "header": "Setting regularization parameter",
    "text": "Setting regularization parameter\n--------------------------------\n\nThe ``alpha`` parameter controls the degree of sparsity of the estimated\ncoefficients."
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_12",
    "header": "Using cross-validation",
    "text": "Using cross-validation\n^^^^^^^^^^^^^^^^^^^^^^^\n\nscikit-learn exposes objects that set the Lasso ``alpha`` parameter by\ncross-validation: :class:`LassoCV` and :class:`LassoLarsCV`.\n:class:`LassoLarsCV` is based on the :ref:`least_angle_regression` algorithm\nexplained below.\n\nFor high-dimensional datasets with many collinear features,\n:class:`LassoCV` is most often preferable. However, :class:`LassoLarsCV` has\nthe advantage of exploring more relevant values of `alpha` parameter, and\nif the number of samples is very small compared to the number of\nfeatures, it is often faster than :class:`LassoCV`.\n\n.. |lasso_cv_1| image:: ../auto_examples/linear_model/images/sphx_glr_plot_lasso_model_selection_002.png\n    :target: ../auto_examples/linear_model/plot_lasso_model_selection.html\n    :scale: 48%\n\n.. |lasso_cv_2| image:: ../auto_examples/linear_model/images/sphx_glr_plot_lasso_model_selection_003.png\n    :target: ../auto_examples/linear_model/plot_lasso_model_selection.html\n    :scale: 48%\n\n.. centered:: |lasso_cv_1| |lasso_cv_2|\n\n.. _lasso_lars_ic:"
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_13",
    "header": "Information-criteria based model selection",
    "text": "Information-criteria based model selection\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nAlternatively, the estimator :class:`LassoLarsIC` proposes to use the\nAkaike information criterion (AIC) and the Bayes Information criterion (BIC).\nIt is a computationally cheaper alternative to find the optimal value of alpha\nas the regularization path is computed only once instead of k+1 times\nwhen using k-fold cross-validation.\n\nIndeed, these criteria are computed on the in-sample training set. In short,\nthey penalize the over-optimistic scores of the different Lasso models by\ntheir flexibility (cf. to \"Mathematical details\" section below).\n\nHowever, such criteria need a proper estimation of the degrees of freedom of\nthe solution, are derived for large samples (asymptotic results) and assume the\ncorrect model is candidates under investigation. They also tend to break when\nthe problem is badly conditioned (e.g. more features than samples).\n\n.. figure:: ../auto_examples/linear_model/images/sphx_glr_plot_lasso_lars_ic_001.png\n    :target: ../auto_examples/linear_model/plot_lasso_lars_ic.html\n    :align: center\n    :scale: 50%\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py`\n* :ref:`sphx_glr_auto_examples_linear_model_plot_lasso_lars_ic.py`\n\n.. _aic_bic:"
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_14",
    "header": "AIC and BIC criteria",
    "text": "AIC and BIC criteria\n^^^^^^^^^^^^^^^^^^^^\n\nThe definition of AIC (and thus BIC) might differ in the literature. In this\nsection, we give more information regarding the criterion computed in\nscikit-learn.\n\n.. dropdown:: Mathematical details\n\n  The AIC criterion is defined as:\n\n  .. math::\n      AIC = -2 \\log(\\hat{L}) + 2 d\n\n  where :math:`\\hat{L}` is the maximum likelihood of the model and\n  :math:`d` is the number of parameters (as well referred to as degrees of\n  freedom in the previous section).\n\n  The definition of BIC replaces the constant :math:`2` by :math:`\\log(N)`:\n\n  .. math::\n      BIC = -2 \\log(\\hat{L}) + \\log(N) d\n\n  where :math:`N` is the number of samples.\n\n  For a linear Gaussian model, the maximum log-likelihood is defined as:\n\n  .. math::\n      \\log(\\hat{L}) = - \\frac{n}{2} \\log(2 \\pi) - \\frac{n}{2} \\log(\\sigma^2) - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{2\\sigma^2}\n\n  where :math:`\\sigma^2` is an estimate of the noise variance,\n  :math:`y_i` and :math:`\\hat{y}_i` are respectively the true and predicted\n  targets, and :math:`n` is the number of samples.\n\n  Plugging the maximum log-likelihood in the AIC formula yields:\n\n  .. math::\n      AIC = n \\log(2 \\pi \\sigma^2) + \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sigma^2} + 2 d\n\n  The first term of the above expression is sometimes discarded since it is a\n  constant when :math:`\\sigma^2` is provided. In addition,\n  it is sometimes stated that the AIC is equivalent to the :math:`C_p` statistic\n  [12]_. In a strict sense, however, it is equivalent only up to some constant\n  and a multiplicative factor.\n\n  At last, we mentioned above that :math:`\\sigma^2` is an estimate of the\n  noise variance. In :class:`LassoLarsIC` when the parameter `noise_variance` is\n  not provided (default), the noise variance is estimated via the unbiased\n  estimator [13]_ defined as:\n\n  .. math::\n      \\sigma^2 = \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{n - p}\n\n  where :math:`p` is the number of features and :math:`\\hat{y}_i` is the\n  predicted target using an ordinary least squares regression. Note, that this\n  formula is valid only when `n_samples > n_features`.\n\n  .. rubric:: References\n\n  .. [12] :arxiv:`Zou, Hui, Trevor Hastie, and Robert Tibshirani.\n          \"On the degrees of freedom of the lasso.\"\n          The Annals of Statistics 35.5 (2007): 2173-2192.\n          <0712.0881.pdf>`\n\n  .. [13] :doi:`Cherkassky, Vladimir, and Yunqian Ma.\n          \"Comparison of model selection for regression.\"\n          Neural computation 15.7 (2003): 1691-1714.\n          <10.1162/089976603321891864>`"
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_15",
    "header": "Comparison with the regularization parameter of SVM",
    "text": "Comparison with the regularization parameter of SVM\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe equivalence between ``alpha`` and the regularization parameter of SVM,\n``C`` is given by ``alpha = 1 / C`` or ``alpha = 1 / (n_samples * C)``,\ndepending on the estimator and the exact objective function optimized by the\nmodel.\n\n.. _multi_task_lasso:"
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_16",
    "header": "Multi-task Lasso",
    "text": "Multi-task Lasso\n================\n\nThe :class:`MultiTaskLasso` is a linear model that estimates sparse\ncoefficients for multiple regression problems jointly: ``y`` is a 2D array,\nof shape ``(n_samples, n_tasks)``. The constraint is that the selected\nfeatures are the same for all the regression problems, also called tasks.\n\nThe following figure compares the location of the non-zero entries in the\ncoefficient matrix W obtained with a simple Lasso or a MultiTaskLasso.\nThe Lasso estimates yield scattered non-zeros while the non-zeros of\nthe MultiTaskLasso are full columns.\n\n.. |multi_task_lasso_1| image:: ../auto_examples/linear_model/images/sphx_glr_plot_multi_task_lasso_support_001.png\n    :target: ../auto_examples/linear_model/plot_multi_task_lasso_support.html\n    :scale: 48%\n\n.. |multi_task_lasso_2| image:: ../auto_examples/linear_model/images/sphx_glr_plot_multi_task_lasso_support_002.png\n    :target: ../auto_examples/linear_model/plot_multi_task_lasso_support.html\n    :scale: 48%\n\n.. centered:: |multi_task_lasso_1| |multi_task_lasso_2|\n\n.. centered:: Fitting a time-series model, imposing that any active feature be active at all times.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_linear_model_plot_multi_task_lasso_support.py`\n\n\n.. dropdown:: Mathematical details\n\n  Mathematically, it consists of a linear model trained with a mixed\n  :math:`\\ell_1` :math:`\\ell_2`-norm for regularization.\n  The objective function to minimize is:\n\n  .. math::  \\min_{W} { \\frac{1}{2n_{\\text{samples}}} ||X W - Y||_{\\text{Fro}} ^ 2 + \\alpha ||W||_{21}}\n\n  where :math:`\\text{Fro}` indicates the Frobenius norm\n\n  .. math:: ||A||_{\\text{Fro}} = \\sqrt{\\sum_{ij} a_{ij}^2}\n\n  and :math:`\\ell_1` :math:`\\ell_2` reads\n\n  .. math:: ||A||_{2 1} = \\sum_i \\sqrt{\\sum_j a_{ij}^2}.\n\n  The implementation in the class :class:`MultiTaskLasso` uses\n  coordinate descent as the algorithm to fit the coefficients.\n\n.. _elastic_net:"
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_17",
    "header": "Elastic-Net",
    "text": "Elastic-Net\n===========\n:class:`ElasticNet` is a linear regression model trained with both\n:math:`\\ell_1` and :math:`\\ell_2`-norm regularization of the coefficients.\nThis combination  allows for learning a sparse model where few of\nthe weights are non-zero like :class:`Lasso`, while still maintaining\nthe regularization properties of :class:`Ridge`. We control the convex\ncombination of :math:`\\ell_1` and :math:`\\ell_2` using the ``l1_ratio``\nparameter.\n\nElastic-net is useful when there are multiple features that are\ncorrelated with one another. Lasso is likely to pick one of these\nat random, while elastic-net is likely to pick both.\n\nA practical advantage of trading-off between Lasso and Ridge is that it\nallows Elastic-Net to inherit some of Ridge's stability under rotation.\n\nThe objective function to minimize is in this case\n\n.. math::\n\n    \\min_{w} { \\frac{1}{2n_{\\text{samples}}} ||X w - y||_2 ^ 2 + \\alpha \\rho ||w||_1 +\n    \\frac{\\alpha(1-\\rho)}{2} ||w||_2 ^ 2}\n\n\n.. figure:: ../auto_examples/linear_model/images/sphx_glr_plot_lasso_lasso_lars_elasticnet_path_002.png\n   :target: ../auto_examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.html\n   :align: center\n   :scale: 50%\n\nThe class :class:`ElasticNetCV` can be used to set the parameters\n``alpha`` (:math:`\\alpha`) and ``l1_ratio`` (:math:`\\rho`) by cross-validation.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_linear_model_plot_lasso_and_elasticnet.py`\n* :ref:`sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py`\n* :ref:`sphx_glr_auto_examples_linear_model_plot_elastic_net_precomputed_gram_matrix_with_weighted_samples.py`\n\n.. dropdown:: References\n\n  The following two references explain the iterations\n  used in the coordinate descent solver of scikit-learn, as well as\n  the duality gap computation used for convergence control.\n\n  * \"Regularization Path For Generalized linear Models by Coordinate Descent\",\n    Friedman, Hastie & Tibshirani, J Stat Softw, 2010 (`Paper\n    <https://www.jstatsoft.org/article/view/v033i01/v33i01.pdf>`__).\n  * \"An Interior-Point Method for Large-Scale L1-Regularized Least Squares,\"\n    S. J. Kim, K. Koh, M. Lustig, S. Boyd and D. Gorinevsky,\n    in IEEE Journal of Selected Topics in Signal Processing, 2007\n    (`Paper <https://web.stanford.edu/~boyd/papers/pdf/l1_ls.pdf>`__)\n\n.. _multi_task_elastic_net:"
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_18",
    "header": "Multi-task Elastic-Net",
    "text": "Multi-task Elastic-Net\n======================\n\nThe :class:`MultiTaskElasticNet` is an elastic-net model that estimates sparse\ncoefficients for multiple regression problems jointly: ``Y`` is a 2D array\nof shape ``(n_samples, n_tasks)``. The constraint is that the selected\nfeatures are the same for all the regression problems, also called tasks.\n\nMathematically, it consists of a linear model trained with a mixed\n:math:`\\ell_1` :math:`\\ell_2`-norm and :math:`\\ell_2`-norm for regularization.\nThe objective function to minimize is:\n\n.. math::\n\n    \\min_{W} { \\frac{1}{2n_{\\text{samples}}} ||X W - Y||_{\\text{Fro}}^2 + \\alpha \\rho ||W||_{2 1} +\n    \\frac{\\alpha(1-\\rho)}{2} ||W||_{\\text{Fro}}^2}\n\nThe implementation in the class :class:`MultiTaskElasticNet` uses coordinate descent as\nthe algorithm to fit the coefficients.\n\nThe class :class:`MultiTaskElasticNetCV` can be used to set the parameters\n``alpha`` (:math:`\\alpha`) and ``l1_ratio`` (:math:`\\rho`) by cross-validation.\n\n.. _least_angle_regression:"
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_19",
    "header": "Least Angle Regression",
    "text": "Least Angle Regression\n======================\n\nLeast-angle regression (LARS) is a regression algorithm for\nhigh-dimensional data, developed by Bradley Efron, Trevor Hastie, Iain\nJohnstone and Robert Tibshirani. LARS is similar to forward stepwise\nregression. At each step, it finds the feature most correlated with the\ntarget. When there are multiple features having equal correlation, instead\nof continuing along the same feature, it proceeds in a direction equiangular\nbetween the features.\n\nThe advantages of LARS are:\n\n- It is numerically efficient in contexts where the number of features\n  is significantly greater than the number of samples.\n\n- It is computationally just as fast as forward selection and has\n  the same order of complexity as ordinary least squares.\n\n- It produces a full piecewise linear solution path, which is\n  useful in cross-validation or similar attempts to tune the model.\n\n- If two features are almost equally correlated with the target,\n  then their coefficients should increase at approximately the same\n  rate. The algorithm thus behaves as intuition would expect, and\n  also is more stable.\n\n- It is easily modified to produce solutions for other estimators,\n  like the Lasso.\n\nThe disadvantages of the LARS method include:\n\n- Because LARS is based upon an iterative refitting of the\n  residuals, it would appear to be especially sensitive to the\n  effects of noise. This problem is discussed in detail by Weisberg\n  in the discussion section of the Efron et al. (2004) Annals of\n  Statistics article.\n\nThe LARS model can be used via the estimator :class:`Lars`, or its\nlow-level implementation :func:`lars_path` or :func:`lars_path_gram`."
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_20",
    "header": "LARS Lasso",
    "text": "LARS Lasso\n==========\n\n:class:`LassoLars` is a lasso model implemented using the LARS\nalgorithm, and unlike the implementation based on coordinate descent,\nthis yields the exact solution, which is piecewise linear as a\nfunction of the norm of its coefficients.\n\n.. figure:: ../auto_examples/linear_model/images/sphx_glr_plot_lasso_lasso_lars_elasticnet_path_001.png\n   :target: ../auto_examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.html\n   :align: center\n   :scale: 50%\n\n::\n\n   >>> from sklearn import linear_model\n   >>> reg = linear_model.LassoLars(alpha=.1)\n   >>> reg.fit([[0, 0], [1, 1]], [0, 1])\n   LassoLars(alpha=0.1)\n   >>> reg.coef_\n   array([0.6, 0.        ])\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py`\n\nThe LARS algorithm provides the full path of the coefficients along\nthe regularization parameter almost for free, thus a common operation\nis to retrieve the path with one of the functions :func:`lars_path`\nor :func:`lars_path_gram`.\n\n.. dropdown:: Mathematical formulation\n\n  The algorithm is similar to forward stepwise regression, but instead\n  of including features at each step, the estimated coefficients are\n  increased in a direction equiangular to each one's correlations with\n  the residual.\n\n  Instead of giving a vector result, the LARS solution consists of a\n  curve denoting the solution for each value of the :math:`\\ell_1` norm of the\n  parameter vector. The full coefficients path is stored in the array\n  ``coef_path_`` of shape `(n_features, max_features + 1)`. The first\n  column is always zero.\n\n  .. rubric:: References\n\n  * Original Algorithm is detailed in the paper `Least Angle Regression\n    <https://hastie.su.domains/Papers/LARS/LeastAngle_2002.pdf>`_\n    by Hastie et al.\n\n.. _omp:"
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_21",
    "header": "Orthogonal Matching Pursuit (OMP)",
    "text": "Orthogonal Matching Pursuit (OMP)\n=================================\n:class:`OrthogonalMatchingPursuit` and :func:`orthogonal_mp` implement the OMP\nalgorithm for approximating the fit of a linear model with constraints imposed\non the number of non-zero coefficients (i.e. the :math:`\\ell_0` pseudo-norm).\n\nBeing a forward feature selection method like :ref:`least_angle_regression`,\northogonal matching pursuit can approximate the optimum solution vector with a\nfixed number of non-zero elements:\n\n.. math::\n    \\underset{w}{\\operatorname{arg\\,min\\,}}  ||y - Xw||_2^2 \\text{ subject to } ||w||_0 \\leq n_{\\text{nonzero_coefs}}\n\nAlternatively, orthogonal matching pursuit can target a specific error instead\nof a specific number of non-zero coefficients. This can be expressed as:\n\n.. math::\n    \\underset{w}{\\operatorname{arg\\,min\\,}} ||w||_0 \\text{ subject to } ||y-Xw||_2^2 \\leq \\text{tol}\n\n\nOMP is based on a greedy algorithm that includes at each step the atom most\nhighly correlated with the current residual. It is similar to the simpler\nmatching pursuit (MP) method, but better in that at each iteration, the\nresidual is recomputed using an orthogonal projection on the space of the\npreviously chosen dictionary elements.\n\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_linear_model_plot_omp.py`\n\n.. dropdown:: References\n\n  * https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf\n\n  * `Matching pursuits with time-frequency dictionaries\n    <https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf>`_,\n    S. G. Mallat, Z. Zhang,\n\n.. _bayesian_regression:"
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_22",
    "header": "Bayesian Regression",
    "text": "Bayesian Regression\n===================\n\nBayesian regression techniques can be used to include regularization\nparameters in the estimation procedure: the regularization parameter is\nnot set in a hard sense but tuned to the data at hand.\n\nThis can be done by introducing `uninformative priors\n<https://en.wikipedia.org/wiki/Non-informative_prior#Uninformative_priors>`__\nover the hyper parameters of the model.\nThe :math:`\\ell_{2}` regularization used in :ref:`ridge_regression` is\nequivalent to finding a maximum a posteriori estimation under a Gaussian prior\nover the coefficients :math:`w` with precision :math:`\\lambda^{-1}`.\nInstead of setting `\\lambda` manually, it is possible to treat it as a random\nvariable to be estimated from the data.\n\nTo obtain a fully probabilistic model, the output :math:`y` is assumed\nto be Gaussian distributed around :math:`X w`:\n\n.. math::  p(y|X,w,\\alpha) = \\mathcal{N}(y|X w,\\alpha^{-1})\n\nwhere :math:`\\alpha` is again treated as a random variable that is to be\nestimated from the data.\n\nThe advantages of Bayesian Regression are:\n\n- It adapts to the data at hand.\n\n- It can be used to include regularization parameters in the\n  estimation procedure.\n\nThe disadvantages of Bayesian regression include:\n\n- Inference of the model can be time consuming.\n\n.. dropdown:: References\n\n  * A good introduction to Bayesian methods is given in C. Bishop: Pattern\n    Recognition and Machine learning\n\n  * Original Algorithm is detailed in the  book `Bayesian learning for neural\n    networks` by Radford M. Neal\n\n.. _bayesian_ridge_regression:"
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_23",
    "header": "Bayesian Ridge Regression",
    "text": "Bayesian Ridge Regression\n-------------------------\n\n:class:`BayesianRidge` estimates a probabilistic model of the\nregression problem as described above.\nThe prior for the coefficient :math:`w` is given by a spherical Gaussian:\n\n.. math:: p(w|\\lambda) =\n    \\mathcal{N}(w|0,\\lambda^{-1}\\mathbf{I}_{p})\n\nThe priors over :math:`\\alpha` and :math:`\\lambda` are chosen to be `gamma\ndistributions <https://en.wikipedia.org/wiki/Gamma_distribution>`__, the\nconjugate prior for the precision of the Gaussian. The resulting model is\ncalled *Bayesian Ridge Regression*, and is similar to the classical\n:class:`Ridge`.\n\nThe parameters :math:`w`, :math:`\\alpha` and :math:`\\lambda` are estimated\njointly during the fit of the model, the regularization parameters\n:math:`\\alpha` and :math:`\\lambda` being estimated by maximizing the\n*log marginal likelihood*. The scikit-learn implementation\nis based on the algorithm described in Appendix A of (Tipping, 2001)\nwhere the update of the parameters :math:`\\alpha` and :math:`\\lambda` is done\nas suggested in (MacKay, 1992). The initial value of the maximization procedure\ncan be set with the hyperparameters ``alpha_init`` and ``lambda_init``.\n\nThere are four more hyperparameters, :math:`\\alpha_1`, :math:`\\alpha_2`,\n:math:`\\lambda_1` and :math:`\\lambda_2` of the gamma prior distributions over\n:math:`\\alpha` and :math:`\\lambda`. These are usually chosen to be\n*non-informative*. By default :math:`\\alpha_1 = \\alpha_2 =  \\lambda_1 = \\lambda_2 = 10^{-6}`.\n\nBayesian Ridge Regression is used for regression::\n\n    >>> from sklearn import linear_model\n    >>> X = [[0., 0.], [1., 1.], [2., 2.], [3., 3.]]\n    >>> Y = [0., 1., 2., 3.]\n    >>> reg = linear_model.BayesianRidge()\n    >>> reg.fit(X, Y)\n    BayesianRidge()\n\nAfter being fitted, the model can then be used to predict new values::\n\n    >>> reg.predict([[1, 0.]])\n    array([0.50000013])\n\nThe coefficients :math:`w` of the model can be accessed::\n\n    >>> reg.coef_\n    array([0.49999993, 0.49999993])\n\nDue to the Bayesian framework, the weights found are slightly different from the\nones found by :ref:`ordinary_least_squares`. However, Bayesian Ridge Regression\nis more robust to ill-posed problems.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_linear_model_plot_bayesian_ridge_curvefit.py`\n\n.. dropdown:: References\n\n  * Section 3.3 in Christopher M. Bishop: Pattern Recognition and Machine Learning, 2006\n\n  * David J. C. MacKay, `Bayesian Interpolation <https://citeseerx.ist.psu.edu/doc_view/pid/b14c7cc3686e82ba40653c6dff178356a33e5e2c>`_, 1992.\n\n  * Michael E. Tipping, `Sparse Bayesian Learning and the Relevance Vector Machine <https://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf>`_, 2001.\n\n.. _automatic_relevance_determination:"
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_24",
    "header": "Automatic Relevance Determination - ARD",
    "text": "Automatic Relevance Determination - ARD\n---------------------------------------\n\nThe Automatic Relevance Determination (as being implemented in\n:class:`ARDRegression`) is a kind of linear model which is very similar to the\n`Bayesian Ridge Regression`_, but that leads to sparser coefficients :math:`w`\n[1]_ [2]_.\n\n:class:`ARDRegression` poses a different prior over :math:`w`: it drops\nthe spherical Gaussian distribution for a centered elliptic Gaussian\ndistribution. This means each coefficient :math:`w_{i}` can itself be drawn from\na Gaussian distribution, centered on zero and with a precision\n:math:`\\lambda_{i}`:\n\n.. math:: p(w|\\lambda) = \\mathcal{N}(w|0,A^{-1})\n\nwith :math:`A` being a positive definite diagonal matrix and\n:math:`\\text{diag}(A) = \\lambda = \\{\\lambda_{1},...,\\lambda_{p}\\}`.\n\nIn contrast to the `Bayesian Ridge Regression`_, each coordinate of\n:math:`w_{i}` has its own standard deviation :math:`\\frac{1}{\\lambda_i}`. The\nprior over all :math:`\\lambda_i` is chosen to be the same gamma distribution\ngiven by the hyperparameters :math:`\\lambda_1` and :math:`\\lambda_2`.\n\nARD is also known in the literature as *Sparse Bayesian Learning* and *Relevance\nVector Machine* [3]_ [4]_.\n\nSee :ref:`sphx_glr_auto_examples_linear_model_plot_ard.py` for a worked-out comparison between ARD and `Bayesian Ridge Regression`_.\n\nSee :ref:`sphx_glr_auto_examples_linear_model_plot_lasso_and_elasticnet.py` for a comparison between various methods - Lasso, ARD and ElasticNet - on correlated data.\n\n.. rubric:: References\n\n.. [1] Christopher M. Bishop: Pattern Recognition and Machine Learning, Chapter 7.2.1\n\n.. [2] David Wipf and Srikantan Nagarajan: `A New View of Automatic Relevance Determination <https://papers.nips.cc/paper/3372-a-new-view-of-automatic-relevance-determination.pdf>`_\n\n.. [3] Michael E. Tipping: `Sparse Bayesian Learning and the Relevance Vector Machine <https://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf>`_\n\n.. [4] Tristan Fletcher: `Relevance Vector Machines Explained <https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=3dc9d625404fdfef6eaccc3babddefe4c176abd4>`_\n\n.. _Logistic_regression:"
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_25",
    "header": "Logistic regression",
    "text": "Logistic regression\n===================\n\nThe logistic regression is implemented in :class:`LogisticRegression`. Despite\nits name, it is implemented as a linear model for classification rather than\nregression in terms of the scikit-learn/ML nomenclature. The logistic\nregression is also known in the literature as logit regression,\nmaximum-entropy classification (MaxEnt) or the log-linear classifier. In this\nmodel, the probabilities describing the possible outcomes of a single trial\nare modeled using a `logistic function\n<https://en.wikipedia.org/wiki/Logistic_function>`_.\n\nThis implementation can fit binary, One-vs-Rest, or multinomial logistic\nregression with optional :math:`\\ell_1`, :math:`\\ell_2` or Elastic-Net\nregularization.\n\n.. note:: **Regularization**\n\n    Regularization is applied by default, which is common in machine\n    learning but not in statistics. Another advantage of regularization is\n    that it improves numerical stability. No regularization amounts to\n    setting C to a very high value.\n\n.. note:: **Logistic Regression as a special case of the Generalized Linear Models (GLM)**\n\n    Logistic regression is a special case of\n    :ref:`generalized_linear_models` with a Binomial / Bernoulli conditional\n    distribution and a Logit link. The numerical output of the logistic\n    regression, which is the predicted probability, can be used as a classifier\n    by applying a threshold (by default 0.5) to it. This is how it is\n    implemented in scikit-learn, so it expects a categorical target, making\n    the Logistic Regression a classifier.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_linear_model_plot_logistic_l1_l2_sparsity.py`\n* :ref:`sphx_glr_auto_examples_linear_model_plot_logistic_path.py`\n* :ref:`sphx_glr_auto_examples_linear_model_plot_logistic_multinomial.py`\n* :ref:`sphx_glr_auto_examples_linear_model_plot_sparse_logistic_regression_20newsgroups.py`\n* :ref:`sphx_glr_auto_examples_linear_model_plot_sparse_logistic_regression_mnist.py`\n* :ref:`sphx_glr_auto_examples_classification_plot_classification_probability.py`"
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_26",
    "header": "Binary Case",
    "text": "Binary Case\n-----------\n\nFor notational ease, we assume that the target :math:`y_i` takes values in the\nset :math:`\\{0, 1\\}` for data point :math:`i`.\nOnce fitted, the :meth:`~sklearn.linear_model.LogisticRegression.predict_proba`\nmethod of :class:`~sklearn.linear_model.LogisticRegression` predicts\nthe probability of the positive class :math:`P(y_i=1|X_i)` as\n\n.. math:: \\hat{p}(X_i) = \\operatorname{expit}(X_i w + w_0) = \\frac{1}{1 + \\exp(-X_i w - w_0)}.\n\n\nAs an optimization problem, binary\nclass logistic regression with regularization term :math:`r(w)` minimizes the\nfollowing cost function:\n\n.. math::\n    :name: regularized-logistic-loss\n\n    \\min_{w} \\frac{1}{S}\\sum_{i=1}^n s_i\n    \\left(-y_i \\log(\\hat{p}(X_i)) - (1 - y_i) \\log(1 - \\hat{p}(X_i))\\right)\n    + \\frac{r(w)}{S C}\\,,\n\nwhere :math:`{s_i}` corresponds to the weights assigned by the user to a\nspecific training sample (the vector :math:`s` is formed by element-wise\nmultiplication of the class weights and sample weights),\nand the sum :math:`S = \\sum_{i=1}^n s_i`.\n\nWe currently provide four choices for the regularization term  :math:`r(w)` via\nthe `penalty` argument:\n\n+----------------+-------------------------------------------------+\n| penalty        | :math:`r(w)`                                    |\n+================+=================================================+\n| `None`         | :math:`0`                                       |\n+----------------+-------------------------------------------------+\n| :math:`\\ell_1` | :math:`\\|w\\|_1`                                 |\n+----------------+-------------------------------------------------+\n| :math:`\\ell_2` | :math:`\\frac{1}{2}\\|w\\|_2^2 = \\frac{1}{2}w^T w` |\n+----------------+-------------------------------------------------+\n| `ElasticNet`   | :math:`\\frac{1 - \\rho}{2}w^T w + \\rho \\|w\\|_1`  |\n+----------------+-------------------------------------------------+\n\nFor ElasticNet, :math:`\\rho` (which corresponds to the `l1_ratio` parameter)\ncontrols the strength of :math:`\\ell_1` regularization vs. :math:`\\ell_2`\nregularization. Elastic-Net is equivalent to :math:`\\ell_1` when\n:math:`\\rho = 1` and equivalent to :math:`\\ell_2` when :math:`\\rho=0`.\n\nNote that the scale of the class weights and the sample weights will influence\nthe optimization problem. For instance, multiplying the sample weights by a\nconstant :math:`b>0` is equivalent to multiplying the (inverse) regularization\nstrength `C` by :math:`b`."
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_27",
    "header": "Multinomial Case",
    "text": "Multinomial Case\n----------------\n\nThe binary case can be extended to :math:`K` classes leading to the multinomial\nlogistic regression, see also `log-linear model\n<https://en.wikipedia.org/wiki/Multinomial_logistic_regression#As_a_log-linear_model>`_.\n\n.. note::\n   It is possible to parameterize a :math:`K`-class classification model\n   using only :math:`K-1` weight vectors, leaving one class probability fully\n   determined by the other class probabilities by leveraging the fact that all\n   class probabilities must sum to one. We deliberately choose to overparameterize the model\n   using :math:`K` weight vectors for ease of implementation and to preserve the\n   symmetrical inductive bias regarding ordering of classes, see [16]_. This effect becomes\n   especially important when using regularization. The choice of overparameterization can be\n   detrimental for unpenalized models since then the solution may not be unique, as shown in [16]_.\n\n.. dropdown:: Mathematical details\n\n  Let :math:`y_i \\in \\{1, \\ldots, K\\}` be the label (ordinal) encoded target variable for observation :math:`i`.\n  Instead of a single coefficient vector, we now have\n  a matrix of coefficients :math:`W` where each row vector :math:`W_k` corresponds to class\n  :math:`k`. We aim at predicting the class probabilities :math:`P(y_i=k|X_i)` via\n  :meth:`~sklearn.linear_model.LogisticRegression.predict_proba` as:\n\n  .. math:: \\hat{p}_k(X_i) = \\frac{\\exp(X_i W_k + W_{0, k})}{\\sum_{l=0}^{K-1} \\exp(X_i W_l + W_{0, l})}.\n\n  The objective for the optimization becomes\n\n  .. math::\n    \\min_W -\\frac{1}{S}\\sum_{i=1}^n \\sum_{k=0}^{K-1} s_{ik} [y_i = k] \\log(\\hat{p}_k(X_i))\n    + \\frac{r(W)}{S C}\\,,\n\n  where :math:`[P]` represents the Iverson bracket which evaluates to :math:`0`\n  if :math:`P` is false, otherwise it evaluates to :math:`1`.\n\n  Again, :math:`s_{ik}` are the weights assigned by the user (multiplication of sample\n  weights and class weights) with their sum :math:`S = \\sum_{i=1}^n \\sum_{k=0}^{K-1} s_{ik}`.\n\n  We currently provide four choices\n  for the regularization term :math:`r(W)` via the `penalty` argument, where :math:`m`\n  is the number of features:\n\n  +----------------+----------------------------------------------------------------------------------+\n  | penalty        | :math:`r(W)`                                                                     |\n  +================+==================================================================================+\n  | `None`         | :math:`0`                                                                        |\n  +----------------+----------------------------------------------------------------------------------+\n  | :math:`\\ell_1` | :math:`\\|W\\|_{1,1} = \\sum_{i=1}^m\\sum_{j=1}^{K}|W_{i,j}|`                        |\n  +----------------+----------------------------------------------------------------------------------+\n  | :math:`\\ell_2` | :math:`\\frac{1}{2}\\|W\\|_F^2 = \\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^{K} W_{i,j}^2`   |\n  +----------------+----------------------------------------------------------------------------------+\n  | `ElasticNet`   | :math:`\\frac{1 - \\rho}{2}\\|W\\|_F^2 + \\rho \\|W\\|_{1,1}`                           |\n  +----------------+----------------------------------------------------------------------------------+\n\n.. _logistic_regression_solvers:"
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_28",
    "header": "Solvers",
    "text": "Solvers\n-------\n\nThe solvers implemented in the class :class:`LogisticRegression`\nare \"lbfgs\", \"liblinear\", \"newton-cg\", \"newton-cholesky\", \"sag\" and \"saga\":\n\nThe following table summarizes the penalties and multinomial multiclass supported by each solver:\n\n+------------------------------+-----------------+-------------+-----------------+-----------------------+-----------+------------+\n|                              |                       **Solvers**                                                                |\n+------------------------------+-------------+-----------------+-----------------+-----------------------+-----------+------------+\n| **Penalties**                | **'lbfgs'** | **'liblinear'** | **'newton-cg'** | **'newton-cholesky'** | **'sag'** | **'saga'** |\n+------------------------------+-------------+-----------------+-----------------+-----------------------+-----------+------------+\n| L2 penalty                   |     yes     |       yes       |       yes       |     yes               |    yes    |    yes     |\n+------------------------------+-------------+-----------------+-----------------+-----------------------+-----------+------------+\n| L1 penalty                   |     no      |       yes       |       no        |     no                |    no     |    yes     |\n+------------------------------+-------------+-----------------+-----------------+-----------------------+-----------+------------+\n| Elastic-Net (L1 + L2)        |     no      |       no        |       no        |     no                |    no     |    yes     |\n+------------------------------+-------------+-----------------+-----------------+-----------------------+-----------+------------+\n| No penalty ('none')          |     yes     |       no        |       yes       |     yes               |    yes    |    yes     |\n+------------------------------+-------------+-----------------+-----------------+-----------------------+-----------+------------+\n| **Multiclass support**       |                                                                                                  |\n+------------------------------+-------------+-----------------+-----------------+-----------------------+-----------+------------+\n| multinomial multiclass       |     yes     |       no        |       yes       |     yes               |    yes    |    yes     |\n+------------------------------+-------------+-----------------+-----------------+-----------------------+-----------+------------+\n| **Behaviors**                |                                                                                                  |\n+------------------------------+-------------+-----------------+-----------------+-----------------------+-----------+------------+\n| Penalize the intercept (bad) |     no      |       yes       |       no        |     no                |    no     |    no      |\n+------------------------------+-------------+-----------------+-----------------+-----------------------+-----------+------------+\n| Faster for large datasets    |     no      |       no        |       no        |     no                |    yes    |    yes     |\n+------------------------------+-------------+-----------------+-----------------+-----------------------+-----------+------------+\n| Robust to unscaled datasets  |     yes     |       yes       |       yes       |     yes               |    no     |    no      |\n+------------------------------+-------------+-----------------+-----------------+-----------------------+-----------+------------+\n\nThe \"lbfgs\" solver is used by default for its robustness. For\n`n_samples >> n_features`, \"newton-cholesky\" is a good choice and can reach high\nprecision (tiny `tol` values). For large datasets\nthe \"saga\" solver is usually faster (than \"lbfgs\"), in particular for low precision\n(high `tol`).\nFor large dataset, you may also consider using :class:`SGDClassifier`\nwith `loss=\"log_loss\"`, which might be even faster but requires more tuning.\n\n.. _liblinear_differences:"
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_29",
    "header": "Differences between solvers",
    "text": "Differences between solvers\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThere might be a difference in the scores obtained between\n:class:`LogisticRegression` with ``solver=liblinear`` or\n:class:`~sklearn.svm.LinearSVC` and the external liblinear library directly,\nwhen ``fit_intercept=False`` and the fit ``coef_`` (or) the data to be predicted\nare zeroes. This is because for the sample(s) with ``decision_function`` zero,\n:class:`LogisticRegression` and :class:`~sklearn.svm.LinearSVC` predict the\nnegative class, while liblinear predicts the positive class. Note that a model\nwith ``fit_intercept=False`` and having many samples with ``decision_function``\nzero, is likely to be an underfit, bad model and you are advised to set\n``fit_intercept=True`` and increase the ``intercept_scaling``.\n\n.. dropdown:: Solvers' details\n\n  * The solver \"liblinear\" uses a coordinate descent (CD) algorithm, and relies\n    on the excellent C++ `LIBLINEAR library\n    <https://www.csie.ntu.edu.tw/~cjlin/liblinear/>`_, which is shipped with\n    scikit-learn. However, the CD algorithm implemented in liblinear cannot learn\n    a true multinomial (multiclass) model; instead, the optimization problem is\n    decomposed in a \"one-vs-rest\" fashion so separate binary classifiers are\n    trained for all classes. This happens under the hood, so\n    :class:`LogisticRegression` instances using this solver behave as multiclass\n    classifiers. For :math:`\\ell_1` regularization :func:`sklearn.svm.l1_min_c` allows to\n    calculate the lower bound for C in order to get a non \"null\" (all feature\n    weights to zero) model.\n\n  * The \"lbfgs\", \"newton-cg\" and \"sag\" solvers only support :math:`\\ell_2`\n    regularization or no regularization, and are found to converge faster for some\n    high-dimensional data. Setting `multi_class` to \"multinomial\" with these solvers\n    learns a true multinomial logistic regression model [5]_, which means that its\n    probability estimates should be better calibrated than the default \"one-vs-rest\"\n    setting.\n\n  * The \"sag\" solver uses Stochastic Average Gradient descent [6]_. It is faster\n    than other solvers for large datasets, when both the number of samples and the\n    number of features are large.\n\n  * The \"saga\" solver [7]_ is a variant of \"sag\" that also supports the\n    non-smooth `penalty=\"l1\"`. This is therefore the solver of choice for sparse\n    multinomial logistic regression. It is also the only solver that supports\n    `penalty=\"elasticnet\"`.\n\n  * The \"lbfgs\" is an optimization algorithm that approximates the\n    Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm [8]_, which belongs to\n    quasi-Newton methods. As such, it can deal with a wide range of different training\n    data and is therefore the default solver. Its performance, however, suffers on poorly\n    scaled datasets and on datasets with one-hot encoded categorical features with rare\n    categories.\n\n  * The \"newton-cholesky\" solver is an exact Newton solver that calculates the Hessian\n    matrix and solves the resulting linear system. It is a very good choice for\n    `n_samples` >> `n_features` and can reach high precision (tiny values of `tol`),\n    but has a few shortcomings: Only :math:`\\ell_2` regularization is supported.\n    Furthermore, because the Hessian matrix is explicitly computed, the memory usage\n    has a quadratic dependency on `n_features` as well as on `n_classes`.\n\n  For a comparison of some of these solvers, see [9]_.\n\n  .. rubric:: References\n\n  .. [5] Christopher M. Bishop: Pattern Recognition and Machine Learning, Chapter 4.3.4\n\n  .. [6] Mark Schmidt, Nicolas Le Roux, and Francis Bach: `Minimizing Finite Sums with the Stochastic Average Gradient. <https://hal.inria.fr/hal-00860051/document>`_\n\n  .. [7] Aaron Defazio, Francis Bach, Simon Lacoste-Julien:\n      :arxiv:`SAGA: A Fast Incremental Gradient Method With Support for\n      Non-Strongly Convex Composite Objectives. <1407.0202>`\n\n  .. [8] https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm\n\n  .. [9] Thomas P. Minka `\"A comparison of numerical optimizers for logistic regression\"\n          <https://tminka.github.io/papers/logreg/minka-logreg.pdf>`_\n\n  .. [16] :arxiv:`Simon, Noah, J. Friedman and T. Hastie.\n      \"A Blockwise Descent Algorithm for Group-penalized Multiresponse and\n      Multinomial Regression.\" <1311.6529>`\n\n\n.. note:: **Feature selection with sparse logistic regression**\n\n   A logistic regression with :math:`\\ell_1` penalty yields sparse models, and can\n   thus be used to perform feature selection, as detailed in\n   :ref:`l1_feature_selection`.\n\n.. note:: **P-value estimation**\n\n    It is possible to obtain the p-values and confidence intervals for\n    coefficients in cases of regression without penalization. The `statsmodels\n    package <https://pypi.org/project/statsmodels/>`_ natively supports this.\n    Within sklearn, one could use bootstrapping instead as well.\n\n\n:class:`LogisticRegressionCV` implements Logistic Regression with built-in\ncross-validation support, to find the optimal `C` and `l1_ratio` parameters\naccording to the ``scoring`` attribute. The \"newton-cg\", \"sag\", \"saga\" and\n\"lbfgs\" solvers are found to be faster for high-dimensional dense data, due\nto warm-starting (see :term:`Glossary <warm_start>`).\n\n.. _Generalized_linear_regression:\n\n.. _Generalized_linear_models:"
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_30",
    "header": "Generalized Linear Models",
    "text": "Generalized Linear Models\n=========================\n\nGeneralized Linear Models (GLM) extend linear models in two ways\n[10]_. First, the predicted values :math:`\\hat{y}` are linked to a linear\ncombination of the input variables :math:`X` via an inverse link function\n:math:`h` as\n\n.. math::    \\hat{y}(w, X) = h(Xw).\n\nSecondly, the squared loss function is replaced by the unit deviance\n:math:`d` of a distribution in the exponential family (or more precisely, a\nreproductive exponential dispersion model (EDM) [11]_).\n\nThe minimization problem becomes:\n\n.. math::    \\min_{w} \\frac{1}{2 n_{\\text{samples}}} \\sum_i d(y_i, \\hat{y}_i) + \\frac{\\alpha}{2} ||w||_2^2,\n\nwhere :math:`\\alpha` is the L2 regularization penalty. When sample weights are\nprovided, the average becomes a weighted average.\n\nThe following table lists some specific EDMs and their unit deviance :\n\n================= ================================  ============================================\nDistribution       Target Domain                    Unit Deviance :math:`d(y, \\hat{y})`\n================= ================================  ============================================\nNormal            :math:`y \\in (-\\infty, \\infty)`   :math:`(y-\\hat{y})^2`\nBernoulli         :math:`y \\in \\{0, 1\\}`            :math:`2({y}\\log\\frac{y}{\\hat{y}}+({1}-{y})\\log\\frac{{1}-{y}}{{1}-\\hat{y}})`\nCategorical       :math:`y \\in \\{0, 1, ..., k\\}`    :math:`2\\sum_{i \\in \\{0, 1, ..., k\\}} I(y = i) y_\\text{i}\\log\\frac{I(y = i)}{\\hat{I(y = i)}}`\nPoisson           :math:`y \\in [0, \\infty)`         :math:`2(y\\log\\frac{y}{\\hat{y}}-y+\\hat{y})`\nGamma             :math:`y \\in (0, \\infty)`         :math:`2(\\log\\frac{\\hat{y}}{y}+\\frac{y}{\\hat{y}}-1)`\nInverse Gaussian  :math:`y \\in (0, \\infty)`         :math:`\\frac{(y-\\hat{y})^2}{y\\hat{y}^2}`\n================= ================================  ============================================\n\nThe Probability Density Functions (PDF) of these distributions are illustrated\nin the following figure,\n\n.. figure:: ./glm_data/poisson_gamma_tweedie_distributions.png\n   :align: center\n   :scale: 100%\n\n   PDF of a random variable Y following Poisson, Tweedie (power=1.5) and Gamma\n   distributions with different mean values (:math:`\\mu`). Observe the point\n   mass at :math:`Y=0` for the Poisson distribution and the Tweedie (power=1.5)\n   distribution, but not for the Gamma distribution which has a strictly\n   positive target domain.\n\nThe Bernoulli distribution is a discrete probability distribution modelling a\nBernoulli trial - an event that has only two mutually exclusive outcomes.\nThe Categorical distribution is a generalization of the Bernoulli distribution\nfor a categorical random variable. While a random variable in a Bernoulli\ndistribution has two possible outcomes, a Categorical random variable can take\non one of K possible categories, with the probability of each category\nspecified separately.\n\nThe choice of the distribution depends on the problem at hand:\n\n* If the target values :math:`y` are counts (non-negative integer valued) or\n  relative frequencies (non-negative), you might use a Poisson distribution\n  with a log-link.\n* If the target values are positive valued and skewed, you might try a Gamma\n  distribution with a log-link.\n* If the target values seem to be heavier tailed than a Gamma distribution, you\n  might try an Inverse Gaussian distribution (or even higher variance powers of\n  the Tweedie family).\n* If the target values :math:`y` are probabilities, you can use the Bernoulli\n  distribution. The Bernoulli distribution with a logit link can be used for\n  binary classification. The Categorical distribution with a softmax link can be\n  used for multiclass classification.\n\n\n.. dropdown:: Examples of use cases\n\n  * Agriculture / weather modeling:  number of rain events per year (Poisson),\n    amount of rainfall per event (Gamma), total rainfall per year (Tweedie /\n    Compound Poisson Gamma).\n  * Risk modeling / insurance policy pricing:  number of claim events /\n    policyholder per year (Poisson), cost per event (Gamma), total cost per\n    policyholder per year (Tweedie / Compound Poisson Gamma).\n  * Credit Default: probability that a loan can't be paid back (Bernoulli).\n  * Fraud Detection: probability that a financial transaction like a cash transfer\n    is a fraudulent transaction (Bernoulli).\n  * Predictive maintenance: number of production interruption events per year\n    (Poisson), duration of interruption (Gamma), total interruption time per year\n    (Tweedie / Compound Poisson Gamma).\n  * Medical Drug Testing: probability of curing a patient in a set of trials or\n    probability that a patient will experience side effects (Bernoulli).\n  * News Classification: classification of news articles into three categories\n    namely Business News, Politics and Entertainment news (Categorical).\n\n.. rubric:: References\n\n.. [10] McCullagh, Peter; Nelder, John (1989). Generalized Linear Models,\n    Second Edition. Boca Raton: Chapman and Hall/CRC. ISBN 0-412-31760-5.\n\n.. [11] J\u00f8rgensen, B. (1992). The theory of exponential dispersion models\n    and analysis of deviance. Monografias de matem\u00e1tica, no. 51.  See also\n    `Exponential dispersion model.\n    <https://en.wikipedia.org/wiki/Exponential_dispersion_model>`_"
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_31",
    "header": "Usage",
    "text": "Usage\n-----\n\n:class:`TweedieRegressor` implements a generalized linear model for the\nTweedie distribution, that allows to model any of the above mentioned\ndistributions using the appropriate ``power`` parameter. In particular:\n\n- ``power = 0``: Normal distribution. Specific estimators such as\n  :class:`Ridge`, :class:`ElasticNet` are generally more appropriate in\n  this case.\n- ``power = 1``: Poisson distribution. :class:`PoissonRegressor` is exposed\n  for convenience. However, it is strictly equivalent to\n  `TweedieRegressor(power=1, link='log')`.\n- ``power = 2``: Gamma distribution. :class:`GammaRegressor` is exposed for\n  convenience. However, it is strictly equivalent to\n  `TweedieRegressor(power=2, link='log')`.\n- ``power = 3``: Inverse Gaussian distribution.\n\nThe link function is determined by the `link` parameter.\n\nUsage example::\n\n    >>> from sklearn.linear_model import TweedieRegressor\n    >>> reg = TweedieRegressor(power=1, alpha=0.5, link='log')\n    >>> reg.fit([[0, 0], [0, 1], [2, 2]], [0, 1, 2])\n    TweedieRegressor(alpha=0.5, link='log', power=1)\n    >>> reg.coef_\n    array([0.2463, 0.4337])\n    >>> reg.intercept_\n    np.float64(-0.7638)\n\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_linear_model_plot_poisson_regression_non_normal_loss.py`\n* :ref:`sphx_glr_auto_examples_linear_model_plot_tweedie_regression_insurance_claims.py`\n\n.. dropdown:: Practical considerations\n\n  The feature matrix `X` should be standardized before fitting. This ensures\n  that the penalty treats features equally.\n\n  Since the linear predictor :math:`Xw` can be negative and Poisson,\n  Gamma and Inverse Gaussian distributions don't support negative values, it\n  is necessary to apply an inverse link function that guarantees the\n  non-negativeness. For example with `link='log'`, the inverse link function\n  becomes :math:`h(Xw)=\\exp(Xw)`.\n\n  If you want to model a relative frequency, i.e. counts per exposure (time,\n  volume, ...) you can do so by using a Poisson distribution and passing\n  :math:`y=\\frac{\\mathrm{counts}}{\\mathrm{exposure}}` as target values\n  together with :math:`\\mathrm{exposure}` as sample weights. For a concrete\n  example see e.g.\n  :ref:`sphx_glr_auto_examples_linear_model_plot_tweedie_regression_insurance_claims.py`.\n\n  When performing cross-validation for the `power` parameter of\n  `TweedieRegressor`, it is advisable to specify an explicit `scoring` function,\n  because the default scorer :meth:`TweedieRegressor.score` is a function of\n  `power` itself."
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_32",
    "header": "Stochastic Gradient Descent - SGD",
    "text": "Stochastic Gradient Descent - SGD\n=================================\n\nStochastic gradient descent is a simple yet very efficient approach\nto fit linear models. It is particularly useful when the number of samples\n(and the number of features) is very large.\nThe ``partial_fit`` method allows online/out-of-core learning.\n\nThe classes :class:`SGDClassifier` and :class:`SGDRegressor` provide\nfunctionality to fit linear models for classification and regression\nusing different (convex) loss functions and different penalties.\nE.g., with ``loss=\"log\"``, :class:`SGDClassifier`\nfits a logistic regression model,\nwhile with ``loss=\"hinge\"`` it fits a linear support vector machine (SVM).\n\nYou can refer to the dedicated :ref:`sgd` documentation section for more details.\n\n.. _perceptron:"
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_33",
    "header": "Perceptron",
    "text": "Perceptron\n==========\n\nThe :class:`Perceptron` is another simple classification algorithm suitable for\nlarge scale learning. By default:\n\n- It does not require a learning rate.\n\n- It is not regularized (penalized).\n\n- It updates its model only on mistakes.\n\nThe last characteristic implies that the Perceptron is slightly faster to\ntrain than SGD with the hinge loss and that the resulting models are\nsparser.\n\nIn fact, the :class:`Perceptron` is a wrapper around the :class:`SGDClassifier`\nclass using a perceptron loss and a constant learning rate. Refer to\n:ref:`mathematical section <sgd_mathematical_formulation>` of the SGD procedure\nfor more details.\n\n.. _passive_aggressive:"
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_34",
    "header": "Passive Aggressive Algorithms",
    "text": "Passive Aggressive Algorithms\n=============================\n\nThe passive-aggressive algorithms are a family of algorithms for large-scale\nlearning. They are similar to the Perceptron in that they do not require a\nlearning rate. However, contrary to the Perceptron, they include a\nregularization parameter ``C``.\n\nFor classification, :class:`PassiveAggressiveClassifier` can be used with\n``loss='hinge'`` (PA-I) or ``loss='squared_hinge'`` (PA-II).  For regression,\n:class:`PassiveAggressiveRegressor` can be used with\n``loss='epsilon_insensitive'`` (PA-I) or\n``loss='squared_epsilon_insensitive'`` (PA-II).\n\n.. dropdown:: References\n\n  * `\"Online Passive-Aggressive Algorithms\"\n    <http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>`_\n    K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR 7 (2006)"
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_35",
    "header": "Robustness regression: outliers and modeling errors",
    "text": "Robustness regression: outliers and modeling errors\n=====================================================\n\nRobust regression aims to fit a regression model in the\npresence of corrupt data: either outliers, or error in the model.\n\n.. figure:: ../auto_examples/linear_model/images/sphx_glr_plot_theilsen_001.png\n   :target: ../auto_examples/linear_model/plot_theilsen.html\n   :scale: 50%\n   :align: center"
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_36",
    "header": "Different scenario and useful concepts",
    "text": "Different scenario and useful concepts\n----------------------------------------\n\nThere are different things to keep in mind when dealing with data\ncorrupted by outliers:\n\n.. |y_outliers| image:: ../auto_examples/linear_model/images/sphx_glr_plot_robust_fit_003.png\n   :target: ../auto_examples/linear_model/plot_robust_fit.html\n   :scale: 60%\n\n.. |X_outliers| image:: ../auto_examples/linear_model/images/sphx_glr_plot_robust_fit_002.png\n   :target: ../auto_examples/linear_model/plot_robust_fit.html\n   :scale: 60%\n\n.. |large_y_outliers| image:: ../auto_examples/linear_model/images/sphx_glr_plot_robust_fit_005.png\n   :target: ../auto_examples/linear_model/plot_robust_fit.html\n   :scale: 60%\n\n* **Outliers in X or in y**?\n\n  ==================================== ====================================\n  Outliers in the y direction          Outliers in the X direction\n  ==================================== ====================================\n  |y_outliers|                         |X_outliers|\n  ==================================== ====================================\n\n* **Fraction of outliers versus amplitude of error**\n\n  The number of outlying points matters, but also how much they are\n  outliers.\n\n  ==================================== ====================================\n  Small outliers                       Large outliers\n  ==================================== ====================================\n  |y_outliers|                         |large_y_outliers|\n  ==================================== ====================================\n\nAn important notion of robust fitting is that of breakdown point: the\nfraction of data that can be outlying for the fit to start missing the\ninlying data.\n\nNote that in general, robust fitting in high-dimensional setting (large\n`n_features`) is very hard. The robust models here will probably not work\nin these settings.\n\n\n.. topic:: Trade-offs: which estimator ?\n\n  Scikit-learn provides 3 robust regression estimators:\n  :ref:`RANSAC <ransac_regression>`,\n  :ref:`Theil Sen <theil_sen_regression>` and\n  :ref:`HuberRegressor <huber_regression>`.\n\n  * :ref:`HuberRegressor <huber_regression>` should be faster than\n    :ref:`RANSAC <ransac_regression>` and :ref:`Theil Sen <theil_sen_regression>`\n    unless the number of samples is very large, i.e. ``n_samples`` >> ``n_features``.\n    This is because :ref:`RANSAC <ransac_regression>` and :ref:`Theil Sen <theil_sen_regression>`\n    fit on smaller subsets of the data. However, both :ref:`Theil Sen <theil_sen_regression>`\n    and :ref:`RANSAC <ransac_regression>` are unlikely to be as robust as\n    :ref:`HuberRegressor <huber_regression>` for the default parameters.\n\n  * :ref:`RANSAC <ransac_regression>` is faster than :ref:`Theil Sen <theil_sen_regression>`\n    and scales much better with the number of samples.\n\n  * :ref:`RANSAC <ransac_regression>` will deal better with large\n    outliers in the y direction (most common situation).\n\n  * :ref:`Theil Sen <theil_sen_regression>` will cope better with\n    medium-size outliers in the X direction, but this property will\n    disappear in high-dimensional settings.\n\n  When in doubt, use :ref:`RANSAC <ransac_regression>`.\n\n.. _ransac_regression:"
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_37",
    "header": "RANSAC: RANdom SAmple Consensus",
    "text": "RANSAC: RANdom SAmple Consensus\n--------------------------------\n\nRANSAC (RANdom SAmple Consensus) fits a model from random subsets of\ninliers from the complete data set.\n\nRANSAC is a non-deterministic algorithm producing only a reasonable result with\na certain probability, which is dependent on the number of iterations (see\n`max_trials` parameter). It is typically used for linear and non-linear\nregression problems and is especially popular in the field of photogrammetric\ncomputer vision.\n\nThe algorithm splits the complete input sample data into a set of inliers,\nwhich may be subject to noise, and outliers, which are e.g. caused by erroneous\nmeasurements or invalid hypotheses about the data. The resulting model is then\nestimated only from the determined inliers.\n\n.. figure:: ../auto_examples/linear_model/images/sphx_glr_plot_ransac_001.png\n   :target: ../auto_examples/linear_model/plot_ransac.html\n   :align: center\n   :scale: 50%\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_linear_model_plot_ransac.py`\n* :ref:`sphx_glr_auto_examples_linear_model_plot_robust_fit.py`\n\n.. dropdown:: Details of the algorithm\n\n  Each iteration performs the following steps:\n\n  1. Select ``min_samples`` random samples from the original data and check\n     whether the set of data is valid (see ``is_data_valid``).\n  2. Fit a model to the random subset (``estimator.fit``) and check\n     whether the estimated model is valid (see ``is_model_valid``).\n  3. Classify all data as inliers or outliers by calculating the residuals\n     to the estimated model (``estimator.predict(X) - y``) - all data\n     samples with absolute residuals smaller than or equal to the\n     ``residual_threshold`` are considered as inliers.\n  4. Save fitted model as best model if number of inlier samples is\n     maximal. In case the current estimated model has the same number of\n     inliers, it is only considered as the best model if it has better score.\n\n  These steps are performed either a maximum number of times (``max_trials``) or\n  until one of the special stop criteria are met (see ``stop_n_inliers`` and\n  ``stop_score``). The final model is estimated using all inlier samples (consensus\n  set) of the previously determined best model.\n\n  The ``is_data_valid`` and ``is_model_valid`` functions allow to identify and reject\n  degenerate combinations of random sub-samples. If the estimated model is not\n  needed for identifying degenerate cases, ``is_data_valid`` should be used as it\n  is called prior to fitting the model and thus leading to better computational\n  performance.\n\n.. dropdown:: References\n\n  * https://en.wikipedia.org/wiki/RANSAC\n  * `\"Random Sample Consensus: A Paradigm for Model Fitting with Applications to\n    Image Analysis and Automated Cartography\"\n    <https://www.cs.ait.ac.th/~mdailey/cvreadings/Fischler-RANSAC.pdf>`_\n    Martin A. Fischler and Robert C. Bolles - SRI International (1981)\n  * `\"Performance Evaluation of RANSAC Family\"\n    <http://www.bmva.org/bmvc/2009/Papers/Paper355/Paper355.pdf>`_\n    Sunglok Choi, Taemin Kim and Wonpil Yu - BMVC (2009)\n\n.. _theil_sen_regression:"
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_38",
    "header": "Theil-Sen estimator: generalized-median-based estimator",
    "text": "Theil-Sen estimator: generalized-median-based estimator\n--------------------------------------------------------\n\nThe :class:`TheilSenRegressor` estimator uses a generalization of the median in\nmultiple dimensions. It is thus robust to multivariate outliers. Note however\nthat the robustness of the estimator decreases quickly with the dimensionality\nof the problem. It loses its robustness properties and becomes no\nbetter than an ordinary least squares in high dimension.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_linear_model_plot_theilsen.py`\n* :ref:`sphx_glr_auto_examples_linear_model_plot_robust_fit.py`\n\n\n.. dropdown:: Theoretical considerations\n\n  :class:`TheilSenRegressor` is comparable to the :ref:`Ordinary Least Squares\n  (OLS) <ordinary_least_squares>` in terms of asymptotic efficiency and as an\n  unbiased estimator. In contrast to OLS, Theil-Sen is a non-parametric\n  method which means it makes no assumption about the underlying\n  distribution of the data. Since Theil-Sen is a median-based estimator, it\n  is more robust against corrupted data aka outliers. In univariate\n  setting, Theil-Sen has a breakdown point of about 29.3% in case of a\n  simple linear regression which means that it can tolerate arbitrary\n  corrupted data of up to 29.3%.\n\n  .. figure:: ../auto_examples/linear_model/images/sphx_glr_plot_theilsen_001.png\n    :target: ../auto_examples/linear_model/plot_theilsen.html\n    :align: center\n    :scale: 50%\n\n  The implementation of :class:`TheilSenRegressor` in scikit-learn follows a\n  generalization to a multivariate linear regression model [#f1]_ using the\n  spatial median which is a generalization of the median to multiple\n  dimensions [#f2]_.\n\n  In terms of time and space complexity, Theil-Sen scales according to\n\n  .. math::\n      \\binom{n_{\\text{samples}}}{n_{\\text{subsamples}}}\n\n  which makes it infeasible to be applied exhaustively to problems with a\n  large number of samples and features. Therefore, the magnitude of a\n  subpopulation can be chosen to limit the time and space complexity by\n  considering only a random subset of all possible combinations.\n\n  .. rubric:: References\n\n  .. [#f1] Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang: `Theil-Sen Estimators in a Multiple Linear Regression Model. <http://home.olemiss.edu/~xdang/papers/MTSE.pdf>`_\n\n  .. [#f2] T. K\u00e4rkk\u00e4inen and S. \u00c4yr\u00e4m\u00f6: `On Computation of Spatial Median for Robust Data Mining. <http://users.jyu.fi/~samiayr/pdf/ayramo_eurogen05.pdf>`_\n\n  Also see the `Wikipedia page <https://en.wikipedia.org/wiki/Theil%E2%80%93Sen_estimator>`_\n\n\n.. _huber_regression:"
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_39",
    "header": "Huber Regression",
    "text": "Huber Regression\n----------------\n\nThe :class:`HuberRegressor` is different from :class:`Ridge` because it applies a\nlinear loss to samples that are defined as outliers by the `epsilon` parameter.\nA sample is classified as an inlier if the absolute error of that sample is\nless than the threshold `epsilon`. It differs from :class:`TheilSenRegressor`\nand :class:`RANSACRegressor` because it does not ignore the effect of the outliers\nbut gives a lesser weight to them.\n\n.. figure:: /auto_examples/linear_model/images/sphx_glr_plot_huber_vs_ridge_001.png\n   :target: ../auto_examples/linear_model/plot_huber_vs_ridge.html\n   :align: center\n   :scale: 50%\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_linear_model_plot_huber_vs_ridge.py`\n\n.. dropdown:: Mathematical details\n\n  :class:`HuberRegressor` minimizes\n\n  .. math::\n\n    \\min_{w, \\sigma} {\\sum_{i=1}^n\\left(\\sigma + H_{\\epsilon}\\left(\\frac{X_{i}w - y_{i}}{\\sigma}\\right)\\sigma\\right) + \\alpha {||w||_2}^2}\n\n  where the loss function is given by\n\n  .. math::\n\n    H_{\\epsilon}(z) = \\begin{cases}\n          z^2, & \\text {if } |z| < \\epsilon, \\\\\n          2\\epsilon|z| - \\epsilon^2, & \\text{otherwise}\n    \\end{cases}\n\n  It is advised to set the parameter ``epsilon`` to 1.35 to achieve 95%\n  statistical efficiency.\n\n  .. rubric:: References\n\n  * Peter J. Huber, Elvezio M. Ronchetti: Robust Statistics, Concomitant scale\n    estimates, p. 172.\n\nThe :class:`HuberRegressor` differs from using :class:`SGDRegressor` with loss set to `huber`\nin the following ways.\n\n- :class:`HuberRegressor` is scaling invariant. Once ``epsilon`` is set, scaling ``X`` and ``y``\n  down or up by different values would produce the same robustness to outliers as before.\n  as compared to :class:`SGDRegressor` where ``epsilon`` has to be set again when ``X`` and ``y`` are\n  scaled.\n\n- :class:`HuberRegressor` should be more efficient to use on data with small number of\n  samples while :class:`SGDRegressor` needs a number of passes on the training data to\n  produce the same robustness.\n\nNote that this estimator is different from the `R implementation of Robust\nRegression <https://stats.oarc.ucla.edu/r/dae/robust-regression/>`_  because the R\nimplementation does a weighted least squares implementation with weights given to each\nsample on the basis of how much the residual is greater than a certain threshold.\n\n.. _quantile_regression:"
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_40",
    "header": "Quantile Regression",
    "text": "Quantile Regression\n===================\n\nQuantile regression estimates the median or other quantiles of :math:`y`\nconditional on :math:`X`, while ordinary least squares (OLS) estimates the\nconditional mean.\n\nQuantile regression may be useful if one is interested in predicting an\ninterval instead of point prediction. Sometimes, prediction intervals are\ncalculated based on the assumption that prediction error is distributed\nnormally with zero mean and constant variance. Quantile regression provides\nsensible prediction intervals even for errors with non-constant (but\npredictable) variance or non-normal distribution.\n\n.. figure:: /auto_examples/linear_model/images/sphx_glr_plot_quantile_regression_002.png\n   :target: ../auto_examples/linear_model/plot_quantile_regression.html\n   :align: center\n   :scale: 50%\n\nBased on minimizing the pinball loss, conditional quantiles can also be\nestimated by models other than linear models. For example,\n:class:`~sklearn.ensemble.GradientBoostingRegressor` can predict conditional\nquantiles if its parameter ``loss`` is set to ``\"quantile\"`` and parameter\n``alpha`` is set to the quantile that should be predicted. See the example in\n:ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_quantile.py`.\n\nMost implementations of quantile regression are based on linear programming\nproblem. The current implementation is based on\n:func:`scipy.optimize.linprog`.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_linear_model_plot_quantile_regression.py`\n\n.. dropdown:: Mathematical details\n\n  As a linear model, the :class:`QuantileRegressor` gives linear predictions\n  :math:`\\hat{y}(w, X) = Xw` for the :math:`q`-th quantile, :math:`q \\in (0, 1)`.\n  The weights or coefficients :math:`w` are then found by the following\n  minimization problem:\n\n  .. math::\n      \\min_{w} {\\frac{1}{n_{\\text{samples}}}\n      \\sum_i PB_q(y_i - X_i w) + \\alpha ||w||_1}.\n\n  This consists of the pinball loss (also known as linear loss),\n  see also :class:`~sklearn.metrics.mean_pinball_loss`,\n\n  .. math::\n      PB_q(t) = q \\max(t, 0) + (1 - q) \\max(-t, 0) =\n      \\begin{cases}\n          q t, & t > 0, \\\\\n          0,    & t = 0, \\\\\n          (q-1) t, & t < 0\n      \\end{cases}\n\n  and the L1 penalty controlled by parameter ``alpha``, similar to\n  :class:`Lasso`.\n\n  As the pinball loss is only linear in the residuals, quantile regression is\n  much more robust to outliers than squared error based estimation of the mean.\n  Somewhat in between is the :class:`HuberRegressor`.\n\n.. dropdown:: References\n\n  * Koenker, R., & Bassett Jr, G. (1978). `Regression quantiles.\n    <https://gib.people.uic.edu/RQ.pdf>`_\n    Econometrica: journal of the Econometric Society, 33-50.\n\n  * Portnoy, S., & Koenker, R. (1997). :doi:`The Gaussian hare and the Laplacian\n    tortoise: computability of squared-error versus absolute-error estimators.\n    Statistical Science, 12, 279-300 <10.1214/ss/1030037960>`.\n\n  * Koenker, R. (2005). :doi:`Quantile Regression <10.1017/CBO9780511754098>`.\n    Cambridge University Press.\n\n\n.. _polynomial_regression:"
  },
  {
    "filename": "linear_model.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\linear_model.rst.txt",
    "id": "linear_model.rst.txt_chunk_41",
    "header": "Polynomial regression: extending linear models with basis functions",
    "text": "Polynomial regression: extending linear models with basis functions\n===================================================================\n\n.. currentmodule:: sklearn.preprocessing\n\nOne common pattern within machine learning is to use linear models trained\non nonlinear functions of the data.  This approach maintains the generally\nfast performance of linear methods, while allowing them to fit a much wider\nrange of data.\n\n.. dropdown:: Mathematical details\n\n  For example, a simple linear regression can be extended by constructing\n  **polynomial features** from the coefficients.  In the standard linear\n  regression case, you might have a model that looks like this for\n  two-dimensional data:\n\n  .. math::    \\hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2\n\n  If we want to fit a paraboloid to the data instead of a plane, we can combine\n  the features in second-order polynomials, so that the model looks like this:\n\n  .. math::    \\hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_1 x_2 + w_4 x_1^2 + w_5 x_2^2\n\n  The (sometimes surprising) observation is that this is *still a linear model*:\n  to see this, imagine creating a new set of features\n\n  .. math::  z = [x_1, x_2, x_1 x_2, x_1^2, x_2^2]\n\n  With this re-labeling of the data, our problem can be written\n\n  .. math::    \\hat{y}(w, z) = w_0 + w_1 z_1 + w_2 z_2 + w_3 z_3 + w_4 z_4 + w_5 z_5\n\n  We see that the resulting *polynomial regression* is in the same class of\n  linear models we considered above (i.e. the model is linear in :math:`w`)\n  and can be solved by the same techniques.  By considering linear fits within\n  a higher-dimensional space built with these basis functions, the model has the\n  flexibility to fit a much broader range of data.\n\nHere is an example of applying this idea to one-dimensional data, using\npolynomial features of varying degrees:\n\n.. figure:: ../auto_examples/linear_model/images/sphx_glr_plot_polynomial_interpolation_001.png\n   :target: ../auto_examples/linear_model/plot_polynomial_interpolation.html\n   :align: center\n   :scale: 50%\n\nThis figure is created using the :class:`PolynomialFeatures` transformer, which\ntransforms an input data matrix into a new data matrix of a given degree.\nIt can be used as follows::\n\n    >>> from sklearn.preprocessing import PolynomialFeatures\n    >>> import numpy as np\n    >>> X = np.arange(6).reshape(3, 2)\n    >>> X\n    array([[0, 1],\n           [2, 3],\n           [4, 5]])\n    >>> poly = PolynomialFeatures(degree=2)\n    >>> poly.fit_transform(X)\n    array([[ 1.,  0.,  1.,  0.,  0.,  1.],\n           [ 1.,  2.,  3.,  4.,  6.,  9.],\n           [ 1.,  4.,  5., 16., 20., 25.]])\n\nThe features of ``X`` have been transformed from :math:`[x_1, x_2]` to\n:math:`[1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]`, and can now be used within\nany linear model.\n\nThis sort of preprocessing can be streamlined with the\n:ref:`Pipeline <pipeline>` tools. A single object representing a simple\npolynomial regression can be created and used as follows::\n\n    >>> from sklearn.preprocessing import PolynomialFeatures\n    >>> from sklearn.linear_model import LinearRegression\n    >>> from sklearn.pipeline import Pipeline\n    >>> import numpy as np\n    >>> model = Pipeline([('poly', PolynomialFeatures(degree=3)),\n    ...                   ('linear', LinearRegression(fit_intercept=False))])\n    >>> # fit to an order-3 polynomial data\n    >>> x = np.arange(5)\n    >>> y = 3 - 2 * x + x ** 2 - x ** 3\n    >>> model = model.fit(x[:, np.newaxis], y)\n    >>> model.named_steps['linear'].coef_\n    array([ 3., -2.,  1., -1.])\n\nThe linear model trained on polynomial features is able to exactly recover\nthe input polynomial coefficients.\n\nIn some cases it's not necessary to include higher powers of any single feature,\nbut only the so-called *interaction features*\nthat multiply together at most :math:`d` distinct features.\nThese can be gotten from :class:`PolynomialFeatures` with the setting\n``interaction_only=True``.\n\nFor example, when dealing with boolean features,\n:math:`x_i^n = x_i` for all :math:`n` and is therefore useless;\nbut :math:`x_i x_j` represents the conjunction of two booleans.\nThis way, we can solve the XOR problem with a linear classifier::\n\n    >>> from sklearn.linear_model import Perceptron\n    >>> from sklearn.preprocessing import PolynomialFeatures\n    >>> import numpy as np\n    >>> X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n    >>> y = X[:, 0] ^ X[:, 1]\n    >>> y\n    array([0, 1, 1, 0])\n    >>> X = PolynomialFeatures(interaction_only=True).fit_transform(X).astype(int)\n    >>> X\n    array([[1, 0, 0, 0],\n           [1, 0, 1, 0],\n           [1, 1, 0, 0],\n           [1, 1, 1, 1]])\n    >>> clf = Perceptron(fit_intercept=False, max_iter=10, tol=None,\n    ...                  shuffle=False).fit(X, y)\n\nAnd the classifier \"predictions\" are perfect::\n\n    >>> clf.predict(X)\n    array([0, 1, 1, 0])\n    >>> clf.score(X, y)\n    1.0"
  },
  {
    "filename": "manifold.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\manifold.rst.txt",
    "id": "manifold.rst.txt_chunk_0",
    "header": "",
    "text": ".. currentmodule:: sklearn.manifold\n\n.. _manifold:\n\n================="
  },
  {
    "filename": "manifold.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\manifold.rst.txt",
    "id": "manifold.rst.txt_chunk_1",
    "header": "Manifold learning",
    "text": "Manifold learning\n=================\n\n| Look for the bare necessities\n| The simple bare necessities\n| Forget about your worries and your strife\n| I mean the bare necessities\n| Old Mother Nature's recipes\n| That bring the bare necessities of life\n|\n|             -- Baloo's song [The Jungle Book]\n\n\n\n.. figure:: ../auto_examples/manifold/images/sphx_glr_plot_compare_methods_001.png\n   :target: ../auto_examples/manifold/plot_compare_methods.html\n   :align: center\n   :scale: 70%\n\n.. |manifold_img3| image:: ../auto_examples/manifold/images/sphx_glr_plot_compare_methods_003.png\n  :target: ../auto_examples/manifold/plot_compare_methods.html\n  :scale: 60%\n\n.. |manifold_img4| image:: ../auto_examples/manifold/images/sphx_glr_plot_compare_methods_004.png\n    :target: ../auto_examples/manifold/plot_compare_methods.html\n    :scale: 60%\n\n.. |manifold_img5| image:: ../auto_examples/manifold/images/sphx_glr_plot_compare_methods_005.png\n    :target: ../auto_examples/manifold/plot_compare_methods.html\n    :scale: 60%\n\n.. |manifold_img6| image:: ../auto_examples/manifold/images/sphx_glr_plot_compare_methods_006.png\n    :target: ../auto_examples/manifold/plot_compare_methods.html\n    :scale: 60%\n\n.. centered:: |manifold_img3| |manifold_img4| |manifold_img5| |manifold_img6|\n\n\nManifold learning is an approach to non-linear dimensionality reduction.\nAlgorithms for this task are based on the idea that the dimensionality of\nmany data sets is only artificially high."
  },
  {
    "filename": "manifold.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\manifold.rst.txt",
    "id": "manifold.rst.txt_chunk_2",
    "header": "Introduction",
    "text": "Introduction\n============\n\nHigh-dimensional datasets can be very difficult to visualize.  While data\nin two or three dimensions can be plotted to show the inherent\nstructure of the data, equivalent high-dimensional plots are much less\nintuitive.  To aid visualization of the structure of a dataset, the\ndimension must be reduced in some way.\n\nThe simplest way to accomplish this dimensionality reduction is by taking\na random projection of the data.  Though this allows some degree of\nvisualization of the data structure, the randomness of the choice leaves much\nto be desired.  In a random projection, it is likely that the more\ninteresting structure within the data will be lost.\n\n\n.. |digits_img| image:: ../auto_examples/manifold/images/sphx_glr_plot_lle_digits_001.png\n    :target: ../auto_examples/manifold/plot_lle_digits.html\n    :scale: 50\n\n.. |projected_img| image::  ../auto_examples/manifold/images/sphx_glr_plot_lle_digits_002.png\n    :target: ../auto_examples/manifold/plot_lle_digits.html\n    :scale: 50\n\n.. centered:: |digits_img| |projected_img|\n\n\nTo address this concern, a number of supervised and unsupervised linear\ndimensionality reduction frameworks have been designed, such as Principal\nComponent Analysis (PCA), Independent Component Analysis, Linear\nDiscriminant Analysis, and others.  These algorithms define specific\nrubrics to choose an \"interesting\" linear projection of the data.\nThese methods can be powerful, but often miss important non-linear\nstructure in the data.\n\n\n.. |PCA_img| image:: ../auto_examples/manifold/images/sphx_glr_plot_lle_digits_003.png\n    :target: ../auto_examples/manifold/plot_lle_digits.html\n    :scale: 50\n\n.. |LDA_img| image::  ../auto_examples/manifold/images/sphx_glr_plot_lle_digits_004.png\n    :target: ../auto_examples/manifold/plot_lle_digits.html\n    :scale: 50\n\n.. centered:: |PCA_img| |LDA_img|\n\nManifold Learning can be thought of as an attempt to generalize linear\nframeworks like PCA to be sensitive to non-linear structure in data. Though\nsupervised variants exist, the typical manifold learning problem is\nunsupervised: it learns the high-dimensional structure of the data\nfrom the data itself, without the use of predetermined classifications.\n\n\n.. rubric:: Examples\n\n* See :ref:`sphx_glr_auto_examples_manifold_plot_lle_digits.py` for an example of\n  dimensionality reduction on handwritten digits.\n\n* See :ref:`sphx_glr_auto_examples_manifold_plot_compare_methods.py` for an example of\n  dimensionality reduction on a toy \"S-curve\" dataset.\n\n* See :ref:`sphx_glr_auto_examples_applications_plot_stock_market.py` for an example of\n  using manifold learning to map the stock market structure based on historical stock\n  prices.\n\n* See :ref:`sphx_glr_auto_examples_manifold_plot_manifold_sphere.py` for an example of\n  manifold learning techniques applied to a spherical data-set.\n\n* See :ref:`sphx_glr_auto_examples_manifold_plot_swissroll.py` for an example of using \n  manifold learning techniques on a Swiss Roll dataset.\n\nThe manifold learning implementations available in scikit-learn are\nsummarized below\n\n.. _isomap:"
  },
  {
    "filename": "manifold.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\manifold.rst.txt",
    "id": "manifold.rst.txt_chunk_3",
    "header": "Isomap",
    "text": "Isomap\n======\n\nOne of the earliest approaches to manifold learning is the Isomap\nalgorithm, short for Isometric Mapping.  Isomap can be viewed as an\nextension of Multi-dimensional Scaling (MDS) or Kernel PCA.\nIsomap seeks a lower-dimensional embedding which maintains geodesic\ndistances between all points.  Isomap can be performed with the object\n:class:`Isomap`.\n\n.. figure:: ../auto_examples/manifold/images/sphx_glr_plot_lle_digits_005.png\n   :target: ../auto_examples/manifold/plot_lle_digits.html\n   :align: center\n   :scale: 50\n\n.. dropdown:: Complexity\n\n  The Isomap algorithm comprises three stages:\n\n  1. **Nearest neighbor search.**  Isomap uses\n     :class:`~sklearn.neighbors.BallTree` for efficient neighbor search.\n     The cost is approximately :math:`O[D \\log(k) N \\log(N)]`, for :math:`k`\n     nearest neighbors of :math:`N` points in :math:`D` dimensions.\n\n  2. **Shortest-path graph search.**  The most efficient known algorithms\n     for this are *Dijkstra's Algorithm*, which is approximately\n     :math:`O[N^2(k + \\log(N))]`, or the *Floyd-Warshall algorithm*, which\n     is :math:`O[N^3]`.  The algorithm can be selected by the user with\n     the ``path_method`` keyword of ``Isomap``.  If unspecified, the code\n     attempts to choose the best algorithm for the input data.\n\n  3. **Partial eigenvalue decomposition.**  The embedding is encoded in the\n     eigenvectors corresponding to the :math:`d` largest eigenvalues of the\n     :math:`N \\times N` isomap kernel.  For a dense solver, the cost is\n     approximately :math:`O[d N^2]`.  This cost can often be improved using\n     the ``ARPACK`` solver.  The eigensolver can be specified by the user\n     with the ``eigen_solver`` keyword of ``Isomap``.  If unspecified, the\n     code attempts to choose the best algorithm for the input data.\n\n  The overall complexity of Isomap is\n  :math:`O[D \\log(k) N \\log(N)] + O[N^2(k + \\log(N))] + O[d N^2]`.\n\n  * :math:`N` : number of training data points\n  * :math:`D` : input dimension\n  * :math:`k` : number of nearest neighbors\n  * :math:`d` : output dimension\n\n.. rubric:: References\n\n* `\"A global geometric framework for nonlinear dimensionality reduction\"\n  <http://science.sciencemag.org/content/290/5500/2319.full>`_\n  Tenenbaum, J.B.; De Silva, V.; & Langford, J.C.  Science 290 (5500)\n\n.. _locally_linear_embedding:"
  },
  {
    "filename": "manifold.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\manifold.rst.txt",
    "id": "manifold.rst.txt_chunk_4",
    "header": "Locally Linear Embedding",
    "text": "Locally Linear Embedding\n========================\n\nLocally linear embedding (LLE) seeks a lower-dimensional projection of the data\nwhich preserves distances within local neighborhoods.  It can be thought\nof as a series of local Principal Component Analyses which are globally\ncompared to find the best non-linear embedding.\n\nLocally linear embedding can be performed with function\n:func:`locally_linear_embedding` or its object-oriented counterpart\n:class:`LocallyLinearEmbedding`.\n\n.. figure:: ../auto_examples/manifold/images/sphx_glr_plot_lle_digits_006.png\n   :target: ../auto_examples/manifold/plot_lle_digits.html\n   :align: center\n   :scale: 50\n\n.. dropdown:: Complexity\n\n  The standard LLE algorithm comprises three stages:\n\n  1. **Nearest Neighbors Search**.  See discussion under Isomap above.\n\n  2. **Weight Matrix Construction**. :math:`O[D N k^3]`.\n     The construction of the LLE weight matrix involves the solution of a\n     :math:`k \\times k` linear equation for each of the :math:`N` local\n     neighborhoods.\n\n  3. **Partial Eigenvalue Decomposition**. See discussion under Isomap above.\n\n  The overall complexity of standard LLE is\n  :math:`O[D \\log(k) N \\log(N)] + O[D N k^3] + O[d N^2]`.\n\n  * :math:`N` : number of training data points\n  * :math:`D` : input dimension\n  * :math:`k` : number of nearest neighbors\n  * :math:`d` : output dimension\n\n.. rubric:: References\n\n* `\"Nonlinear dimensionality reduction by locally linear embedding\"\n  <http://www.sciencemag.org/content/290/5500/2323.full>`_\n  Roweis, S. & Saul, L.  Science 290:2323 (2000)"
  },
  {
    "filename": "manifold.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\manifold.rst.txt",
    "id": "manifold.rst.txt_chunk_5",
    "header": "Modified Locally Linear Embedding",
    "text": "Modified Locally Linear Embedding\n=================================\n\nOne well-known issue with LLE is the regularization problem.  When the number\nof neighbors is greater than the number of input dimensions, the matrix\ndefining each local neighborhood is rank-deficient.  To address this, standard\nLLE applies an arbitrary regularization parameter :math:`r`, which is chosen\nrelative to the trace of the local weight matrix.  Though it can be shown\nformally that as :math:`r \\to 0`, the solution converges to the desired\nembedding, there is no guarantee that the optimal solution will be found\nfor :math:`r > 0`.  This problem manifests itself in embeddings which distort\nthe underlying geometry of the manifold.\n\nOne method to address the regularization problem is to use multiple weight\nvectors in each neighborhood.  This is the essence of *modified locally\nlinear embedding* (MLLE).  MLLE can be  performed with function\n:func:`locally_linear_embedding` or its object-oriented counterpart\n:class:`LocallyLinearEmbedding`, with the keyword ``method = 'modified'``.\nIt requires ``n_neighbors > n_components``.\n\n.. figure:: ../auto_examples/manifold/images/sphx_glr_plot_lle_digits_007.png\n   :target: ../auto_examples/manifold/plot_lle_digits.html\n   :align: center\n   :scale: 50\n\n.. dropdown:: Complexity\n\n  The MLLE algorithm comprises three stages:\n\n  1. **Nearest Neighbors Search**.  Same as standard LLE\n\n  2. **Weight Matrix Construction**. Approximately\n     :math:`O[D N k^3] + O[N (k-D) k^2]`.  The first term is exactly equivalent\n     to that of standard LLE.  The second term has to do with constructing the\n     weight matrix from multiple weights.  In practice, the added cost of\n     constructing the MLLE weight matrix is relatively small compared to the\n     cost of stages 1 and 3.\n\n  3. **Partial Eigenvalue Decomposition**. Same as standard LLE\n\n  The overall complexity of MLLE is\n  :math:`O[D \\log(k) N \\log(N)] + O[D N k^3] + O[N (k-D) k^2] + O[d N^2]`.\n\n  * :math:`N` : number of training data points\n  * :math:`D` : input dimension\n  * :math:`k` : number of nearest neighbors\n  * :math:`d` : output dimension\n\n.. rubric:: References\n\n* `\"MLLE: Modified Locally Linear Embedding Using Multiple Weights\"\n  <https://citeseerx.ist.psu.edu/doc_view/pid/0b060fdbd92cbcc66b383bcaa9ba5e5e624d7ee3>`_\n  Zhang, Z. & Wang, J."
  },
  {
    "filename": "manifold.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\manifold.rst.txt",
    "id": "manifold.rst.txt_chunk_6",
    "header": "Hessian Eigenmapping",
    "text": "Hessian Eigenmapping\n====================\n\nHessian Eigenmapping (also known as Hessian-based LLE: HLLE) is another method\nof solving the regularization problem of LLE.  It revolves around a\nhessian-based quadratic form at each neighborhood which is used to recover\nthe locally linear structure.  Though other implementations note its poor\nscaling with data size, ``sklearn`` implements some algorithmic\nimprovements which make its cost comparable to that of other LLE variants\nfor small output dimension.  HLLE can be  performed with function\n:func:`locally_linear_embedding` or its object-oriented counterpart\n:class:`LocallyLinearEmbedding`, with the keyword ``method = 'hessian'``.\nIt requires ``n_neighbors > n_components * (n_components + 3) / 2``.\n\n.. figure:: ../auto_examples/manifold/images/sphx_glr_plot_lle_digits_008.png\n   :target: ../auto_examples/manifold/plot_lle_digits.html\n   :align: center\n   :scale: 50\n\n.. dropdown:: Complexity\n\n  The HLLE algorithm comprises three stages:\n\n  1. **Nearest Neighbors Search**.  Same as standard LLE\n\n  2. **Weight Matrix Construction**. Approximately\n     :math:`O[D N k^3] + O[N d^6]`.  The first term reflects a similar\n     cost to that of standard LLE.  The second term comes from a QR\n     decomposition of the local hessian estimator.\n\n  3. **Partial Eigenvalue Decomposition**. Same as standard LLE.\n\n  The overall complexity of standard HLLE is\n  :math:`O[D \\log(k) N \\log(N)] + O[D N k^3] + O[N d^6] + O[d N^2]`.\n\n  * :math:`N` : number of training data points\n  * :math:`D` : input dimension\n  * :math:`k` : number of nearest neighbors\n  * :math:`d` : output dimension\n\n.. rubric:: References\n\n* `\"Hessian Eigenmaps: Locally linear embedding techniques for\n  high-dimensional data\" <http://www.pnas.org/content/100/10/5591>`_\n  Donoho, D. & Grimes, C. Proc Natl Acad Sci USA. 100:5591 (2003)\n\n.. _spectral_embedding:"
  },
  {
    "filename": "manifold.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\manifold.rst.txt",
    "id": "manifold.rst.txt_chunk_7",
    "header": "Spectral Embedding",
    "text": "Spectral Embedding\n====================\n\nSpectral Embedding is an approach to calculating a non-linear embedding.\nScikit-learn implements Laplacian Eigenmaps, which finds a low dimensional\nrepresentation of the data using a spectral decomposition of the graph\nLaplacian. The graph generated can be considered as a discrete approximation of\nthe low dimensional manifold in the high dimensional space. Minimization of a\ncost function based on the graph ensures that points close to each other on\nthe manifold are mapped close to each other in the low dimensional space,\npreserving local distances. Spectral embedding can be  performed with the\nfunction :func:`spectral_embedding` or its object-oriented counterpart\n:class:`SpectralEmbedding`.\n\n.. dropdown:: Complexity\n\n  The Spectral Embedding (Laplacian Eigenmaps) algorithm comprises three stages:\n\n  1. **Weighted Graph Construction**. Transform the raw input data into\n     graph representation using affinity (adjacency) matrix representation.\n\n  2. **Graph Laplacian Construction**. unnormalized Graph Laplacian\n     is constructed as :math:`L = D - A` for and normalized one as\n     :math:`L = D^{-\\frac{1}{2}} (D - A) D^{-\\frac{1}{2}}`.\n\n  3. **Partial Eigenvalue Decomposition**. Eigenvalue decomposition is\n     done on graph Laplacian.\n\n  The overall complexity of spectral embedding is\n  :math:`O[D \\log(k) N \\log(N)] + O[D N k^3] + O[d N^2]`.\n\n  * :math:`N` : number of training data points\n  * :math:`D` : input dimension\n  * :math:`k` : number of nearest neighbors\n  * :math:`d` : output dimension\n\n.. rubric:: References\n\n* `\"Laplacian Eigenmaps for Dimensionality Reduction\n  and Data Representation\"\n  <https://web.cse.ohio-state.edu/~mbelkin/papers/LEM_NC_03.pdf>`_\n  M. Belkin, P. Niyogi, Neural Computation, June 2003; 15 (6):1373-1396"
  },
  {
    "filename": "manifold.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\manifold.rst.txt",
    "id": "manifold.rst.txt_chunk_8",
    "header": "Local Tangent Space Alignment",
    "text": "Local Tangent Space Alignment\n=============================\n\nThough not technically a variant of LLE, Local tangent space alignment (LTSA)\nis algorithmically similar enough to LLE that it can be put in this category.\nRather than focusing on preserving neighborhood distances as in LLE, LTSA\nseeks to characterize the local geometry at each neighborhood via its\ntangent space, and performs a global optimization to align these local\ntangent spaces to learn the embedding.  LTSA can be performed with function\n:func:`locally_linear_embedding` or its object-oriented counterpart\n:class:`LocallyLinearEmbedding`, with the keyword ``method = 'ltsa'``.\n\n.. figure:: ../auto_examples/manifold/images/sphx_glr_plot_lle_digits_009.png\n   :target: ../auto_examples/manifold/plot_lle_digits.html\n   :align: center\n   :scale: 50\n\n.. dropdown:: Complexity\n\n  The LTSA algorithm comprises three stages:\n\n  1. **Nearest Neighbors Search**.  Same as standard LLE\n\n  2. **Weight Matrix Construction**. Approximately\n     :math:`O[D N k^3] + O[k^2 d]`.  The first term reflects a similar\n     cost to that of standard LLE.\n\n  3. **Partial Eigenvalue Decomposition**. Same as standard LLE\n\n  The overall complexity of standard LTSA is\n  :math:`O[D \\log(k) N \\log(N)] + O[D N k^3] + O[k^2 d] + O[d N^2]`.\n\n  * :math:`N` : number of training data points\n  * :math:`D` : input dimension\n  * :math:`k` : number of nearest neighbors\n  * :math:`d` : output dimension\n\n.. rubric:: References\n\n* :arxiv:`\"Principal manifolds and nonlinear dimensionality reduction via\n  tangent space alignment\"\n  <cs/0212008>`\n  Zhang, Z. & Zha, H. Journal of Shanghai Univ. 8:406 (2004)\n\n.. _multidimensional_scaling:"
  },
  {
    "filename": "manifold.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\manifold.rst.txt",
    "id": "manifold.rst.txt_chunk_9",
    "header": "Multi-dimensional Scaling (MDS)",
    "text": "Multi-dimensional Scaling (MDS)\n===============================\n\n`Multidimensional scaling <https://en.wikipedia.org/wiki/Multidimensional_scaling>`_\n(:class:`MDS`) seeks a low-dimensional\nrepresentation of the data in which the distances respect well the\ndistances in the original high-dimensional space.\n\nIn general, :class:`MDS` is a technique used for analyzing\ndissimilarity data. It attempts to model dissimilarities as\ndistances in a Euclidean space. The data can be ratings of dissimilarity between\nobjects, interaction frequencies of molecules, or trade indices between\ncountries.\n\nThere exist two types of MDS algorithm: metric and non-metric. In\nscikit-learn, the class :class:`MDS` implements both. In metric MDS,\nthe distances in the embedding space are set as\nclose as possible to the dissimilarity data. In the non-metric\nversion, the algorithm will try to preserve the order of the distances, and\nhence seek for a monotonic relationship between the distances in the embedded\nspace and the input dissimilarities.\n\n.. figure:: ../auto_examples/manifold/images/sphx_glr_plot_lle_digits_010.png\n   :target: ../auto_examples/manifold/plot_lle_digits.html\n   :align: center\n   :scale: 50\n\n\nLet :math:`\\delta_{ij}` be the dissimilarity matrix between the\n:math:`n` input points (possibly arising as some pairwise distances\n:math:`d_{ij}(X)` between the coordinates :math:`X` of the input points).\nDisparities :math:`\\hat{d}_{ij} = f(\\delta_{ij})` are some transformation of\nthe dissimilarities. The MDS objective, called the raw stress, is then\ndefined by :math:`\\sum_{i < j} (\\hat{d}_{ij} - d_{ij}(Z))^2`,\nwhere :math:`d_{ij}(Z)` are the pairwise distances between the\ncoordinates :math:`Z` of the embedded points.\n\n\n.. dropdown:: Metric MDS\n\n  In the metric :class:`MDS` model (sometimes also called *absolute MDS*),\n  disparities are simply equal to the input dissimilarities\n  :math:`\\hat{d}_{ij} = \\delta_{ij}`.\n\n.. dropdown:: Nonmetric MDS\n\n  Non metric :class:`MDS` focuses on the ordination of the data. If\n  :math:`\\delta_{ij} > \\delta_{kl}`, then the embedding\n  seeks to enforce :math:`d_{ij}(Z) > d_{kl}(Z)`. A simple algorithm\n  to enforce proper ordination is to use an\n  isotonic regression of :math:`d_{ij}(Z)` on :math:`\\delta_{ij}`, yielding\n  disparities :math:`\\hat{d}_{ij}` that are a monotonic transformation\n  of dissimilarities :math:`\\delta_{ij}` and hence having the same ordering.\n  This is done repeatedly after every step of the optimization algorithm.\n  In order to avoid the trivial solution where all embedding points are\n  overlapping, the disparities :math:`\\hat{d}_{ij}` are normalized.\n\n  Note that since we only care about relative ordering, our objective should be\n  invariant to simple translation and scaling, however the stress used in metric\n  MDS is sensitive to scaling. To address this, non-metric MDS returns\n  normalized stress, also known as Stress-1, defined as\n\n  .. math::\n      \\sqrt{\\frac{\\sum_{i < j} (\\hat{d}_{ij} - d_{ij}(Z))^2}{\\sum_{i < j}\n      d_{ij}(Z)^2}}.\n\n  Normalized Stress-1 is returned if `normalized_stress=True`.\n\n  .. figure:: ../auto_examples/manifold/images/sphx_glr_plot_mds_001.png\n    :target: ../auto_examples/manifold/plot_mds.html\n    :align: center\n    :scale: 60\n\n.. rubric:: References\n\n* `\"More on Multidimensional Scaling and Unfolding in R: smacof Version 2\"\n  <https://www.jstatsoft.org/article/view/v102i10>`_\n  Mair P, Groenen P., de Leeuw J. Journal of Statistical Software (2022)\n\n* `\"Modern Multidimensional Scaling - Theory and Applications\"\n  <https://www.springer.com/fr/book/9780387251509>`_\n  Borg, I.; Groenen P. Springer Series in Statistics (1997)\n\n* `\"Nonmetric multidimensional scaling: a numerical method\"\n  <http://cda.psych.uiuc.edu/psychometrika_highly_cited_articles/kruskal_1964b.pdf>`_\n  Kruskal, J. Psychometrika, 29 (1964)\n\n* `\"Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis\"\n  <http://cda.psych.uiuc.edu/psychometrika_highly_cited_articles/kruskal_1964a.pdf>`_\n  Kruskal, J. Psychometrika, 29, (1964)\n\n.. _t_sne:"
  },
  {
    "filename": "manifold.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\manifold.rst.txt",
    "id": "manifold.rst.txt_chunk_10",
    "header": "t-distributed Stochastic Neighbor Embedding (t-SNE)",
    "text": "t-distributed Stochastic Neighbor Embedding (t-SNE)\n===================================================\n\nt-SNE (:class:`TSNE`) converts affinities of data points to probabilities.\nThe affinities in the original space are represented by Gaussian joint\nprobabilities and the affinities in the embedded space are represented by\nStudent's t-distributions. This allows t-SNE to be particularly sensitive\nto local structure and has a few other advantages over existing techniques:\n\n* Revealing the structure at many scales on a single map\n* Revealing data that lie in multiple, different, manifolds or clusters\n* Reducing the tendency to crowd points together at the center\n\nWhile Isomap, LLE and variants are best suited to unfold a single continuous\nlow dimensional manifold, t-SNE will focus on the local structure of the data\nand will tend to extract clustered local groups of samples as highlighted on\nthe S-curve example. This ability to group samples based on the local structure\nmight be beneficial to visually disentangle a dataset that comprises several\nmanifolds at once as is the case in the digits dataset.\n\nThe Kullback-Leibler (KL) divergence of the joint\nprobabilities in the original space and the embedded space will be minimized\nby gradient descent. Note that the KL divergence is not convex, i.e.\nmultiple restarts with different initializations will end up in local minima\nof the KL divergence. Hence, it is sometimes useful to try different seeds\nand select the embedding with the lowest KL divergence.\n\nThe disadvantages to using t-SNE are roughly:\n\n* t-SNE is computationally expensive, and can take several hours on million-sample\n  datasets where PCA will finish in seconds or minutes\n* The Barnes-Hut t-SNE method is limited to two or three dimensional embeddings.\n* The algorithm is stochastic and multiple restarts with different seeds can\n  yield different embeddings. However, it is perfectly legitimate to pick the\n  embedding with the least error.\n* Global structure is not explicitly preserved. This problem is mitigated by\n  initializing points with PCA (using `init='pca'`).\n\n\n.. figure:: ../auto_examples/manifold/images/sphx_glr_plot_lle_digits_013.png\n   :target: ../auto_examples/manifold/plot_lle_digits.html\n   :align: center\n   :scale: 50\n\n.. dropdown:: Optimizing t-SNE\n\n  The main purpose of t-SNE is visualization of high-dimensional data. Hence,\n  it works best when the data will be embedded on two or three dimensions.\n\n  Optimizing the KL divergence can be a little bit tricky sometimes. There are\n  five parameters that control the optimization of t-SNE and therefore possibly\n  the quality of the resulting embedding:\n\n  * perplexity\n  * early exaggeration factor\n  * learning rate\n  * maximum number of iterations\n  * angle (not used in the exact method)\n\n  The perplexity is defined as :math:`k=2^{(S)}` where :math:`S` is the Shannon\n  entropy of the conditional probability distribution. The perplexity of a\n  :math:`k`-sided die is :math:`k`, so that :math:`k` is effectively the number of\n  nearest neighbors t-SNE considers when generating the conditional probabilities.\n  Larger perplexities lead to more nearest neighbors and less sensitive to small\n  structure. Conversely a lower perplexity considers a smaller number of\n  neighbors, and thus ignores more global information in favour of the\n  local neighborhood. As dataset sizes get larger more points will be\n  required to get a reasonable sample of the local neighborhood, and hence\n  larger perplexities may be required. Similarly noisier datasets will require\n  larger perplexity values to encompass enough local neighbors to see beyond\n  the background noise.\n\n  The maximum number of iterations is usually high enough and does not need\n  any tuning. The optimization consists of two phases: the early exaggeration\n  phase and the final optimization. During early exaggeration the joint\n  probabilities in the original space will be artificially increased by\n  multiplication with a given factor. Larger factors result in larger gaps\n  between natural clusters in the data. If the factor is too high, the KL\n  divergence could increase during this phase. Usually it does not have to be\n  tuned. A critical parameter is the learning rate. If it is too low gradient\n  descent will get stuck in a bad local minimum. If it is too high the KL\n  divergence will increase during optimization. A heuristic suggested in\n  Belkina et al. (2019) is to set the learning rate to the sample size\n  divided by the early exaggeration factor. We implement this heuristic\n  as `learning_rate='auto'` argument. More tips can be found in\n  Laurens van der Maaten's FAQ (see references). The last parameter, angle,\n  is a tradeoff between performance and accuracy. Larger angles imply that we\n  can approximate larger regions by a single point, leading to better speed\n  but less accurate results.\n\n  `\"How to Use t-SNE Effectively\" <https://distill.pub/2016/misread-tsne/>`_\n  provides a good discussion of the effects of the various parameters, as well\n  as interactive plots to explore the effects of different parameters.\n\n.. dropdown:: Barnes-Hut t-SNE\n\n  The Barnes-Hut t-SNE that has been implemented here is usually much slower than\n  other manifold learning algorithms. The optimization is quite difficult\n  and the computation of the gradient is :math:`O[d N log(N)]`, where :math:`d`\n  is the number of output dimensions and :math:`N` is the number of samples. The\n  Barnes-Hut method improves on the exact method where t-SNE complexity is\n  :math:`O[d N^2]`, but has several other notable differences:\n\n  * The Barnes-Hut implementation only works when the target dimensionality is 3\n    or less. The 2D case is typical when building visualizations.\n  * Barnes-Hut only works with dense input data. Sparse data matrices can only be\n    embedded with the exact method or can be approximated by a dense low rank\n    projection for instance using :class:`~sklearn.decomposition.PCA`\n  * Barnes-Hut is an approximation of the exact method. The approximation is\n    parameterized with the angle parameter, therefore the angle parameter is\n    unused when method=\"exact\"\n  * Barnes-Hut is significantly more scalable. Barnes-Hut can be used to embed\n    hundreds of thousands of data points while the exact method can handle\n    thousands of samples before becoming computationally intractable\n\n  For visualization purpose (which is the main use case of t-SNE), using the\n  Barnes-Hut method is strongly recommended. The exact t-SNE method is useful\n  for checking the theoretical properties of the embedding possibly in higher\n  dimensional space but limited to small datasets due to computational constraints.\n\n  Also note that the digits labels roughly match the natural grouping found by\n  t-SNE while the linear 2D projection of the PCA model yields a representation\n  where label regions largely overlap. This is a strong clue that this data can\n  be well separated by non linear methods that focus on the local structure (e.g.\n  an SVM with a Gaussian RBF kernel). However, failing to visualize well\n  separated homogeneously labeled groups with t-SNE in 2D does not necessarily\n  imply that the data cannot be correctly classified by a supervised model. It\n  might be the case that 2 dimensions are not high enough to accurately represent\n  the internal structure of the data.\n\n.. rubric:: References\n\n* `\"Visualizing High-Dimensional Data Using t-SNE\"\n  <https://jmlr.org/papers/v9/vandermaaten08a.html>`_\n  van der Maaten, L.J.P.; Hinton, G. Journal of Machine Learning Research (2008)\n\n* `\"t-Distributed Stochastic Neighbor Embedding\"\n  <https://lvdmaaten.github.io/tsne/>`_ van der Maaten, L.J.P.\n\n* `\"Accelerating t-SNE using Tree-Based Algorithms\"\n  <https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf>`_\n  van der Maaten, L.J.P.; Journal of Machine Learning Research 15(Oct):3221-3245, 2014.\n\n* `\"Automated optimized parameters for T-distributed stochastic neighbor\n  embedding improve visualization and analysis of large datasets\"\n  <https://www.nature.com/articles/s41467-019-13055-y>`_\n  Belkina, A.C., Ciccolella, C.O., Anno, R., Halpert, R., Spidlen, J.,\n  Snyder-Cappione, J.E., Nature Communications 10, 5415 (2019)."
  },
  {
    "filename": "manifold.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\manifold.rst.txt",
    "id": "manifold.rst.txt_chunk_11",
    "header": "Tips on practical use",
    "text": "Tips on practical use\n=====================\n\n* Make sure the same scale is used over all features. Because manifold\n  learning methods are based on a nearest-neighbor search, the algorithm\n  may perform poorly otherwise.  See :ref:`StandardScaler <preprocessing_scaler>`\n  for convenient ways of scaling heterogeneous data.\n\n* The reconstruction error computed by each routine can be used to choose\n  the optimal output dimension.  For a :math:`d`-dimensional manifold embedded\n  in a :math:`D`-dimensional parameter space, the reconstruction error will\n  decrease as ``n_components`` is increased until ``n_components == d``.\n\n* Note that noisy data can \"short-circuit\" the manifold, in essence acting\n  as a bridge between parts of the manifold that would otherwise be\n  well-separated.  Manifold learning on noisy and/or incomplete data is\n  an active area of research.\n\n* Certain input configurations can lead to singular weight matrices, for\n  example when more than two points in the dataset are identical, or when\n  the data is split into disjointed groups.  In this case, ``solver='arpack'``\n  will fail to find the null space.  The easiest way to address this is to\n  use ``solver='dense'`` which will work on a singular matrix, though it may\n  be very slow depending on the number of input points.  Alternatively, one\n  can attempt to understand the source of the singularity: if it is due to\n  disjoint sets, increasing ``n_neighbors`` may help.  If it is due to\n  identical points in the dataset, removing these points may help.\n\n.. seealso::\n\n   :ref:`random_trees_embedding` can also be useful to derive non-linear\n   representations of feature space, but it does not perform\n   dimensionality reduction."
  },
  {
    "filename": "metrics.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\metrics.rst.txt",
    "id": "metrics.rst.txt_chunk_0",
    "header": ".. _metrics:",
    "text": ".. _metrics:"
  },
  {
    "filename": "metrics.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\metrics.rst.txt",
    "id": "metrics.rst.txt_chunk_1",
    "header": "Pairwise metrics, Affinities and Kernels",
    "text": "Pairwise metrics, Affinities and Kernels\n========================================\n\nThe :mod:`sklearn.metrics.pairwise` submodule implements utilities to evaluate\npairwise distances or affinity of sets of samples.\n\nThis module contains both distance metrics and kernels. A brief summary is\ngiven on the two here.\n\nDistance metrics are functions ``d(a, b)`` such that ``d(a, b) < d(a, c)``\nif objects ``a`` and ``b`` are considered \"more similar\" than objects ``a``\nand ``c``. Two objects exactly alike would have a distance of zero.\nOne of the most popular examples is Euclidean distance.\nTo be a 'true' metric, it must obey the following four conditions::\n\n    1. d(a, b) >= 0, for all a and b\n    2. d(a, b) == 0, if and only if a = b, positive definiteness\n    3. d(a, b) == d(b, a), symmetry\n    4. d(a, c) <= d(a, b) + d(b, c), the triangle inequality\n\nKernels are measures of similarity, i.e. ``s(a, b) > s(a, c)``\nif objects ``a`` and ``b`` are considered \"more similar\" than objects\n``a`` and ``c``. A kernel must also be positive semi-definite.\n\nThere are a number of ways to convert between a distance metric and a\nsimilarity measure, such as a kernel. Let ``D`` be the distance, and ``S`` be\nthe kernel:\n\n1. ``S = np.exp(-D * gamma)``, where one heuristic for choosing\n    ``gamma`` is ``1 / num_features``\n2. ``S = 1. / (D / np.max(D))``\n\n\n.. currentmodule:: sklearn.metrics\n\nThe distances between the row vectors of ``X`` and the row vectors of ``Y``\ncan be evaluated using :func:`pairwise_distances`. If ``Y`` is omitted the\npairwise distances of the row vectors of ``X`` are calculated. Similarly,\n:func:`pairwise.pairwise_kernels` can be used to calculate the kernel between `X`\nand `Y` using different kernel functions. See the API reference for more\ndetails.\n\n    >>> import numpy as np\n    >>> from sklearn.metrics import pairwise_distances\n    >>> from sklearn.metrics.pairwise import pairwise_kernels\n    >>> X = np.array([[2, 3], [3, 5], [5, 8]])\n    >>> Y = np.array([[1, 0], [2, 1]])\n    >>> pairwise_distances(X, Y, metric='manhattan')\n    array([[ 4.,  2.],\n           [ 7.,  5.],\n           [12., 10.]])\n    >>> pairwise_distances(X, metric='manhattan')\n    array([[0., 3., 8.],\n           [3., 0., 5.],\n           [8., 5., 0.]])\n    >>> pairwise_kernels(X, Y, metric='linear')\n    array([[ 2.,  7.],\n           [ 3., 11.],\n           [ 5., 18.]])\n\n\n.. currentmodule:: sklearn.metrics.pairwise\n\n.. _cosine_similarity:"
  },
  {
    "filename": "metrics.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\metrics.rst.txt",
    "id": "metrics.rst.txt_chunk_2",
    "header": "Cosine similarity",
    "text": "Cosine similarity\n-----------------\n:func:`cosine_similarity` computes the L2-normalized dot product of vectors.\nThat is, if :math:`x` and :math:`y` are row vectors,\ntheir cosine similarity :math:`k` is defined as:\n\n.. math::\n\n    k(x, y) = \\frac{x y^\\top}{\\|x\\| \\|y\\|}\n\nThis is called cosine similarity, because Euclidean (L2) normalization\nprojects the vectors onto the unit sphere,\nand their dot product is then the cosine of the angle between the points\ndenoted by the vectors.\n\nThis kernel is a popular choice for computing the similarity of documents\nrepresented as tf-idf vectors.\n:func:`cosine_similarity` accepts ``scipy.sparse`` matrices.\n(Note that the tf-idf functionality in ``sklearn.feature_extraction.text``\ncan produce normalized vectors, in which case :func:`cosine_similarity`\nis equivalent to :func:`linear_kernel`, only slower.)\n\n.. rubric:: References\n\n* C.D. Manning, P. Raghavan and H. Sch\u00fctze (2008). Introduction to\n  Information Retrieval. Cambridge University Press.\n  https://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html\n\n.. _linear_kernel:"
  },
  {
    "filename": "metrics.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\metrics.rst.txt",
    "id": "metrics.rst.txt_chunk_3",
    "header": "Linear kernel",
    "text": "Linear kernel\n-------------\nThe function :func:`linear_kernel` computes the linear kernel, that is, a\nspecial case of :func:`polynomial_kernel` with ``degree=1`` and ``coef0=0`` (homogeneous).\nIf ``x`` and ``y`` are column vectors, their linear kernel is:\n\n.. math::\n\n    k(x, y) = x^\\top y\n\n.. _polynomial_kernel:"
  },
  {
    "filename": "metrics.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\metrics.rst.txt",
    "id": "metrics.rst.txt_chunk_4",
    "header": "Polynomial kernel",
    "text": "Polynomial kernel\n-----------------\nThe function :func:`polynomial_kernel` computes the degree-d polynomial kernel\nbetween two vectors. The polynomial kernel represents the similarity between two\nvectors. Conceptually, the polynomial kernel considers not only the similarity\nbetween vectors under the same dimension, but also across dimensions. When used\nin machine learning algorithms, this allows to account for feature interaction.\n\nThe polynomial kernel is defined as:\n\n.. math::\n\n    k(x, y) = (\\gamma x^\\top y +c_0)^d\n\nwhere:\n\n* ``x``, ``y`` are the input vectors\n* ``d`` is the kernel degree\n\nIf :math:`c_0 = 0` the kernel is said to be homogeneous.\n\n.. _sigmoid_kernel:"
  },
  {
    "filename": "metrics.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\metrics.rst.txt",
    "id": "metrics.rst.txt_chunk_5",
    "header": "Sigmoid kernel",
    "text": "Sigmoid kernel\n--------------\nThe function :func:`sigmoid_kernel` computes the sigmoid kernel between two\nvectors. The sigmoid kernel is also known as hyperbolic tangent, or Multilayer\nPerceptron (because, in the neural network field, it is often used as neuron\nactivation function). It is defined as:\n\n.. math::\n\n    k(x, y) = \\tanh( \\gamma x^\\top y + c_0)\n\nwhere:\n\n* ``x``, ``y`` are the input vectors\n* :math:`\\gamma` is known as slope\n* :math:`c_0` is known as intercept\n\n.. _rbf_kernel:"
  },
  {
    "filename": "metrics.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\metrics.rst.txt",
    "id": "metrics.rst.txt_chunk_6",
    "header": "RBF kernel",
    "text": "RBF kernel\n----------\nThe function :func:`rbf_kernel` computes the radial basis function (RBF) kernel\nbetween two vectors. This kernel is defined as:\n\n.. math::\n\n    k(x, y) = \\exp( -\\gamma \\| x-y \\|^2)\n\nwhere ``x`` and ``y`` are the input vectors. If :math:`\\gamma = \\sigma^{-2}`\nthe kernel is known as the Gaussian kernel of variance :math:`\\sigma^2`.\n\n.. _laplacian_kernel:"
  },
  {
    "filename": "metrics.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\metrics.rst.txt",
    "id": "metrics.rst.txt_chunk_7",
    "header": "Laplacian kernel",
    "text": "Laplacian kernel\n----------------\nThe function :func:`laplacian_kernel` is a variant on the radial basis\nfunction kernel defined as:\n\n.. math::\n\n    k(x, y) = \\exp( -\\gamma \\| x-y \\|_1)\n\nwhere ``x`` and ``y`` are the input vectors and :math:`\\|x-y\\|_1` is the\nManhattan distance between the input vectors.\n\nIt has proven useful in ML applied to noiseless data.\nSee e.g. `Machine learning for quantum mechanics in a nutshell\n<https://onlinelibrary.wiley.com/doi/10.1002/qua.24954/abstract/>`_.\n\n.. _chi2_kernel:"
  },
  {
    "filename": "metrics.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\metrics.rst.txt",
    "id": "metrics.rst.txt_chunk_8",
    "header": "Chi-squared kernel",
    "text": "Chi-squared kernel\n------------------\nThe chi-squared kernel is a very popular choice for training non-linear SVMs in\ncomputer vision applications.\nIt can be computed using :func:`chi2_kernel` and then passed to an\n:class:`~sklearn.svm.SVC` with ``kernel=\"precomputed\"``::\n\n    >>> from sklearn.svm import SVC\n    >>> from sklearn.metrics.pairwise import chi2_kernel\n    >>> X = [[0, 1], [1, 0], [.2, .8], [.7, .3]]\n    >>> y = [0, 1, 0, 1]\n    >>> K = chi2_kernel(X, gamma=.5)\n    >>> K\n    array([[1.        , 0.36787944, 0.89483932, 0.58364548],\n           [0.36787944, 1.        , 0.51341712, 0.83822343],\n           [0.89483932, 0.51341712, 1.        , 0.7768366 ],\n           [0.58364548, 0.83822343, 0.7768366 , 1.        ]])\n\n    >>> svm = SVC(kernel='precomputed').fit(K, y)\n    >>> svm.predict(K)\n    array([0, 1, 0, 1])\n\nIt can also be directly used as the ``kernel`` argument::\n\n    >>> svm = SVC(kernel=chi2_kernel).fit(X, y)\n    >>> svm.predict(X)\n    array([0, 1, 0, 1])\n\n\nThe chi squared kernel is given by\n\n.. math::\n\n        k(x, y) = \\exp \\left (-\\gamma \\sum_i \\frac{(x[i] - y[i]) ^ 2}{x[i] + y[i]} \\right )\n\nThe data is assumed to be non-negative, and is often normalized to have an L1-norm of one.\nThe normalization is rationalized with the connection to the chi squared distance,\nwhich is a distance between discrete probability distributions.\n\nThe chi squared kernel is most commonly used on histograms (bags) of visual words.\n\n.. rubric:: References\n\n* Zhang, J. and Marszalek, M. and Lazebnik, S. and Schmid, C.\n  Local features and kernels for classification of texture and object\n  categories: A comprehensive study\n  International Journal of Computer Vision 2007\n  https://hal.archives-ouvertes.fr/hal-00171412/document"
  },
  {
    "filename": "mixture.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\mixture.rst.txt",
    "id": "mixture.rst.txt_chunk_0",
    "header": ".. _mixture:",
    "text": ".. _mixture:\n\n.. _gmm:\n\n======================="
  },
  {
    "filename": "mixture.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\mixture.rst.txt",
    "id": "mixture.rst.txt_chunk_1",
    "header": "Gaussian mixture models",
    "text": "Gaussian mixture models\n=======================\n\n.. currentmodule:: sklearn.mixture\n\n``sklearn.mixture`` is a package which enables one to learn\nGaussian Mixture Models (diagonal, spherical, tied and full covariance\nmatrices supported), sample them, and estimate them from\ndata. Facilities to help determine the appropriate number of\ncomponents are also provided.\n\n.. figure:: ../auto_examples/mixture/images/sphx_glr_plot_gmm_pdf_001.png\n  :target: ../auto_examples/mixture/plot_gmm_pdf.html\n  :align: center\n  :scale: 50%\n\n  **Two-component Gaussian mixture model:** *data points, and equi-probability\n  surfaces of the model.*\n\nA Gaussian mixture model is a probabilistic model that assumes all the\ndata points are generated from a mixture of a finite number of\nGaussian distributions with unknown parameters. One can think of\nmixture models as generalizing k-means clustering to incorporate\ninformation about the covariance structure of the data as well as the\ncenters of the latent Gaussians.\n\nScikit-learn implements different classes to estimate Gaussian\nmixture models, that correspond to different estimation strategies,\ndetailed below."
  },
  {
    "filename": "mixture.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\mixture.rst.txt",
    "id": "mixture.rst.txt_chunk_2",
    "header": "Gaussian Mixture",
    "text": "Gaussian Mixture\n================\n\nThe :class:`GaussianMixture` object implements the\n:ref:`expectation-maximization <expectation_maximization>` (EM)\nalgorithm for fitting mixture-of-Gaussian models. It can also draw\nconfidence ellipsoids for multivariate models, and compute the\nBayesian Information Criterion to assess the number of clusters in the\ndata. A :meth:`GaussianMixture.fit` method is provided that learns a Gaussian\nMixture Model from training data. Given test data, it can assign to each\nsample the Gaussian it most probably belongs to using\nthe :meth:`GaussianMixture.predict` method.\n\n..\n    Alternatively, the probability of each\n    sample belonging to the various Gaussians may be retrieved using the\n    :meth:`GaussianMixture.predict_proba` method.\n\nThe :class:`GaussianMixture` comes with different options to constrain the\ncovariance of the difference classes estimated: spherical, diagonal, tied or\nfull covariance.\n\n.. figure:: ../auto_examples/mixture/images/sphx_glr_plot_gmm_covariances_001.png\n   :target: ../auto_examples/mixture/plot_gmm_covariances.html\n   :align: center\n   :scale: 75%\n\n.. rubric:: Examples\n\n* See :ref:`sphx_glr_auto_examples_mixture_plot_gmm_covariances.py` for an example of\n  using the Gaussian mixture as clustering on the iris dataset.\n\n* See :ref:`sphx_glr_auto_examples_mixture_plot_gmm_pdf.py` for an example on plotting the\n  density estimation.\n\n.. dropdown:: Pros and cons of class GaussianMixture\n\n  .. rubric:: Pros\n\n  :Speed: It is the fastest algorithm for learning mixture models\n\n  :Agnostic: As this algorithm maximizes only the likelihood, it\n    will not bias the means towards zero, or bias the cluster sizes to\n    have specific structures that might or might not apply.\n\n  .. rubric:: Cons\n\n  :Singularities: When one has insufficiently many points per\n    mixture, estimating the covariance matrices becomes difficult,\n    and the algorithm is known to diverge and find solutions with\n    infinite likelihood unless one regularizes the covariances artificially.\n\n  :Number of components: This algorithm will always use all the\n    components it has access to, needing held-out data\n    or information theoretical criteria to decide how many components to use\n    in the absence of external cues.\n\n.. dropdown:: Selecting the number of components in a classical Gaussian Mixture model\n\n  The BIC criterion can be used to select the number of components in a Gaussian\n  Mixture in an efficient way. In theory, it recovers the true number of\n  components only in the asymptotic regime (i.e. if much data is available and\n  assuming that the data was actually generated i.i.d. from a mixture of Gaussian\n  distributions). Note that using a :ref:`Variational Bayesian Gaussian mixture <bgmm>`\n  avoids the specification of the number of components for a Gaussian mixture\n  model.\n\n  .. figure:: ../auto_examples/mixture/images/sphx_glr_plot_gmm_selection_002.png\n    :target: ../auto_examples/mixture/plot_gmm_selection.html\n    :align: center\n    :scale: 50%\n\n  .. rubric:: Examples\n\n  * See :ref:`sphx_glr_auto_examples_mixture_plot_gmm_selection.py` for an example\n    of model selection performed with classical Gaussian mixture.\n\n.. _expectation_maximization:\n\n.. dropdown:: Estimation algorithm expectation-maximization\n\n  The main difficulty in learning Gaussian mixture models from unlabeled\n  data is that one usually doesn't know which points came from\n  which latent component (if one has access to this information it gets\n  very easy to fit a separate Gaussian distribution to each set of\n  points). `Expectation-maximization\n  <https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm>`_\n  is a well-founded statistical\n  algorithm to get around this problem by an iterative process. First\n  one assumes random components (randomly centered on data points,\n  learned from k-means, or even just normally distributed around the\n  origin) and computes for each point a probability of being generated by\n  each component of the model. Then, one tweaks the\n  parameters to maximize the likelihood of the data given those\n  assignments. Repeating this process is guaranteed to always converge\n  to a local optimum.\n\n.. dropdown:: Choice of the Initialization method\n\n  There is a choice of four initialization methods (as well as inputting user defined\n  initial means) to generate the initial centers for the model components:\n\n  k-means (default)\n    This applies a traditional k-means clustering algorithm.\n    This can be computationally expensive compared to other initialization methods.\n\n  k-means++\n    This uses the initialization method of k-means clustering: k-means++.\n    This will pick the first center at random from the data. Subsequent centers will be\n    chosen from a weighted distribution of the data favouring points further away from\n    existing centers. k-means++ is the default initialization for k-means so will be\n    quicker than running a full k-means but can still take a significant amount of\n    time for large data sets with many components.\n\n  random_from_data\n    This will pick random data points from the input data as the initial\n    centers. This is a very fast method of initialization but can produce non-convergent\n    results if the chosen points are too close to each other.\n\n  random\n    Centers are chosen as a small perturbation away from the mean of all data.\n    This method is simple but can lead to the model taking longer to converge.\n\n  .. figure:: ../auto_examples/mixture/images/sphx_glr_plot_gmm_init_001.png\n    :target: ../auto_examples/mixture/plot_gmm_init.html\n    :align: center\n    :scale: 50%\n\n  .. rubric:: Examples\n\n  * See :ref:`sphx_glr_auto_examples_mixture_plot_gmm_init.py` for an example of\n    using different initializations in Gaussian Mixture.\n\n.. _bgmm:"
  },
  {
    "filename": "mixture.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\mixture.rst.txt",
    "id": "mixture.rst.txt_chunk_3",
    "header": "Variational Bayesian Gaussian Mixture",
    "text": "Variational Bayesian Gaussian Mixture\n=====================================\n\nThe :class:`BayesianGaussianMixture` object implements a variant of the\nGaussian mixture model with variational inference algorithms. The API is\nsimilar to the one defined by :class:`GaussianMixture`.\n\n.. _variational_inference:\n\n**Estimation algorithm: variational inference**\n\nVariational inference is an extension of expectation-maximization that\nmaximizes a lower bound on model evidence (including\npriors) instead of data likelihood. The principle behind\nvariational methods is the same as expectation-maximization (that is\nboth are iterative algorithms that alternate between finding the\nprobabilities for each point to be generated by each mixture and\nfitting the mixture to these assigned points), but variational\nmethods add regularization by integrating information from prior\ndistributions. This avoids the singularities often found in\nexpectation-maximization solutions but introduces some subtle biases\nto the model. Inference is often notably slower, but not usually as\nmuch so as to render usage unpractical.\n\nDue to its Bayesian nature, the variational algorithm needs more hyperparameters\nthan expectation-maximization, the most important of these being the\nconcentration parameter ``weight_concentration_prior``. Specifying a low value\nfor the concentration prior will make the model put most of the weight on a few\ncomponents and set the remaining components' weights very close to zero. High\nvalues of the concentration prior will allow a larger number of components to\nbe active in the mixture.\n\nThe parameters implementation of the :class:`BayesianGaussianMixture` class\nproposes two types of prior for the weights distribution: a finite mixture model\nwith Dirichlet distribution and an infinite mixture model with the Dirichlet\nProcess. In practice Dirichlet Process inference algorithm is approximated and\nuses a truncated distribution with a fixed maximum number of components (called\nthe Stick-breaking representation). The number of components actually used\nalmost always depends on the data.\n\nThe next figure compares the results obtained for the different types of the\nweight concentration prior (parameter ``weight_concentration_prior_type``)\nfor different values of ``weight_concentration_prior``.\nHere, we can see the value of the ``weight_concentration_prior`` parameter\nhas a strong impact on the effective number of active components obtained. We\ncan also notice that large values for the concentration weight prior lead to\nmore uniform weights when the type of prior is 'dirichlet_distribution' while\nthis is not necessarily the case for the 'dirichlet_process' type (used by\ndefault).\n\n.. |plot_bgmm| image:: ../auto_examples/mixture/images/sphx_glr_plot_concentration_prior_001.png\n   :target: ../auto_examples/mixture/plot_concentration_prior.html\n   :scale: 48%\n\n.. |plot_dpgmm| image:: ../auto_examples/mixture/images/sphx_glr_plot_concentration_prior_002.png\n   :target: ../auto_examples/mixture/plot_concentration_prior.html\n   :scale: 48%\n\n.. centered:: |plot_bgmm| |plot_dpgmm|\n\nThe examples below compare Gaussian mixture models with a fixed number of\ncomponents, to the variational Gaussian mixture models with a Dirichlet process\nprior. Here, a classical Gaussian mixture is fitted with 5 components on a\ndataset composed of 2 clusters. We can see that the variational Gaussian mixture\nwith a Dirichlet process prior is able to limit itself to only 2 components\nwhereas the Gaussian mixture fits the data with a fixed number of components\nthat has to be set a priori by the user. In this case the user has selected\n``n_components=5`` which does not match the true generative distribution of this\ntoy dataset. Note that with very little observations, the variational Gaussian\nmixture models with a Dirichlet process prior can take a conservative stand, and\nfit only one component.\n\n.. figure:: ../auto_examples/mixture/images/sphx_glr_plot_gmm_001.png\n   :target: ../auto_examples/mixture/plot_gmm.html\n   :align: center\n   :scale: 70%\n\n\nOn the following figure we are fitting a dataset not well-depicted by a\nGaussian mixture. Adjusting the ``weight_concentration_prior``, parameter of the\n:class:`BayesianGaussianMixture` controls the number of components used to fit\nthis data. We also present on the last two plots a random sampling generated\nfrom the two resulting mixtures.\n\n.. figure:: ../auto_examples/mixture/images/sphx_glr_plot_gmm_sin_001.png\n   :target: ../auto_examples/mixture/plot_gmm_sin.html\n   :align: center\n   :scale: 65%\n\n\n\n.. rubric:: Examples\n\n* See :ref:`sphx_glr_auto_examples_mixture_plot_gmm.py` for an example on\n  plotting the confidence ellipsoids for both :class:`GaussianMixture`\n  and :class:`BayesianGaussianMixture`.\n\n* :ref:`sphx_glr_auto_examples_mixture_plot_gmm_sin.py` shows using\n  :class:`GaussianMixture` and :class:`BayesianGaussianMixture` to fit a\n  sine wave.\n\n* See :ref:`sphx_glr_auto_examples_mixture_plot_concentration_prior.py`\n  for an example plotting the confidence ellipsoids for the\n  :class:`BayesianGaussianMixture` with different\n  ``weight_concentration_prior_type`` for different values of the parameter\n  ``weight_concentration_prior``.\n\n.. dropdown:: Pros and cons of variational inference with BayesianGaussianMixture\n\n  .. rubric:: Pros\n\n  :Automatic selection: When ``weight_concentration_prior`` is small enough and\n    ``n_components`` is larger than what is found necessary by the model, the\n    Variational Bayesian mixture model has a natural tendency to set some mixture\n    weights values close to zero. This makes it possible to let the model choose\n    a suitable number of effective components automatically. Only an upper bound\n    of this number needs to be provided. Note however that the \"ideal\" number of\n    active components is very application specific and is typically ill-defined\n    in a data exploration setting.\n\n  :Less sensitivity to the number of parameters: Unlike finite models, which will\n    almost always use all components as much as they can, and hence will produce\n    wildly different solutions for different numbers of components, the\n    variational inference with a Dirichlet process prior\n    (``weight_concentration_prior_type='dirichlet_process'``) won't change much\n    with changes to the parameters, leading to more stability and less tuning.\n\n  :Regularization: Due to the incorporation of prior information,\n    variational solutions have less pathological special cases than\n    expectation-maximization solutions.\n\n  .. rubric:: Cons\n\n  :Speed: The extra parametrization necessary for variational inference makes\n    inference slower, although not by much.\n\n  :Hyperparameters: This algorithm needs an extra hyperparameter\n    that might need experimental tuning via cross-validation.\n\n  :Bias: There are many implicit biases in the inference algorithms (and also in\n    the Dirichlet process if used), and whenever there is a mismatch between\n    these biases and the data it might be possible to fit better models using a\n    finite mixture.\n\n.. _dirichlet_process:"
  },
  {
    "filename": "mixture.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\mixture.rst.txt",
    "id": "mixture.rst.txt_chunk_4",
    "header": "The Dirichlet Process",
    "text": "The Dirichlet Process\n---------------------\n\nHere we describe variational inference algorithms on Dirichlet process\nmixture. The Dirichlet process is a prior probability distribution on\n*clusterings with an infinite, unbounded, number of partitions*.\nVariational techniques let us incorporate this prior structure on\nGaussian mixture models at almost no penalty in inference time, comparing\nwith a finite Gaussian mixture model.\n\nAn important question is how can the Dirichlet process use an infinite,\nunbounded number of clusters and still be consistent. While a full explanation\ndoesn't fit this manual, one can think of its `stick breaking process\n<https://en.wikipedia.org/wiki/Dirichlet_process#The_stick-breaking_process>`_\nanalogy to help understanding it. The stick breaking process is a generative\nstory for the Dirichlet process. We start with a unit-length stick and in each\nstep we break off a portion of the remaining stick. Each time, we associate the\nlength of the piece of the stick to the proportion of points that falls into a\ngroup of the mixture. At the end, to represent the infinite mixture, we\nassociate the last remaining piece of the stick to the proportion of points\nthat don't fall into all the other groups. The length of each piece is a random\nvariable with probability proportional to the concentration parameter. Smaller\nvalues of the concentration will divide the unit-length into larger pieces of\nthe stick (defining more concentrated distribution). Larger concentration\nvalues will create smaller pieces of the stick (increasing the number of\ncomponents with non zero weights).\n\nVariational inference techniques for the Dirichlet process still work\nwith a finite approximation to this infinite mixture model, but\ninstead of having to specify a priori how many components one wants to\nuse, one just specifies the concentration parameter and an upper bound\non the number of mixture components (this upper bound, assuming it is\nhigher than the \"true\" number of components, affects only algorithmic\ncomplexity, not the actual number of components used)."
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_0",
    "header": ".. currentmodule:: sklearn",
    "text": ".. currentmodule:: sklearn\n\n.. _model_evaluation:\n\n==========================================================="
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_1",
    "header": "Metrics and scoring: quantifying the quality of predictions",
    "text": "Metrics and scoring: quantifying the quality of predictions\n===========================================================\n\n.. _which_scoring_function:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_2",
    "header": "Which scoring function should I use?",
    "text": "Which scoring function should I use?\n====================================\n\nBefore we take a closer look into the details of the many scores and\n:term:`evaluation metrics`, we want to give some guidance, inspired by statistical\ndecision theory, on the choice of **scoring functions** for **supervised learning**,\nsee [Gneiting2009]_:\n\n- *Which scoring function should I use?*\n- *Which scoring function is a good one for my task?*\n\nIn a nutshell, if the scoring function is given, e.g. in a kaggle competition\nor in a business context, use that one.\nIf you are free to choose, it starts by considering the ultimate goal and application\nof the prediction. It is useful to distinguish two steps:\n\n* Predicting\n* Decision making\n\n**Predicting:**\nUsually, the response variable :math:`Y` is a random variable, in the sense that there\nis *no deterministic* function :math:`Y = g(X)` of the features :math:`X`.\nInstead, there is a probability distribution :math:`F` of :math:`Y`.\nOne can aim to predict the whole distribution, known as *probabilistic prediction*,\nor---more the focus of scikit-learn---issue a *point prediction* (or point forecast)\nby choosing a property or functional of that distribution :math:`F`.\nTypical examples are the mean (expected value), the median or a quantile of the\nresponse variable :math:`Y` (conditionally on :math:`X`).\n\nOnce that is settled, use a **strictly consistent** scoring function for that\n(target) functional, see [Gneiting2009]_.\nThis means using a scoring function that is aligned with *measuring the distance\nbetween predictions* `y_pred` *and the true target functional using observations of*\n:math:`Y`, i.e. `y_true`.\nFor classification **strictly proper scoring rules**, see\n`Wikipedia entry for Scoring rule <https://en.wikipedia.org/wiki/Scoring_rule>`_\nand [Gneiting2007]_, coincide with strictly consistent scoring functions.\nThe table further below provides examples.\nOne could say that consistent scoring functions act as *truth serum* in that\nthey guarantee *\"that truth telling [. . .] is an optimal strategy in\nexpectation\"* [Gneiting2014]_.\n\nOnce a strictly consistent scoring function is chosen, it is best used for both: as\nloss function for model training and as metric/score in model evaluation and model\ncomparison.\n\nNote that for regressors, the prediction is done with :term:`predict` while for\nclassifiers it is usually :term:`predict_proba`.\n\n**Decision Making:**\nThe most common decisions are done on binary classification tasks, where the result of\n:term:`predict_proba` is turned into a single outcome, e.g., from the predicted\nprobability of rain a decision is made on how to act (whether to take mitigating\nmeasures like an umbrella or not).\nFor classifiers, this is what :term:`predict` returns.\nSee also :ref:`TunedThresholdClassifierCV`.\nThere are many scoring functions which measure different aspects of such a\ndecision, most of them are covered with or derived from the\n:func:`metrics.confusion_matrix`.\n\n**List of strictly consistent scoring functions:**\nHere, we list some of the most relevant statistical functionals and corresponding\nstrictly consistent scoring functions for tasks in practice. Note that the list is not\ncomplete and that there are more of them.\nFor further criteria on how to select a specific one, see [Fissler2022]_.\n\n==================  ===================================================  ====================  =================================\nfunctional          scoring or loss function                             response `y`          prediction\n==================  ===================================================  ====================  =================================\n**Classification**\nmean                :ref:`Brier score <brier_score_loss>` :sup:`1`       multi-class           ``predict_proba``\nmean                :ref:`log loss <log_loss>`                           multi-class           ``predict_proba``\nmode                :ref:`zero-one loss <zero_one_loss>` :sup:`2`        multi-class           ``predict``, categorical\n**Regression**\nmean                :ref:`squared error <mean_squared_error>` :sup:`3`   all reals             ``predict``, all reals\nmean                :ref:`Poisson deviance <mean_tweedie_deviance>`      non-negative          ``predict``, strictly positive\nmean                :ref:`Gamma deviance <mean_tweedie_deviance>`        strictly positive     ``predict``, strictly positive\nmean                :ref:`Tweedie deviance <mean_tweedie_deviance>`      depends on ``power``  ``predict``, depends on ``power``\nmedian              :ref:`absolute error <mean_absolute_error>`          all reals             ``predict``, all reals\nquantile            :ref:`pinball loss <pinball_loss>`                   all reals             ``predict``, all reals\nmode                no consistent one exists                             reals\n==================  ===================================================  ====================  =================================\n\n:sup:`1` The Brier score is just a different name for the squared error in case of\nclassification.\n\n:sup:`2` The zero-one loss is only consistent but not strictly consistent for the mode.\nThe zero-one loss is equivalent to one minus the accuracy score, meaning it gives\ndifferent score values but the same ranking.\n\n:sup:`3` R\u00b2 gives the same ranking as squared error.\n\n**Fictitious Example:**\nLet's make the above arguments more tangible. Consider a setting in network reliability\nengineering, such as maintaining stable internet or Wi-Fi connections.\nAs provider of the network, you have access to the dataset of log entries of network\nconnections containing network load over time and many interesting features.\nYour goal is to improve the reliability of the connections.\nIn fact, you promise your customers that on at least 99% of all days there are no\nconnection discontinuities larger than 1 minute.\nTherefore, you are interested in a prediction of the 99% quantile (of longest\nconnection interruption duration per day) in order to know in advance when to add\nmore bandwidth and thereby satisfy your customers. So the *target functional* is the\n99% quantile. From the table above, you choose the pinball loss as scoring function\n(fair enough, not much choice given), for model training (e.g.\n`HistGradientBoostingRegressor(loss=\"quantile\", quantile=0.99)`) as well as model\nevaluation (`mean_pinball_loss(..., alpha=0.99)` - we apologize for the different\nargument names, `quantile` and `alpha`) be it in grid search for finding\nhyperparameters or in comparing to other models like\n`QuantileRegressor(quantile=0.99)`.\n\n.. rubric:: References\n\n.. [Gneiting2007] T. Gneiting and A. E. Raftery. :doi:`Strictly Proper\n    Scoring Rules, Prediction, and Estimation <10.1198/016214506000001437>`\n    In: Journal of the American Statistical Association 102 (2007),\n    pp. 359\u2013 378.\n    `link to pdf <https://sites.stat.washington.edu/raftery/Research/PDF/Gneiting2007jasa.pdf>`_\n\n.. [Gneiting2009] T. Gneiting. :arxiv:`Making and Evaluating Point Forecasts\n    <0912.0902>`\n    Journal of the American Statistical Association 106 (2009): 746 - 762.\n\n.. [Gneiting2014] T. Gneiting and M. Katzfuss. :doi:`Probabilistic Forecasting\n    <10.1146/annurev-statistics-062713-085831>`. In: Annual Review of Statistics and Its Application 1.1 (2014), pp. 125\u2013151.\n\n.. [Fissler2022] T. Fissler, C. Lorentzen and M. Mayer. :arxiv:`Model\n    Comparison and Calibration Assessment: User Guide for Consistent Scoring\n    Functions in Machine Learning and Actuarial Practice. <2202.12780>`\n\n.. _scoring_api_overview:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_3",
    "header": "Scoring API overview",
    "text": "Scoring API overview\n====================\n\nThere are 3 different APIs for evaluating the quality of a model's\npredictions:\n\n* **Estimator score method**: Estimators have a ``score`` method providing a\n  default evaluation criterion for the problem they are designed to solve.\n  Most commonly this is :ref:`accuracy <accuracy_score>` for classifiers and the\n  :ref:`coefficient of determination <r2_score>` (:math:`R^2`) for regressors.\n  Details for each estimator can be found in its documentation.\n\n* **Scoring parameter**: Model-evaluation tools that use\n  :ref:`cross-validation <cross_validation>` (such as\n  :class:`model_selection.GridSearchCV`, :func:`model_selection.validation_curve` and\n  :class:`linear_model.LogisticRegressionCV`) rely on an internal *scoring* strategy.\n  This can be specified using the `scoring` parameter of that tool and is discussed\n  in the section :ref:`scoring_parameter`.\n\n* **Metric functions**: The :mod:`sklearn.metrics` module implements functions\n  assessing prediction error for specific purposes. These metrics are detailed\n  in sections on :ref:`classification_metrics`,\n  :ref:`multilabel_ranking_metrics`, :ref:`regression_metrics` and\n  :ref:`clustering_metrics`.\n\nFinally, :ref:`dummy_estimators` are useful to get a baseline\nvalue of those metrics for random predictions.\n\n.. seealso::\n\n   For \"pairwise\" metrics, between *samples* and not estimators or\n   predictions, see the :ref:`metrics` section.\n\n.. _scoring_parameter:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_4",
    "header": "The ``scoring`` parameter: defining model evaluation rules",
    "text": "The ``scoring`` parameter: defining model evaluation rules\n==========================================================\n\nModel selection and evaluation tools that internally use\n:ref:`cross-validation <cross_validation>` (such as\n:class:`model_selection.GridSearchCV`, :func:`model_selection.validation_curve` and\n:class:`linear_model.LogisticRegressionCV`) take a ``scoring`` parameter that\ncontrols what metric they apply to the estimators evaluated.\n\nThey can be specified in several ways:\n\n* `None`: the estimator's default evaluation criterion (i.e., the metric used in the\n  estimator's `score` method) is used.\n* :ref:`String name <scoring_string_names>`: common metrics can be passed via a string\n  name.\n* :ref:`Callable <scoring_callable>`: more complex metrics can be passed via a custom\n  metric callable (e.g., function).\n\nSome tools do also accept multiple metric evaluation. See :ref:`multimetric_scoring`\nfor details.\n\n.. _scoring_string_names:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_5",
    "header": "String name scorers",
    "text": "String name scorers\n-------------------\n\nFor the most common use cases, you can designate a scorer object with the\n``scoring`` parameter via a string name; the table below shows all possible values.\nAll scorer objects follow the convention that **higher return values are better\nthan lower return values**. Thus metrics which measure the distance between\nthe model and the data, like :func:`metrics.mean_squared_error`, are\navailable as 'neg_mean_squared_error' which return the negated value\nof the metric.\n\n====================================   ==============================================     ==================================\nScoring string name                    Function                                           Comment\n====================================   ==============================================     ==================================\n**Classification**\n'accuracy'                             :func:`metrics.accuracy_score`\n'balanced_accuracy'                    :func:`metrics.balanced_accuracy_score`\n'top_k_accuracy'                       :func:`metrics.top_k_accuracy_score`\n'average_precision'                    :func:`metrics.average_precision_score`\n'neg_brier_score'                      :func:`metrics.brier_score_loss`\n'f1'                                   :func:`metrics.f1_score`                           for binary targets\n'f1_micro'                             :func:`metrics.f1_score`                           micro-averaged\n'f1_macro'                             :func:`metrics.f1_score`                           macro-averaged\n'f1_weighted'                          :func:`metrics.f1_score`                           weighted average\n'f1_samples'                           :func:`metrics.f1_score`                           by multilabel sample\n'neg_log_loss'                         :func:`metrics.log_loss`                           requires ``predict_proba`` support\n'precision' etc.                       :func:`metrics.precision_score`                    suffixes apply as with 'f1'\n'recall' etc.                          :func:`metrics.recall_score`                       suffixes apply as with 'f1'\n'jaccard' etc.                         :func:`metrics.jaccard_score`                      suffixes apply as with 'f1'\n'roc_auc'                              :func:`metrics.roc_auc_score`\n'roc_auc_ovr'                          :func:`metrics.roc_auc_score`\n'roc_auc_ovo'                          :func:`metrics.roc_auc_score`\n'roc_auc_ovr_weighted'                 :func:`metrics.roc_auc_score`\n'roc_auc_ovo_weighted'                 :func:`metrics.roc_auc_score`\n'd2_log_loss_score'                    :func:`metrics.d2_log_loss_score`\n\n**Clustering**\n'adjusted_mutual_info_score'           :func:`metrics.adjusted_mutual_info_score`\n'adjusted_rand_score'                  :func:`metrics.adjusted_rand_score`\n'completeness_score'                   :func:`metrics.completeness_score`\n'fowlkes_mallows_score'                :func:`metrics.fowlkes_mallows_score`\n'homogeneity_score'                    :func:`metrics.homogeneity_score`\n'mutual_info_score'                    :func:`metrics.mutual_info_score`\n'normalized_mutual_info_score'         :func:`metrics.normalized_mutual_info_score`\n'rand_score'                           :func:`metrics.rand_score`\n'v_measure_score'                      :func:`metrics.v_measure_score`\n\n**Regression**\n'explained_variance'                   :func:`metrics.explained_variance_score`\n'neg_max_error'                        :func:`metrics.max_error`\n'neg_mean_absolute_error'              :func:`metrics.mean_absolute_error`\n'neg_mean_squared_error'               :func:`metrics.mean_squared_error`\n'neg_root_mean_squared_error'          :func:`metrics.root_mean_squared_error`\n'neg_mean_squared_log_error'           :func:`metrics.mean_squared_log_error`\n'neg_root_mean_squared_log_error'      :func:`metrics.root_mean_squared_log_error`\n'neg_median_absolute_error'            :func:`metrics.median_absolute_error`\n'r2'                                   :func:`metrics.r2_score`\n'neg_mean_poisson_deviance'            :func:`metrics.mean_poisson_deviance`\n'neg_mean_gamma_deviance'              :func:`metrics.mean_gamma_deviance`\n'neg_mean_absolute_percentage_error'   :func:`metrics.mean_absolute_percentage_error`\n'd2_absolute_error_score'              :func:`metrics.d2_absolute_error_score`\n====================================   ==============================================     ==================================\n\nUsage examples:\n\n    >>> from sklearn import svm, datasets\n    >>> from sklearn.model_selection import cross_val_score\n    >>> X, y = datasets.load_iris(return_X_y=True)\n    >>> clf = svm.SVC(random_state=0)\n    >>> cross_val_score(clf, X, y, cv=5, scoring='recall_macro')\n    array([0.96, 0.96, 0.96, 0.93, 1.        ])\n\n.. note::\n\n    If a wrong scoring name is passed, an ``InvalidParameterError`` is raised.\n    You can retrieve the names of all available scorers by calling\n    :func:`~sklearn.metrics.get_scorer_names`.\n\n.. currentmodule:: sklearn.metrics\n\n.. _scoring_callable:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_6",
    "header": "Callable scorers",
    "text": "Callable scorers\n----------------\n\nFor more complex use cases and more flexibility, you can pass a callable to\nthe `scoring` parameter. This can be done by:\n\n* :ref:`scoring_adapt_metric`\n* :ref:`scoring_custom` (most flexible)\n\n.. _scoring_adapt_metric:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_7",
    "header": "Adapting predefined metrics via `make_scorer`",
    "text": "Adapting predefined metrics via `make_scorer`\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe following metric functions are not implemented as named scorers,\nsometimes because they require additional parameters, such as\n:func:`fbeta_score`. They cannot be passed to the ``scoring``\nparameters; instead their callable needs to be passed to\n:func:`make_scorer` together with the value of the user-settable\nparameters.\n\n=====================================  =========  ==============================================\nFunction                               Parameter  Example usage\n=====================================  =========  ==============================================\n**Classification**\n:func:`metrics.fbeta_score`            ``beta``   ``make_scorer(fbeta_score, beta=2)``\n\n**Regression**\n:func:`metrics.mean_tweedie_deviance`  ``power``  ``make_scorer(mean_tweedie_deviance, power=1.5)``\n:func:`metrics.mean_pinball_loss`      ``alpha``  ``make_scorer(mean_pinball_loss, alpha=0.95)``\n:func:`metrics.d2_tweedie_score`       ``power``  ``make_scorer(d2_tweedie_score, power=1.5)``\n:func:`metrics.d2_pinball_score`       ``alpha``  ``make_scorer(d2_pinball_score, alpha=0.95)``\n=====================================  =========  ==============================================\n\nOne typical use case is to wrap an existing metric function from the library\nwith non-default values for its parameters, such as the ``beta`` parameter for\nthe :func:`fbeta_score` function::\n\n    >>> from sklearn.metrics import fbeta_score, make_scorer\n    >>> ftwo_scorer = make_scorer(fbeta_score, beta=2)\n    >>> from sklearn.model_selection import GridSearchCV\n    >>> from sklearn.svm import LinearSVC\n    >>> grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]},\n    ...                     scoring=ftwo_scorer, cv=5)\n\nThe module :mod:`sklearn.metrics` also exposes a set of simple functions\nmeasuring a prediction error given ground truth and prediction:\n\n- functions ending with ``_score`` return a value to\n  maximize, the higher the better.\n\n- functions ending with ``_error``, ``_loss``, or ``_deviance`` return a\n  value to minimize, the lower the better. When converting\n  into a scorer object using :func:`make_scorer`, set\n  the ``greater_is_better`` parameter to ``False`` (``True`` by default; see the\n  parameter description below).\n\n.. _scoring_custom:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_8",
    "header": "Creating a custom scorer object",
    "text": "Creating a custom scorer object\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nYou can create your own custom scorer object using\n:func:`make_scorer` or for the most flexibility, from scratch. See below for details.\n\n.. dropdown:: Custom scorer objects using `make_scorer`\n\n  You can build a completely custom scorer object\n  from a simple python function using :func:`make_scorer`, which can\n  take several parameters:\n\n  * the python function you want to use (``my_custom_loss_func``\n    in the example below)\n\n  * whether the python function returns a score (``greater_is_better=True``,\n    the default) or a loss (``greater_is_better=False``). If a loss, the output\n    of the python function is negated by the scorer object, conforming to\n    the cross validation convention that scorers return higher values for better models.\n\n  * for classification metrics only: whether the python function you provided requires\n    continuous decision certainties. If the scoring function only accepts probability\n    estimates (e.g. :func:`metrics.log_loss`), then one needs to set the parameter\n    `response_method=\"predict_proba\"`. Some scoring\n    functions do not necessarily require probability estimates but rather non-thresholded\n    decision values (e.g. :func:`metrics.roc_auc_score`). In this case, one can provide a\n    list (e.g., `response_method=[\"decision_function\", \"predict_proba\"]`),\n    and scorer will use the first available method, in the order given in the list,\n    to compute the scores.\n\n  * any additional parameters of the scoring function, such as ``beta`` or ``labels``.\n\n  Here is an example of building custom scorers, and of using the\n  ``greater_is_better`` parameter::\n\n      >>> import numpy as np\n      >>> def my_custom_loss_func(y_true, y_pred):\n      ...     diff = np.abs(y_true - y_pred).max()\n      ...     return float(np.log1p(diff))\n      ...\n      >>> # score will negate the return value of my_custom_loss_func,\n      >>> # which will be np.log(2), 0.693, given the values for X\n      >>> # and y defined below.\n      >>> score = make_scorer(my_custom_loss_func, greater_is_better=False)\n      >>> X = [[1], [1]]\n      >>> y = [0, 1]\n      >>> from sklearn.dummy import DummyClassifier\n      >>> clf = DummyClassifier(strategy='most_frequent', random_state=0)\n      >>> clf = clf.fit(X, y)\n      >>> my_custom_loss_func(y, clf.predict(X))\n      0.69\n      >>> score(clf, X, y)\n      -0.69\n\n.. dropdown:: Custom scorer objects from scratch\n\n  You can generate even more flexible model scorers by constructing your own\n  scoring object from scratch, without using the :func:`make_scorer` factory.\n\n  For a callable to be a scorer, it needs to meet the protocol specified by\n  the following two rules:\n\n  - It can be called with parameters ``(estimator, X, y)``, where ``estimator``\n    is the model that should be evaluated, ``X`` is validation data, and ``y`` is\n    the ground truth target for ``X`` (in the supervised case) or ``None`` (in the\n    unsupervised case).\n\n  - It returns a floating point number that quantifies the\n    ``estimator`` prediction quality on ``X``, with reference to ``y``.\n    Again, by convention higher numbers are better, so if your scorer\n    returns loss, that value should be negated.\n\n  - Advanced: If it requires extra metadata to be passed to it, it should expose\n    a ``get_metadata_routing`` method returning the requested metadata. The user\n    should be able to set the requested metadata via a ``set_score_request``\n    method. Please see :ref:`User Guide <metadata_routing>` and :ref:`Developer\n    Guide <sphx_glr_auto_examples_miscellaneous_plot_metadata_routing.py>` for\n    more details.\n\n\n.. dropdown:: Using custom scorers in functions where n_jobs > 1\n\n    While defining the custom scoring function alongside the calling function\n    should work out of the box with the default joblib backend (loky),\n    importing it from another module will be a more robust approach and work\n    independently of the joblib backend.\n\n    For example, to use ``n_jobs`` greater than 1 in the example below,\n    ``custom_scoring_function`` function is saved in a user-created module\n    (``custom_scorer_module.py``) and imported::\n\n        >>> from custom_scorer_module import custom_scoring_function # doctest: +SKIP\n        >>> cross_val_score(model,\n        ...  X_train,\n        ...  y_train,\n        ...  scoring=make_scorer(custom_scoring_function, greater_is_better=False),\n        ...  cv=5,\n        ...  n_jobs=-1) # doctest: +SKIP\n\n.. _multimetric_scoring:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_9",
    "header": "Using multiple metric evaluation",
    "text": "Using multiple metric evaluation\n--------------------------------\n\nScikit-learn also permits evaluation of multiple metrics in ``GridSearchCV``,\n``RandomizedSearchCV`` and ``cross_validate``.\n\nThere are three ways to specify multiple scoring metrics for the ``scoring``\nparameter:\n\n- As an iterable of string metrics::\n\n    >>> scoring = ['accuracy', 'precision']\n\n- As a ``dict`` mapping the scorer name to the scoring function::\n\n    >>> from sklearn.metrics import accuracy_score\n    >>> from sklearn.metrics import make_scorer\n    >>> scoring = {'accuracy': make_scorer(accuracy_score),\n    ...            'prec': 'precision'}\n\n  Note that the dict values can either be scorer functions or one of the\n  predefined metric strings.\n\n- As a callable that returns a dictionary of scores::\n\n    >>> from sklearn.model_selection import cross_validate\n    >>> from sklearn.metrics import confusion_matrix\n    >>> # A sample toy binary classification dataset\n    >>> X, y = datasets.make_classification(n_classes=2, random_state=0)\n    >>> svm = LinearSVC(random_state=0)\n    >>> def confusion_matrix_scorer(clf, X, y):\n    ...      y_pred = clf.predict(X)\n    ...      cm = confusion_matrix(y, y_pred)\n    ...      return {'tn': cm[0, 0], 'fp': cm[0, 1],\n    ...              'fn': cm[1, 0], 'tp': cm[1, 1]}\n    >>> cv_results = cross_validate(svm, X, y, cv=5,\n    ...                             scoring=confusion_matrix_scorer)\n    >>> # Getting the test set true positive scores\n    >>> print(cv_results['test_tp'])\n    [10  9  8  7  8]\n    >>> # Getting the test set false negative scores\n    >>> print(cv_results['test_fn'])\n    [0 1 2 3 2]\n\n.. _classification_metrics:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_10",
    "header": "Classification metrics",
    "text": "Classification metrics\n=======================\n\n.. currentmodule:: sklearn.metrics\n\nThe :mod:`sklearn.metrics` module implements several loss, score, and utility\nfunctions to measure classification performance.\nSome metrics might require probability estimates of the positive class,\nconfidence values, or binary decisions values.\nMost implementations allow each sample to provide a weighted contribution\nto the overall score, through the ``sample_weight`` parameter.\n\nSome of these are restricted to the binary classification case:\n\n.. autosummary::\n\n   precision_recall_curve\n   roc_curve\n   class_likelihood_ratios\n   det_curve\n\n\nOthers also work in the multiclass case:\n\n.. autosummary::\n\n   balanced_accuracy_score\n   cohen_kappa_score\n   confusion_matrix\n   hinge_loss\n   matthews_corrcoef\n   roc_auc_score\n   top_k_accuracy_score\n\n\nSome also work in the multilabel case:\n\n.. autosummary::\n\n   accuracy_score\n   classification_report\n   f1_score\n   fbeta_score\n   hamming_loss\n   jaccard_score\n   log_loss\n   multilabel_confusion_matrix\n   precision_recall_fscore_support\n   precision_score\n   recall_score\n   roc_auc_score\n   zero_one_loss\n   d2_log_loss_score\n\nAnd some work with binary and multilabel (but not multiclass) problems:\n\n.. autosummary::\n\n   average_precision_score\n\n\nIn the following sub-sections, we will describe each of those functions,\npreceded by some notes on common API and metric definition.\n\n.. _average:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_11",
    "header": "From binary to multiclass and multilabel",
    "text": "From binary to multiclass and multilabel\n----------------------------------------\n\nSome metrics are essentially defined for binary classification tasks (e.g.\n:func:`f1_score`, :func:`roc_auc_score`). In these cases, by default\nonly the positive label is evaluated, assuming by default that the positive\nclass is labelled ``1`` (though this may be configurable through the\n``pos_label`` parameter).\n\nIn extending a binary metric to multiclass or multilabel problems, the data\nis treated as a collection of binary problems, one for each class.\nThere are then a number of ways to average binary metric calculations across\nthe set of classes, each of which may be useful in some scenario.\nWhere available, you should select among these using the ``average`` parameter.\n\n* ``\"macro\"`` simply calculates the mean of the binary metrics,\n  giving equal weight to each class.  In problems where infrequent classes\n  are nonetheless important, macro-averaging may be a means of highlighting\n  their performance. On the other hand, the assumption that all classes are\n  equally important is often untrue, such that macro-averaging will\n  over-emphasize the typically low performance on an infrequent class.\n* ``\"weighted\"`` accounts for class imbalance by computing the average of\n  binary metrics in which each class's score is weighted by its presence in the\n  true data sample.\n* ``\"micro\"`` gives each sample-class pair an equal contribution to the overall\n  metric (except as a result of sample-weight). Rather than summing the\n  metric per class, this sums the dividends and divisors that make up the\n  per-class metrics to calculate an overall quotient.\n  Micro-averaging may be preferred in multilabel settings, including\n  multiclass classification where a majority class is to be ignored.\n* ``\"samples\"`` applies only to multilabel problems. It does not calculate a\n  per-class measure, instead calculating the metric over the true and predicted\n  classes for each sample in the evaluation data, and returning their\n  (``sample_weight``-weighted) average.\n* Selecting ``average=None`` will return an array with the score for each\n  class.\n\nWhile multiclass data is provided to the metric, like binary targets, as an\narray of class labels, multilabel data is specified as an indicator matrix,\nin which cell ``[i, j]`` has value 1 if sample ``i`` has label ``j`` and value\n0 otherwise.\n\n.. _accuracy_score:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_12",
    "header": "Accuracy score",
    "text": "Accuracy score\n--------------\n\nThe :func:`accuracy_score` function computes the\n`accuracy <https://en.wikipedia.org/wiki/Accuracy_and_precision>`_, either the fraction\n(default) or the count (normalize=False) of correct predictions.\n\n\nIn multilabel classification, the function returns the subset accuracy. If\nthe entire set of predicted labels for a sample strictly match with the true\nset of labels, then the subset accuracy is 1.0; otherwise it is 0.0.\n\nIf :math:`\\hat{y}_i` is the predicted value of\nthe :math:`i`-th sample and :math:`y_i` is the corresponding true value,\nthen the fraction of correct predictions over :math:`n_\\text{samples}` is\ndefined as\n\n.. math::\n\n  \\texttt{accuracy}(y, \\hat{y}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples}-1} 1(\\hat{y}_i = y_i)\n\nwhere :math:`1(x)` is the `indicator function\n<https://en.wikipedia.org/wiki/Indicator_function>`_.\n\n  >>> import numpy as np\n  >>> from sklearn.metrics import accuracy_score\n  >>> y_pred = [0, 2, 1, 3]\n  >>> y_true = [0, 1, 2, 3]\n  >>> accuracy_score(y_true, y_pred)\n  0.5\n  >>> accuracy_score(y_true, y_pred, normalize=False)\n  2.0\n\nIn the multilabel case with binary label indicators::\n\n  >>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n  0.5\n\n.. rubric:: Examples\n\n* See :ref:`sphx_glr_auto_examples_model_selection_plot_permutation_tests_for_classification.py`\n  for an example of accuracy score usage using permutations of\n  the dataset.\n\n.. _top_k_accuracy_score:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_13",
    "header": "Top-k accuracy score",
    "text": "Top-k accuracy score\n--------------------\n\nThe :func:`top_k_accuracy_score` function is a generalization of\n:func:`accuracy_score`. The difference is that a prediction is considered\ncorrect as long as the true label is associated with one of the ``k`` highest\npredicted scores. :func:`accuracy_score` is the special case of `k = 1`.\n\nThe function covers the binary and multiclass classification cases but not the\nmultilabel case.\n\nIf :math:`\\hat{f}_{i,j}` is the predicted class for the :math:`i`-th sample\ncorresponding to the :math:`j`-th largest predicted score and :math:`y_i` is the\ncorresponding true value, then the fraction of correct predictions over\n:math:`n_\\text{samples}` is defined as\n\n.. math::\n\n   \\texttt{top-k accuracy}(y, \\hat{f}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples}-1} \\sum_{j=1}^{k} 1(\\hat{f}_{i,j} = y_i)\n\nwhere :math:`k` is the number of guesses allowed and :math:`1(x)` is the\n`indicator function <https://en.wikipedia.org/wiki/Indicator_function>`_.\n\n  >>> import numpy as np\n  >>> from sklearn.metrics import top_k_accuracy_score\n  >>> y_true = np.array([0, 1, 2, 2])\n  >>> y_score = np.array([[0.5, 0.2, 0.2],\n  ...                     [0.3, 0.4, 0.2],\n  ...                     [0.2, 0.4, 0.3],\n  ...                     [0.7, 0.2, 0.1]])\n  >>> top_k_accuracy_score(y_true, y_score, k=2)\n  0.75\n  >>> # Not normalizing gives the number of \"correctly\" classified samples\n  >>> top_k_accuracy_score(y_true, y_score, k=2, normalize=False)\n  3.0\n\n.. _balanced_accuracy_score:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_14",
    "header": "Balanced accuracy score",
    "text": "Balanced accuracy score\n-----------------------\n\nThe :func:`balanced_accuracy_score` function computes the `balanced accuracy\n<https://en.wikipedia.org/wiki/Accuracy_and_precision>`_, which avoids inflated\nperformance estimates on imbalanced datasets. It is the macro-average of recall\nscores per class or, equivalently, raw accuracy where each sample is weighted\naccording to the inverse prevalence of its true class.\nThus for balanced datasets, the score is equal to accuracy.\n\nIn the binary case, balanced accuracy is equal to the arithmetic mean of\n`sensitivity <https://en.wikipedia.org/wiki/Sensitivity_and_specificity>`_\n(true positive rate) and `specificity\n<https://en.wikipedia.org/wiki/Sensitivity_and_specificity>`_ (true negative\nrate), or the area under the ROC curve with binary predictions rather than\nscores:\n\n.. math::\n\n   \\texttt{balanced-accuracy} = \\frac{1}{2}\\left( \\frac{TP}{TP + FN} + \\frac{TN}{TN + FP}\\right )\n\nIf the classifier performs equally well on either class, this term reduces to\nthe conventional accuracy (i.e., the number of correct predictions divided by\nthe total number of predictions).\n\nIn contrast, if the conventional accuracy is above chance only because the\nclassifier takes advantage of an imbalanced test set, then the balanced\naccuracy, as appropriate, will drop to :math:`\\frac{1}{n\\_classes}`.\n\nThe score ranges from 0 to 1, or when ``adjusted=True`` is used, it is rescaled to\nthe range :math:`\\frac{1}{1 - n\\_classes}` to 1, inclusive, with\nperformance at random scoring 0.\n\nIf :math:`y_i` is the true value of the :math:`i`-th sample, and :math:`w_i`\nis the corresponding sample weight, then we adjust the sample weight to:\n\n.. math::\n\n   \\hat{w}_i = \\frac{w_i}{\\sum_j{1(y_j = y_i) w_j}}\n\nwhere :math:`1(x)` is the `indicator function <https://en.wikipedia.org/wiki/Indicator_function>`_.\nGiven predicted :math:`\\hat{y}_i` for sample :math:`i`, balanced accuracy is\ndefined as:\n\n.. math::\n\n   \\texttt{balanced-accuracy}(y, \\hat{y}, w) = \\frac{1}{\\sum{\\hat{w}_i}} \\sum_i 1(\\hat{y}_i = y_i) \\hat{w}_i\n\nWith ``adjusted=True``, balanced accuracy reports the relative increase from\n:math:`\\texttt{balanced-accuracy}(y, \\mathbf{0}, w) =\n\\frac{1}{n\\_classes}`.  In the binary case, this is also known as\n`*Youden's J statistic* <https://en.wikipedia.org/wiki/Youden%27s_J_statistic>`_,\nor *informedness*.\n\n.. note::\n\n    The multiclass definition here seems the most reasonable extension of the\n    metric used in binary classification, though there is no certain consensus\n    in the literature:\n\n    * Our definition: [Mosley2013]_, [Kelleher2015]_ and [Guyon2015]_, where\n      [Guyon2015]_ adopt the adjusted version to ensure that random predictions\n      have a score of :math:`0` and perfect predictions have a score of :math:`1`..\n    * Class balanced accuracy as described in [Mosley2013]_: the minimum between the precision\n      and the recall for each class is computed. Those values are then averaged over the total\n      number of classes to get the balanced accuracy.\n    * Balanced Accuracy as described in [Urbanowicz2015]_: the average of sensitivity and specificity\n      is computed for each class and then averaged over total number of classes.\n\n.. rubric:: References\n\n.. [Guyon2015] I. Guyon, K. Bennett, G. Cawley, H.J. Escalante, S. Escalera, T.K. Ho, N. Maci\u00e0,\n    B. Ray, M. Saeed, A.R. Statnikov, E. Viegas, `Design of the 2015 ChaLearn AutoML Challenge\n    <https://ieeexplore.ieee.org/document/7280767>`_, IJCNN 2015.\n.. [Mosley2013] L. Mosley, `A balanced approach to the multi-class imbalance problem\n    <https://lib.dr.iastate.edu/etd/13537/>`_, IJCV 2010.\n.. [Kelleher2015] John. D. Kelleher, Brian Mac Namee, Aoife D'Arcy, `Fundamentals of\n    Machine Learning for Predictive Data Analytics: Algorithms, Worked Examples,\n    and Case Studies <https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics>`_,\n    2015.\n.. [Urbanowicz2015] Urbanowicz R.J.,  Moore, J.H. :doi:`ExSTraCS 2.0: description\n    and evaluation of a scalable learning classifier\n    system <10.1007/s12065-015-0128-8>`, Evol. Intel. (2015) 8: 89.\n\n.. _cohen_kappa:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_15",
    "header": "Cohen's kappa",
    "text": "Cohen's kappa\n-------------\n\nThe function :func:`cohen_kappa_score` computes `Cohen's kappa\n<https://en.wikipedia.org/wiki/Cohen%27s_kappa>`_ statistic.\nThis measure is intended to compare labelings by different human annotators,\nnot a classifier versus a ground truth.\n\nThe kappa score is a number between -1 and 1.\nScores above .8 are generally considered good agreement;\nzero or lower means no agreement (practically random labels).\n\nKappa scores can be computed for binary or multiclass problems,\nbut not for multilabel problems (except by manually computing a per-label score)\nand not for more than two annotators.\n\n  >>> from sklearn.metrics import cohen_kappa_score\n  >>> labeling1 = [2, 0, 2, 2, 0, 1]\n  >>> labeling2 = [0, 0, 2, 2, 0, 2]\n  >>> cohen_kappa_score(labeling1, labeling2)\n  0.4285714285714286\n\n.. _confusion_matrix:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_16",
    "header": "Confusion matrix",
    "text": "Confusion matrix\n----------------\n\nThe :func:`confusion_matrix` function evaluates\nclassification accuracy by computing the `confusion matrix\n<https://en.wikipedia.org/wiki/Confusion_matrix>`_ with each row corresponding\nto the true class (Wikipedia and other references may use different convention\nfor axes).\n\nBy definition, entry :math:`i, j` in a confusion matrix is\nthe number of observations actually in group :math:`i`, but\npredicted to be in group :math:`j`. Here is an example::\n\n  >>> from sklearn.metrics import confusion_matrix\n  >>> y_true = [2, 0, 2, 2, 0, 1]\n  >>> y_pred = [0, 0, 2, 2, 0, 2]\n  >>> confusion_matrix(y_true, y_pred)\n  array([[2, 0, 0],\n         [0, 0, 1],\n         [1, 0, 2]])\n\n:class:`ConfusionMatrixDisplay` can be used to visually represent a confusion\nmatrix as shown in the\n:ref:`sphx_glr_auto_examples_model_selection_plot_confusion_matrix.py`\nexample, which creates the following figure:\n\n.. image:: ../auto_examples/model_selection/images/sphx_glr_plot_confusion_matrix_001.png\n   :target: ../auto_examples/model_selection/plot_confusion_matrix.html\n   :scale: 75\n   :align: center\n\nThe parameter ``normalize`` allows to report ratios instead of counts. The\nconfusion matrix can be normalized in 3 different ways: ``'pred'``, ``'true'``,\nand ``'all'`` which will divide the counts by the sum of each columns, rows, or\nthe entire matrix, respectively.\n\n  >>> y_true = [0, 0, 0, 1, 1, 1, 1, 1]\n  >>> y_pred = [0, 1, 0, 1, 0, 1, 0, 1]\n  >>> confusion_matrix(y_true, y_pred, normalize='all')\n  array([[0.25 , 0.125],\n         [0.25 , 0.375]])\n\nFor binary problems, we can get counts of true negatives, false positives,\nfalse negatives and true positives as follows::\n\n  >>> y_true = [0, 0, 0, 1, 1, 1, 1, 1]\n  >>> y_pred = [0, 1, 0, 1, 0, 1, 0, 1]\n  >>> tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel().tolist()\n  >>> tn, fp, fn, tp\n  (2, 1, 2, 3)\n\n.. rubric:: Examples\n\n* See :ref:`sphx_glr_auto_examples_model_selection_plot_confusion_matrix.py`\n  for an example of using a confusion matrix to evaluate classifier output\n  quality.\n\n* See :ref:`sphx_glr_auto_examples_classification_plot_digits_classification.py`\n  for an example of using a confusion matrix to classify\n  hand-written digits.\n\n* See :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n  for an example of using a confusion matrix to classify text\n  documents.\n\n.. _classification_report:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_17",
    "header": "Classification report",
    "text": "Classification report\n----------------------\n\nThe :func:`classification_report` function builds a text report showing the\nmain classification metrics. Here is a small example with custom ``target_names``\nand inferred labels::\n\n   >>> from sklearn.metrics import classification_report\n   >>> y_true = [0, 1, 2, 2, 0]\n   >>> y_pred = [0, 0, 2, 1, 0]\n   >>> target_names = ['class 0', 'class 1', 'class 2']\n   >>> print(classification_report(y_true, y_pred, target_names=target_names))\n                 precision    recall  f1-score   support\n   <BLANKLINE>\n        class 0       0.67      1.00      0.80         2\n        class 1       0.00      0.00      0.00         1\n        class 2       1.00      0.50      0.67         2\n   <BLANKLINE>\n       accuracy                           0.60         5\n      macro avg       0.56      0.50      0.49         5\n   weighted avg       0.67      0.60      0.59         5\n   <BLANKLINE>\n\n.. rubric:: Examples\n\n* See :ref:`sphx_glr_auto_examples_classification_plot_digits_classification.py`\n  for an example of classification report usage for\n  hand-written digits.\n\n* See :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_digits.py`\n  for an example of classification report usage for\n  grid search with nested cross-validation.\n\n.. _hamming_loss:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_18",
    "header": "Hamming loss",
    "text": "Hamming loss\n-------------\n\nThe :func:`hamming_loss` computes the average Hamming loss or `Hamming\ndistance <https://en.wikipedia.org/wiki/Hamming_distance>`_ between two sets\nof samples.\n\nIf :math:`\\hat{y}_{i,j}` is the predicted value for the :math:`j`-th label of a\ngiven sample :math:`i`, :math:`y_{i,j}` is the corresponding true value,\n:math:`n_\\text{samples}` is the number of samples and :math:`n_\\text{labels}`\nis the number of labels, then the Hamming loss :math:`L_{Hamming}` is defined\nas:\n\n.. math::\n\n   L_{Hamming}(y, \\hat{y}) = \\frac{1}{n_\\text{samples} * n_\\text{labels}} \\sum_{i=0}^{n_\\text{samples}-1} \\sum_{j=0}^{n_\\text{labels} - 1} 1(\\hat{y}_{i,j} \\not= y_{i,j})\n\nwhere :math:`1(x)` is the `indicator function\n<https://en.wikipedia.org/wiki/Indicator_function>`_.\n\nThe equation above does not hold true in the case of multiclass classification.\nPlease refer to the note below for more information. ::\n\n  >>> from sklearn.metrics import hamming_loss\n  >>> y_pred = [1, 2, 3, 4]\n  >>> y_true = [2, 2, 3, 4]\n  >>> hamming_loss(y_true, y_pred)\n  0.25\n\nIn the multilabel case with binary label indicators::\n\n  >>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))\n  0.75\n\n.. note::\n\n    In multiclass classification, the Hamming loss corresponds to the Hamming\n    distance between ``y_true`` and ``y_pred`` which is similar to the\n    :ref:`zero_one_loss` function.  However, while zero-one loss penalizes\n    prediction sets that do not strictly match true sets, the Hamming loss\n    penalizes individual labels.  Thus the Hamming loss, upper bounded by the zero-one\n    loss, is always between zero and one, inclusive; and predicting a proper subset\n    or superset of the true labels will give a Hamming loss between\n    zero and one, exclusive.\n\n.. _precision_recall_f_measure_metrics:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_19",
    "header": "Precision, recall and F-measures",
    "text": "Precision, recall and F-measures\n---------------------------------\n\nIntuitively, `precision\n<https://en.wikipedia.org/wiki/Precision_and_recall#Precision>`_ is the ability\nof the classifier not to label as positive a sample that is negative, and\n`recall <https://en.wikipedia.org/wiki/Precision_and_recall#Recall>`_ is the\nability of the classifier to find all the positive samples.\n\nThe  `F-measure <https://en.wikipedia.org/wiki/F1_score>`_\n(:math:`F_\\beta` and :math:`F_1` measures) can be interpreted as a weighted\nharmonic mean of the precision and recall. A\n:math:`F_\\beta` measure reaches its best value at 1 and its worst score at 0.\nWith :math:`\\beta = 1`,  :math:`F_\\beta` and\n:math:`F_1`  are equivalent, and the recall and the precision are equally important.\n\nThe :func:`precision_recall_curve` computes a precision-recall curve\nfrom the ground truth label and a score given by the classifier\nby varying a decision threshold.\n\nThe :func:`average_precision_score` function computes the\n`average precision <https://en.wikipedia.org/w/index.php?title=Information_retrieval&oldid=793358396#Average_precision>`_\n(AP) from prediction scores. The value is between 0 and 1 and higher is better.\nAP is defined as\n\n.. math::\n    \\text{AP} = \\sum_n (R_n - R_{n-1}) P_n\n\nwhere :math:`P_n` and :math:`R_n` are the precision and recall at the\nnth threshold. With random predictions, the AP is the fraction of positive\nsamples.\n\nReferences [Manning2008]_ and [Everingham2010]_ present alternative variants of\nAP that interpolate the precision-recall curve. Currently,\n:func:`average_precision_score` does not implement any interpolated variant.\nReferences [Davis2006]_ and [Flach2015]_ describe why a linear interpolation of\npoints on the precision-recall curve provides an overly-optimistic measure of\nclassifier performance. This linear interpolation is used when computing area\nunder the curve with the trapezoidal rule in :func:`auc`.\n\nSeveral functions allow you to analyze the precision, recall and F-measures\nscore:\n\n.. autosummary::\n\n   average_precision_score\n   f1_score\n   fbeta_score\n   precision_recall_curve\n   precision_recall_fscore_support\n   precision_score\n   recall_score\n\nNote that the :func:`precision_recall_curve` function is restricted to the\nbinary case. The :func:`average_precision_score` function supports multiclass\nand multilabel formats by computing each class score in a One-vs-the-rest (OvR)\nfashion and averaging them or not depending of its ``average`` argument value.\n\nThe :func:`PrecisionRecallDisplay.from_estimator` and\n:func:`PrecisionRecallDisplay.from_predictions` functions will plot the\nprecision-recall curve as follows.\n\n.. image:: ../auto_examples/model_selection/images/sphx_glr_plot_precision_recall_001.png\n        :target: ../auto_examples/model_selection/plot_precision_recall.html#plot-the-precision-recall-curve\n        :scale: 75\n        :align: center\n\n.. rubric:: Examples\n\n* See :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_digits.py`\n  for an example of :func:`precision_score` and :func:`recall_score` usage\n  to estimate parameters using grid search with nested cross-validation.\n\n* See :ref:`sphx_glr_auto_examples_model_selection_plot_precision_recall.py`\n  for an example of :func:`precision_recall_curve` usage to evaluate\n  classifier output quality.\n\n.. rubric:: References\n\n.. [Manning2008] C.D. Manning, P. Raghavan, H. Sch\u00fctze, `Introduction to Information Retrieval\n    <https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html>`_,\n    2008.\n.. [Everingham2010] M. Everingham, L. Van Gool, C.K.I. Williams, J. Winn, A. Zisserman,\n    `The Pascal Visual Object Classes (VOC) Challenge\n    <https://citeseerx.ist.psu.edu/doc_view/pid/b6bebfd529b233f00cb854b7d8070319600cf59d>`_,\n    IJCV 2010.\n.. [Davis2006] J. Davis, M. Goadrich, `The Relationship Between Precision-Recall and ROC Curves\n    <https://www.biostat.wisc.edu/~page/rocpr.pdf>`_,\n    ICML 2006.\n.. [Flach2015] P.A. Flach, M. Kull, `Precision-Recall-Gain Curves: PR Analysis Done Right\n    <https://papers.nips.cc/paper/5867-precision-recall-gain-curves-pr-analysis-done-right.pdf>`_,\n    NIPS 2015."
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_20",
    "header": "Binary classification",
    "text": "Binary classification\n^^^^^^^^^^^^^^^^^^^^^\n\nIn a binary classification task, the terms ''positive'' and ''negative'' refer\nto the classifier's prediction, and the terms ''true'' and ''false'' refer to\nwhether that prediction corresponds to the external judgment (sometimes known\nas the ''observation''). Given these definitions, we can formulate the\nfollowing table:\n\n+-------------------+------------------------------------------------+\n|                   |    Actual class (observation)                  |\n+-------------------+---------------------+--------------------------+\n|   Predicted class | tp (true positive)  | fp (false positive)      |\n|   (expectation)   | Correct result      | Unexpected result        |\n|                   +---------------------+--------------------------+\n|                   | fn (false negative) | tn (true negative)       |\n|                   | Missing result      | Correct absence of result|\n+-------------------+---------------------+--------------------------+\n\nIn this context, we can define the notions of precision and recall:\n\n.. math::\n\n   \\text{precision} = \\frac{\\text{tp}}{\\text{tp} + \\text{fp}},\n\n.. math::\n\n   \\text{recall} = \\frac{\\text{tp}}{\\text{tp} + \\text{fn}},\n\n(Sometimes recall is also called ''sensitivity'')\n\nF-measure is the weighted harmonic mean of precision and recall, with precision's\ncontribution to the mean weighted by some parameter :math:`\\beta`:\n\n.. math::\n\n   F_\\beta = (1 + \\beta^2) \\frac{\\text{precision} \\times \\text{recall}}{\\beta^2 \\text{precision} + \\text{recall}}\n\nTo avoid division by zero when precision and recall are zero, Scikit-Learn calculates F-measure with this\notherwise-equivalent formula:\n\n.. math::\n\n   F_\\beta = \\frac{(1 + \\beta^2) \\text{tp}}{(1 + \\beta^2) \\text{tp} + \\text{fp} + \\beta^2 \\text{fn}}\n\nNote that this formula is still undefined when there are no true positives, false\npositives, or false negatives. By default, F-1 for a set of exclusively true negatives\nis calculated as 0, however this behavior can be changed using the `zero_division`\nparameter.\nHere are some small examples in binary classification::\n\n  >>> from sklearn import metrics\n  >>> y_pred = [0, 1, 0, 0]\n  >>> y_true = [0, 1, 0, 1]\n  >>> metrics.precision_score(y_true, y_pred)\n  1.0\n  >>> metrics.recall_score(y_true, y_pred)\n  0.5\n  >>> metrics.f1_score(y_true, y_pred)\n  0.66\n  >>> metrics.fbeta_score(y_true, y_pred, beta=0.5)\n  0.83\n  >>> metrics.fbeta_score(y_true, y_pred, beta=1)\n  0.66\n  >>> metrics.fbeta_score(y_true, y_pred, beta=2)\n  0.55\n  >>> metrics.precision_recall_fscore_support(y_true, y_pred, beta=0.5)\n  (array([0.66, 1.        ]), array([1. , 0.5]), array([0.71, 0.83]), array([2, 2]))\n\n\n  >>> import numpy as np\n  >>> from sklearn.metrics import precision_recall_curve\n  >>> from sklearn.metrics import average_precision_score\n  >>> y_true = np.array([0, 0, 1, 1])\n  >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n  >>> precision, recall, threshold = precision_recall_curve(y_true, y_scores)\n  >>> precision\n  array([0.5       , 0.66, 0.5       , 1.        , 1.        ])\n  >>> recall\n  array([1. , 1. , 0.5, 0.5, 0. ])\n  >>> threshold\n  array([0.1 , 0.35, 0.4 , 0.8 ])\n  >>> average_precision_score(y_true, y_scores)\n  0.83"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_21",
    "header": "Multiclass and multilabel classification",
    "text": "Multiclass and multilabel classification\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nIn a multiclass and multilabel classification task, the notions of precision,\nrecall, and F-measures can be applied to each label independently.\nThere are a few ways to combine results across labels,\nspecified by the ``average`` argument to the\n:func:`average_precision_score`, :func:`f1_score`,\n:func:`fbeta_score`, :func:`precision_recall_fscore_support`,\n:func:`precision_score` and :func:`recall_score` functions, as described\n:ref:`above <average>`.\n\nNote the following behaviors when averaging:\n\n* If all labels are included, \"micro\"-averaging in a multiclass setting will produce\n  precision, recall and :math:`F` that are all identical to accuracy.\n* \"weighted\" averaging may produce a F-score that is not between precision and recall.\n* \"macro\" averaging for F-measures is calculated as the arithmetic mean over\n  per-label/class F-measures, not the harmonic mean over the arithmetic precision and\n  recall means. Both calculations can be seen in the literature but are not equivalent,\n  see [OB2019]_ for details.\n\nTo make this more explicit, consider the following notation:\n\n* :math:`y` the set of *true* :math:`(sample, label)` pairs\n* :math:`\\hat{y}` the set of *predicted* :math:`(sample, label)` pairs\n* :math:`L` the set of labels\n* :math:`S` the set of samples\n* :math:`y_s` the subset of :math:`y` with sample :math:`s`,\n  i.e. :math:`y_s := \\left\\{(s', l) \\in y | s' = s\\right\\}`\n* :math:`y_l` the subset of :math:`y` with label :math:`l`\n* similarly, :math:`\\hat{y}_s` and :math:`\\hat{y}_l` are subsets of\n  :math:`\\hat{y}`\n* :math:`P(A, B) := \\frac{\\left| A \\cap B \\right|}{\\left|B\\right|}` for some\n  sets :math:`A` and :math:`B`\n* :math:`R(A, B) := \\frac{\\left| A \\cap B \\right|}{\\left|A\\right|}`\n  (Conventions vary on handling :math:`A = \\emptyset`; this implementation uses\n  :math:`R(A, B):=0`, and similar for :math:`P`.)\n* :math:`F_\\beta(A, B) := \\left(1 + \\beta^2\\right) \\frac{P(A, B) \\times R(A, B)}{\\beta^2 P(A, B) + R(A, B)}`\n\nThen the metrics are defined as:\n\n+---------------+------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+\n|``average``    | Precision                                                                                                        | Recall                                                                                                           | F\\_beta                                                                                                              |\n+===============+==================================================================================================================+==================================================================================================================+======================================================================================================================+\n|``\"micro\"``    | :math:`P(y, \\hat{y})`                                                                                            | :math:`R(y, \\hat{y})`                                                                                            | :math:`F_\\beta(y, \\hat{y})`                                                                                          |\n+---------------+------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+\n|``\"samples\"``  | :math:`\\frac{1}{\\left|S\\right|} \\sum_{s \\in S} P(y_s, \\hat{y}_s)`                                                | :math:`\\frac{1}{\\left|S\\right|} \\sum_{s \\in S} R(y_s, \\hat{y}_s)`                                                | :math:`\\frac{1}{\\left|S\\right|} \\sum_{s \\in S} F_\\beta(y_s, \\hat{y}_s)`                                              |\n+---------------+------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+\n|``\"macro\"``    | :math:`\\frac{1}{\\left|L\\right|} \\sum_{l \\in L} P(y_l, \\hat{y}_l)`                                                | :math:`\\frac{1}{\\left|L\\right|} \\sum_{l \\in L} R(y_l, \\hat{y}_l)`                                                | :math:`\\frac{1}{\\left|L\\right|} \\sum_{l \\in L} F_\\beta(y_l, \\hat{y}_l)`                                              |\n+---------------+------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+\n|``\"weighted\"`` | :math:`\\frac{1}{\\sum_{l \\in L} \\left|y_l\\right|} \\sum_{l \\in L} \\left|y_l\\right| P(y_l, \\hat{y}_l)`              | :math:`\\frac{1}{\\sum_{l \\in L} \\left|y_l\\right|} \\sum_{l \\in L} \\left|y_l\\right| R(y_l, \\hat{y}_l)`              | :math:`\\frac{1}{\\sum_{l \\in L} \\left|y_l\\right|} \\sum_{l \\in L} \\left|y_l\\right| F_\\beta(y_l, \\hat{y}_l)`            |\n+---------------+------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+\n|``None``       | :math:`\\langle P(y_l, \\hat{y}_l) | l \\in L \\rangle`                                                              | :math:`\\langle R(y_l, \\hat{y}_l) | l \\in L \\rangle`                                                              | :math:`\\langle F_\\beta(y_l, \\hat{y}_l) | l \\in L \\rangle`                                                            |\n+---------------+------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+\n\n  >>> from sklearn import metrics\n  >>> y_true = [0, 1, 2, 0, 1, 2]\n  >>> y_pred = [0, 2, 1, 0, 0, 1]\n  >>> metrics.precision_score(y_true, y_pred, average='macro')\n  0.22\n  >>> metrics.recall_score(y_true, y_pred, average='micro')\n  0.33\n  >>> metrics.f1_score(y_true, y_pred, average='weighted')\n  0.267\n  >>> metrics.fbeta_score(y_true, y_pred, average='macro', beta=0.5)\n  0.238\n  >>> metrics.precision_recall_fscore_support(y_true, y_pred, beta=0.5, average=None)\n  (array([0.667, 0., 0.]), array([1., 0., 0.]), array([0.714, 0., 0.]), array([2, 2, 2]))\n\nFor multiclass classification with a \"negative class\", it is possible to exclude some labels:\n\n  >>> metrics.recall_score(y_true, y_pred, labels=[1, 2], average='micro')\n  ... # excluding 0, no labels were correctly recalled\n  0.0\n\nSimilarly, labels not present in the data sample may be accounted for in macro-averaging.\n\n  >>> metrics.precision_score(y_true, y_pred, labels=[0, 1, 2, 3], average='macro')\n  0.166\n\n.. rubric:: References\n\n.. [OB2019] :arxiv:`Opitz, J., & Burst, S. (2019). \"Macro f1 and macro f1.\"\n    <1911.03347>`\n\n.. _jaccard_similarity_score:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_22",
    "header": "Jaccard similarity coefficient score",
    "text": "Jaccard similarity coefficient score\n-------------------------------------\n\nThe :func:`jaccard_score` function computes the average of `Jaccard similarity\ncoefficients <https://en.wikipedia.org/wiki/Jaccard_index>`_, also called the\nJaccard index, between pairs of label sets.\n\nThe Jaccard similarity coefficient with a ground truth label set :math:`y` and\npredicted label set :math:`\\hat{y}`, is defined as\n\n.. math::\n\n    J(y, \\hat{y}) = \\frac{|y \\cap \\hat{y}|}{|y \\cup \\hat{y}|}.\n\nThe :func:`jaccard_score` (like :func:`precision_recall_fscore_support`) applies\nnatively to binary targets. By computing it set-wise it can be extended to apply\nto multilabel and multiclass through the use of `average` (see\n:ref:`above <average>`).\n\nIn the binary case::\n\n  >>> import numpy as np\n  >>> from sklearn.metrics import jaccard_score\n  >>> y_true = np.array([[0, 1, 1],\n  ...                    [1, 1, 0]])\n  >>> y_pred = np.array([[1, 1, 1],\n  ...                    [1, 0, 0]])\n  >>> jaccard_score(y_true[0], y_pred[0])\n  0.6666\n\nIn the 2D comparison case (e.g. image similarity):\n\n  >>> jaccard_score(y_true, y_pred, average=\"micro\")\n  0.6\n\nIn the multilabel case with binary label indicators::\n\n  >>> jaccard_score(y_true, y_pred, average='samples')\n  0.5833\n  >>> jaccard_score(y_true, y_pred, average='macro')\n  0.6666\n  >>> jaccard_score(y_true, y_pred, average=None)\n  array([0.5, 0.5, 1. ])\n\nMulticlass problems are binarized and treated like the corresponding\nmultilabel problem::\n\n  >>> y_pred = [0, 2, 1, 2]\n  >>> y_true = [0, 1, 2, 2]\n  >>> jaccard_score(y_true, y_pred, average=None)\n  array([1. , 0. , 0.33])\n  >>> jaccard_score(y_true, y_pred, average='macro')\n  0.44\n  >>> jaccard_score(y_true, y_pred, average='micro')\n  0.33\n\n.. _hinge_loss:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_23",
    "header": "Hinge loss",
    "text": "Hinge loss\n----------\n\nThe :func:`hinge_loss` function computes the average distance between\nthe model and the data using\n`hinge loss <https://en.wikipedia.org/wiki/Hinge_loss>`_, a one-sided metric\nthat considers only prediction errors. (Hinge\nloss is used in maximal margin classifiers such as support vector machines.)\n\nIf the true label :math:`y_i` of a binary classification task is encoded as\n:math:`y_i=\\left\\{-1, +1\\right\\}` for every sample :math:`i`; and :math:`w_i`\nis the corresponding predicted decision (an array of shape (`n_samples`,) as\noutput by the `decision_function` method), then the hinge loss is defined as:\n\n.. math::\n\n  L_\\text{Hinge}(y, w) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples}-1} \\max\\left\\{1 - w_i y_i, 0\\right\\}\n\nIf there are more than two labels, :func:`hinge_loss` uses a multiclass variant\ndue to Crammer & Singer.\n`Here <https://jmlr.csail.mit.edu/papers/volume2/crammer01a/crammer01a.pdf>`_ is\nthe paper describing it.\n\nIn this case the predicted decision is an array of shape (`n_samples`,\n`n_labels`). If :math:`w_{i, y_i}` is the predicted decision for the true label\n:math:`y_i` of the :math:`i`-th sample; and\n:math:`\\hat{w}_{i, y_i} = \\max\\left\\{w_{i, y_j}~|~y_j \\ne y_i \\right\\}`\nis the maximum of the\npredicted decisions for all the other labels, then the multi-class hinge loss\nis defined by:\n\n.. math::\n\n  L_\\text{Hinge}(y, w) = \\frac{1}{n_\\text{samples}}\n  \\sum_{i=0}^{n_\\text{samples}-1} \\max\\left\\{1 + \\hat{w}_{i, y_i}\n  - w_{i, y_i}, 0\\right\\}\n\nHere is a small example demonstrating the use of the :func:`hinge_loss` function\nwith a svm classifier in a binary class problem::\n\n  >>> from sklearn import svm\n  >>> from sklearn.metrics import hinge_loss\n  >>> X = [[0], [1]]\n  >>> y = [-1, 1]\n  >>> est = svm.LinearSVC(random_state=0)\n  >>> est.fit(X, y)\n  LinearSVC(random_state=0)\n  >>> pred_decision = est.decision_function([[-2], [3], [0.5]])\n  >>> pred_decision\n  array([-2.18,  2.36,  0.09])\n  >>> hinge_loss([-1, 1, 1], pred_decision)\n  0.3\n\nHere is an example demonstrating the use of the :func:`hinge_loss` function\nwith a svm classifier in a multiclass problem::\n\n  >>> X = np.array([[0], [1], [2], [3]])\n  >>> Y = np.array([0, 1, 2, 3])\n  >>> labels = np.array([0, 1, 2, 3])\n  >>> est = svm.LinearSVC()\n  >>> est.fit(X, Y)\n  LinearSVC()\n  >>> pred_decision = est.decision_function([[-1], [2], [3]])\n  >>> y_true = [0, 2, 3]\n  >>> hinge_loss(y_true, pred_decision, labels=labels)\n  0.56\n\n.. _log_loss:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_24",
    "header": "Log loss",
    "text": "Log loss\n--------\n\nLog loss, also called logistic regression loss or\ncross-entropy loss, is defined on probability estimates.  It is\ncommonly used in (multinomial) logistic regression and neural networks, as well\nas in some variants of expectation-maximization, and can be used to evaluate the\nprobability outputs (``predict_proba``) of a classifier instead of its\ndiscrete predictions.\n\nFor binary classification with a true label :math:`y \\in \\{0,1\\}`\nand a probability estimate :math:`\\hat{p} \\approx \\operatorname{Pr}(y = 1)`,\nthe log loss per sample is the negative log-likelihood\nof the classifier given the true label:\n\n.. math::\n\n    L_{\\log}(y, \\hat{p}) = -\\log \\operatorname{Pr}(y|\\hat{p}) = -(y \\log (\\hat{p}) + (1 - y) \\log (1 - \\hat{p}))\n\nThis extends to the multiclass case as follows.\nLet the true labels for a set of samples\nbe encoded as a 1-of-K binary indicator matrix :math:`Y`,\ni.e., :math:`y_{i,k} = 1` if sample :math:`i` has label :math:`k`\ntaken from a set of :math:`K` labels.\nLet :math:`\\hat{P}` be a matrix of probability estimates,\nwith elements :math:`\\hat{p}_{i,k} \\approx \\operatorname{Pr}(y_{i,k} = 1)`.\nThen the log loss of the whole set is\n\n.. math::\n\n    L_{\\log}(Y, \\hat{P}) = -\\log \\operatorname{Pr}(Y|\\hat{P}) = - \\frac{1}{N} \\sum_{i=0}^{N-1} \\sum_{k=0}^{K-1} y_{i,k} \\log \\hat{p}_{i,k}\n\nTo see how this generalizes the binary log loss given above,\nnote that in the binary case,\n:math:`\\hat{p}_{i,0} = 1 - \\hat{p}_{i,1}` and :math:`y_{i,0} = 1 - y_{i,1}`,\nso expanding the inner sum over :math:`y_{i,k} \\in \\{0,1\\}`\ngives the binary log loss.\n\nThe :func:`log_loss` function computes log loss given a list of ground-truth\nlabels and a probability matrix, as returned by an estimator's ``predict_proba``\nmethod.\n\n    >>> from sklearn.metrics import log_loss\n    >>> y_true = [0, 0, 1, 1]\n    >>> y_pred = [[.9, .1], [.8, .2], [.3, .7], [.01, .99]]\n    >>> log_loss(y_true, y_pred)\n    0.1738\n\nThe first ``[.9, .1]`` in ``y_pred`` denotes 90% probability that the first\nsample has label 0.  The log loss is non-negative.\n\n.. _matthews_corrcoef:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_25",
    "header": "Matthews correlation coefficient",
    "text": "Matthews correlation coefficient\n---------------------------------\n\nThe :func:`matthews_corrcoef` function computes the\n`Matthew's correlation coefficient (MCC) <https://en.wikipedia.org/wiki/Matthews_correlation_coefficient>`_\nfor binary classes.  Quoting Wikipedia:\n\n\n    \"The Matthews correlation coefficient is used in machine learning as a\n    measure of the quality of binary (two-class) classifications. It takes\n    into account true and false positives and negatives and is generally\n    regarded as a balanced measure which can be used even if the classes are\n    of very different sizes. The MCC is in essence a correlation coefficient\n    value between -1 and +1. A coefficient of +1 represents a perfect\n    prediction, 0 an average random prediction and -1 an inverse prediction.\n    The statistic is also known as the phi coefficient.\"\n\n\nIn the binary (two-class) case, :math:`tp`, :math:`tn`, :math:`fp` and\n:math:`fn` are respectively the number of true positives, true negatives, false\npositives and false negatives, the MCC is defined as\n\n.. math::\n\n  MCC = \\frac{tp \\times tn - fp \\times fn}{\\sqrt{(tp + fp)(tp + fn)(tn + fp)(tn + fn)}}.\n\nIn the multiclass case, the Matthews correlation coefficient can be `defined\n<http://rk.kvl.dk/introduction/index.html>`_ in terms of a\n:func:`confusion_matrix` :math:`C` for :math:`K` classes.  To simplify the\ndefinition consider the following intermediate variables:\n\n* :math:`t_k=\\sum_{i}^{K} C_{ik}` the number of times class :math:`k` truly occurred,\n* :math:`p_k=\\sum_{i}^{K} C_{ki}` the number of times class :math:`k` was predicted,\n* :math:`c=\\sum_{k}^{K} C_{kk}` the total number of samples correctly predicted,\n* :math:`s=\\sum_{i}^{K} \\sum_{j}^{K} C_{ij}` the total number of samples.\n\nThen the multiclass MCC is defined as:\n\n.. math::\n    MCC = \\frac{\n        c \\times s - \\sum_{k}^{K} p_k \\times t_k\n    }{\\sqrt{\n        (s^2 - \\sum_{k}^{K} p_k^2) \\times\n        (s^2 - \\sum_{k}^{K} t_k^2)\n    }}\n\nWhen there are more than two labels, the value of the MCC will no longer range\nbetween -1 and +1. Instead the minimum value will be somewhere between -1 and 0\ndepending on the number and distribution of ground truth labels. The maximum\nvalue is always +1.\nFor additional information, see [WikipediaMCC2021]_.\n\nHere is a small example illustrating the usage of the :func:`matthews_corrcoef`\nfunction:\n\n    >>> from sklearn.metrics import matthews_corrcoef\n    >>> y_true = [+1, +1, +1, -1]\n    >>> y_pred = [+1, -1, +1, +1]\n    >>> matthews_corrcoef(y_true, y_pred)\n    -0.33\n\n.. rubric:: References\n\n.. [WikipediaMCC2021] Wikipedia contributors. Phi coefficient.\n   Wikipedia, The Free Encyclopedia. April 21, 2021, 12:21 CEST.\n   Available at: https://en.wikipedia.org/wiki/Phi_coefficient\n   Accessed April 21, 2021.\n\n.. _multilabel_confusion_matrix:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_26",
    "header": "Multi-label confusion matrix",
    "text": "Multi-label confusion matrix\n----------------------------\n\nThe :func:`multilabel_confusion_matrix` function computes class-wise (default)\nor sample-wise (samplewise=True) multilabel confusion matrix to evaluate\nthe accuracy of a classification. multilabel_confusion_matrix also treats\nmulticlass data as if it were multilabel, as this is a transformation commonly\napplied to evaluate multiclass problems with binary classification metrics\n(such as precision, recall, etc.).\n\nWhen calculating class-wise multilabel confusion matrix :math:`C`, the\ncount of true negatives for class :math:`i` is :math:`C_{i,0,0}`, false\nnegatives is :math:`C_{i,1,0}`, true positives is :math:`C_{i,1,1}`\nand false positives is :math:`C_{i,0,1}`.\n\nHere is an example demonstrating the use of the\n:func:`multilabel_confusion_matrix` function with\n:term:`multilabel indicator matrix` input::\n\n    >>> import numpy as np\n    >>> from sklearn.metrics import multilabel_confusion_matrix\n    >>> y_true = np.array([[1, 0, 1],\n    ...                    [0, 1, 0]])\n    >>> y_pred = np.array([[1, 0, 0],\n    ...                    [0, 1, 1]])\n    >>> multilabel_confusion_matrix(y_true, y_pred)\n    array([[[1, 0],\n            [0, 1]],\n    <BLANKLINE>\n           [[1, 0],\n            [0, 1]],\n    <BLANKLINE>\n           [[0, 1],\n            [1, 0]]])\n\nOr a confusion matrix can be constructed for each sample's labels:\n\n    >>> multilabel_confusion_matrix(y_true, y_pred, samplewise=True)\n    array([[[1, 0],\n            [1, 1]],\n    <BLANKLINE>\n           [[1, 1],\n            [0, 1]]])\n\nHere is an example demonstrating the use of the\n:func:`multilabel_confusion_matrix` function with\n:term:`multiclass` input::\n\n    >>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n    >>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n    >>> multilabel_confusion_matrix(y_true, y_pred,\n    ...                             labels=[\"ant\", \"bird\", \"cat\"])\n    array([[[3, 1],\n            [0, 2]],\n    <BLANKLINE>\n           [[5, 0],\n            [1, 0]],\n    <BLANKLINE>\n           [[2, 1],\n            [1, 2]]])\n\nHere are some examples demonstrating the use of the\n:func:`multilabel_confusion_matrix` function to calculate recall\n(or sensitivity), specificity, fall out and miss rate for each class in a\nproblem with multilabel indicator matrix input.\n\nCalculating\n`recall <https://en.wikipedia.org/wiki/Sensitivity_and_specificity>`__\n(also called the true positive rate or the sensitivity) for each class::\n\n    >>> y_true = np.array([[0, 0, 1],\n    ...                    [0, 1, 0],\n    ...                    [1, 1, 0]])\n    >>> y_pred = np.array([[0, 1, 0],\n    ...                    [0, 0, 1],\n    ...                    [1, 1, 0]])\n    >>> mcm = multilabel_confusion_matrix(y_true, y_pred)\n    >>> tn = mcm[:, 0, 0]\n    >>> tp = mcm[:, 1, 1]\n    >>> fn = mcm[:, 1, 0]\n    >>> fp = mcm[:, 0, 1]\n    >>> tp / (tp + fn)\n    array([1. , 0.5, 0. ])\n\nCalculating\n`specificity <https://en.wikipedia.org/wiki/Sensitivity_and_specificity>`__\n(also called the true negative rate) for each class::\n\n    >>> tn / (tn + fp)\n    array([1. , 0. , 0.5])\n\nCalculating `fall out <https://en.wikipedia.org/wiki/False_positive_rate>`__\n(also called the false positive rate) for each class::\n\n    >>> fp / (fp + tn)\n    array([0. , 1. , 0.5])\n\nCalculating `miss rate\n<https://en.wikipedia.org/wiki/False_positives_and_false_negatives>`__\n(also called the false negative rate) for each class::\n\n    >>> fn / (fn + tp)\n    array([0. , 0.5, 1. ])\n\n.. _roc_metrics:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_27",
    "header": "Receiver operating characteristic (ROC)",
    "text": "Receiver operating characteristic (ROC)\n---------------------------------------\n\nThe function :func:`roc_curve` computes the\n`receiver operating characteristic curve, or ROC curve <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_.\nQuoting Wikipedia :\n\n  \"A receiver operating characteristic (ROC), or simply ROC curve, is a\n  graphical plot which illustrates the performance of a binary classifier\n  system as its discrimination threshold is varied. It is created by plotting\n  the fraction of true positives out of the positives (TPR = true positive\n  rate) vs. the fraction of false positives out of the negatives (FPR = false\n  positive rate), at various threshold settings. TPR is also known as\n  sensitivity, and FPR is one minus the specificity or true negative rate.\"\n\nThis function requires the true binary value and the target scores, which can\neither be probability estimates of the positive class, confidence values, or\nbinary decisions. Here is a small example of how to use the :func:`roc_curve`\nfunction::\n\n    >>> import numpy as np\n    >>> from sklearn.metrics import roc_curve\n    >>> y = np.array([1, 1, 2, 2])\n    >>> scores = np.array([0.1, 0.4, 0.35, 0.8])\n    >>> fpr, tpr, thresholds = roc_curve(y, scores, pos_label=2)\n    >>> fpr\n    array([0. , 0. , 0.5, 0.5, 1. ])\n    >>> tpr\n    array([0. , 0.5, 0.5, 1. , 1. ])\n    >>> thresholds\n    array([ inf, 0.8 , 0.4 , 0.35, 0.1 ])\n\nCompared to metrics such as the subset accuracy, the Hamming loss, or the\nF1 score, ROC doesn't require optimizing a threshold for each label.\n\nThe :func:`roc_auc_score` function, denoted by ROC-AUC or AUROC, computes the\narea under the ROC curve. By doing so, the curve information is summarized in\none number.\n\nThe following figure shows the ROC curve and ROC-AUC score for a classifier\naimed to distinguish the virginica flower from the rest of the species in the\n:ref:`iris_dataset`:\n\n.. image:: ../auto_examples/model_selection/images/sphx_glr_plot_roc_001.png\n   :target: ../auto_examples/model_selection/plot_roc.html\n   :scale: 75\n   :align: center\n\n\n\nFor more information see the `Wikipedia article on AUC\n<https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve>`_.\n\n.. _roc_auc_binary:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_28",
    "header": "Binary case",
    "text": "Binary case\n^^^^^^^^^^^\n\nIn the **binary case**, you can either provide the probability estimates, using\nthe `classifier.predict_proba()` method, or the non-thresholded decision values\ngiven by the `classifier.decision_function()` method. In the case of providing\nthe probability estimates, the probability of the class with the\n\"greater label\" should be provided. The \"greater label\" corresponds to\n`classifier.classes_[1]` and thus `classifier.predict_proba(X)[:, 1]`.\nTherefore, the `y_score` parameter is of size (n_samples,).\n\n  >>> from sklearn.datasets import load_breast_cancer\n  >>> from sklearn.linear_model import LogisticRegression\n  >>> from sklearn.metrics import roc_auc_score\n  >>> X, y = load_breast_cancer(return_X_y=True)\n  >>> clf = LogisticRegression().fit(X, y)\n  >>> clf.classes_\n  array([0, 1])\n\nWe can use the probability estimates corresponding to `clf.classes_[1]`.\n\n  >>> y_score = clf.predict_proba(X)[:, 1]\n  >>> roc_auc_score(y, y_score)\n  0.99\n\nOtherwise, we can use the non-thresholded decision values\n\n  >>> roc_auc_score(y, clf.decision_function(X))\n  0.99\n\n.. _roc_auc_multiclass:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_29",
    "header": "Multi-class case",
    "text": "Multi-class case\n^^^^^^^^^^^^^^^^\n\nThe :func:`roc_auc_score` function can also be used in **multi-class\nclassification**. Two averaging strategies are currently supported: the\none-vs-one algorithm computes the average of the pairwise ROC AUC scores, and\nthe one-vs-rest algorithm computes the average of the ROC AUC scores for each\nclass against all other classes. In both cases, the predicted labels are\nprovided in an array with values from 0 to ``n_classes``, and the scores\ncorrespond to the probability estimates that a sample belongs to a particular\nclass. The OvO and OvR algorithms support weighting uniformly\n(``average='macro'``) and by prevalence (``average='weighted'``).\n\n.. dropdown:: One-vs-one Algorithm\n\n  Computes the average AUC of all possible pairwise\n  combinations of classes. [HT2001]_ defines a multiclass AUC metric weighted\n  uniformly:\n\n  .. math::\n\n    \\frac{1}{c(c-1)}\\sum_{j=1}^{c}\\sum_{k > j}^c (\\text{AUC}(j | k) +\n    \\text{AUC}(k | j))\n\n  where :math:`c` is the number of classes and :math:`\\text{AUC}(j | k)` is the\n  AUC with class :math:`j` as the positive class and class :math:`k` as the\n  negative class. In general,\n  :math:`\\text{AUC}(j | k) \\neq \\text{AUC}(k | j))` in the multiclass\n  case. This algorithm is used by setting the keyword argument ``multiclass``\n  to ``'ovo'`` and ``average`` to ``'macro'``.\n\n  The [HT2001]_ multiclass AUC metric can be extended to be weighted by the\n  prevalence:\n\n  .. math::\n\n    \\frac{1}{c(c-1)}\\sum_{j=1}^{c}\\sum_{k > j}^c p(j \\cup k)(\n    \\text{AUC}(j | k) + \\text{AUC}(k | j))\n\n  where :math:`c` is the number of classes. This algorithm is used by setting\n  the keyword argument ``multiclass`` to ``'ovo'`` and ``average`` to\n  ``'weighted'``. The ``'weighted'`` option returns a prevalence-weighted average\n  as described in [FC2009]_.\n\n.. dropdown:: One-vs-rest Algorithm\n\n  Computes the AUC of each class against the rest\n  [PD2000]_. The algorithm is functionally the same as the multilabel case. To\n  enable this algorithm set the keyword argument ``multiclass`` to ``'ovr'``.\n  Additionally to ``'macro'`` [F2006]_ and ``'weighted'`` [F2001]_ averaging, OvR\n  supports ``'micro'`` averaging.\n\n  In applications where a high false positive rate is not tolerable the parameter\n  ``max_fpr`` of :func:`roc_auc_score` can be used to summarize the ROC curve up\n  to the given limit.\n\n  The following figure shows the micro-averaged ROC curve and its corresponding\n  ROC-AUC score for a classifier aimed to distinguish the different species in\n  the :ref:`iris_dataset`:\n\n  .. image:: ../auto_examples/model_selection/images/sphx_glr_plot_roc_002.png\n    :target: ../auto_examples/model_selection/plot_roc.html\n    :scale: 75\n    :align: center\n\n.. _roc_auc_multilabel:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_30",
    "header": "Multi-label case",
    "text": "Multi-label case\n^^^^^^^^^^^^^^^^\n\nIn **multi-label classification**, the :func:`roc_auc_score` function is\nextended by averaging over the labels as :ref:`above <average>`. In this case,\nyou should provide a `y_score` of shape `(n_samples, n_classes)`. Thus, when\nusing the probability estimates, one needs to select the probability of the\nclass with the greater label for each output.\n\n  >>> from sklearn.datasets import make_multilabel_classification\n  >>> from sklearn.multioutput import MultiOutputClassifier\n  >>> X, y = make_multilabel_classification(random_state=0)\n  >>> inner_clf = LogisticRegression(random_state=0)\n  >>> clf = MultiOutputClassifier(inner_clf).fit(X, y)\n  >>> y_score = np.transpose([y_pred[:, 1] for y_pred in clf.predict_proba(X)])\n  >>> roc_auc_score(y, y_score, average=None)\n  array([0.828, 0.851, 0.94, 0.87, 0.95])\n\nAnd the decision values do not require such processing.\n\n  >>> from sklearn.linear_model import RidgeClassifierCV\n  >>> clf = RidgeClassifierCV().fit(X, y)\n  >>> y_score = clf.decision_function(X)\n  >>> roc_auc_score(y, y_score, average=None)\n  array([0.82, 0.85, 0.93, 0.87, 0.94])\n\n.. rubric:: Examples\n\n* See :ref:`sphx_glr_auto_examples_model_selection_plot_roc.py` for an example of\n  using ROC to evaluate the quality of the output of a classifier.\n\n* See :ref:`sphx_glr_auto_examples_model_selection_plot_roc_crossval.py`  for an\n  example of using ROC to evaluate classifier output quality, using cross-validation.\n\n* See :ref:`sphx_glr_auto_examples_applications_plot_species_distribution_modeling.py`\n  for an example of using ROC to model species distribution.\n\n.. rubric:: References\n\n.. [HT2001] Hand, D.J. and Till, R.J., (2001). `A simple generalisation\n   of the area under the ROC curve for multiple class classification problems.\n   <http://link.springer.com/article/10.1023/A:1010920819831>`_\n   Machine learning, 45(2), pp. 171-186.\n\n.. [FC2009] Ferri, C\u00e8sar & Hernandez-Orallo, Jose & Modroiu, R. (2009).\n   `An Experimental Comparison of Performance Measures for Classification.\n   <https://www.math.ucdavis.edu/~saito/data/roc/ferri-class-perf-metrics.pdf>`_\n   Pattern Recognition Letters. 30. 27-38.\n\n.. [PD2000] Provost, F., Domingos, P. (2000). `Well-trained PETs: Improving\n   probability estimation trees\n   <https://fosterprovost.com/publication/well-trained-pets-improving-probability-estimation-trees/>`_\n   (Section 6.2), CeDER Working Paper #IS-00-04, Stern School of Business,\n   New York University.\n\n.. [F2006] Fawcett, T., 2006. `An introduction to ROC analysis.\n   <http://www.sciencedirect.com/science/article/pii/S016786550500303X>`_\n   Pattern Recognition Letters, 27(8), pp. 861-874.\n\n.. [F2001] Fawcett, T., 2001. `Using rule sets to maximize\n   ROC performance <https://ieeexplore.ieee.org/document/989510/>`_\n   In Data Mining, 2001.\n   Proceedings IEEE International Conference, pp. 131-138.\n\n.. _det_curve:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_31",
    "header": "Detection error tradeoff (DET)",
    "text": "Detection error tradeoff (DET)\n------------------------------\n\nThe function :func:`det_curve` computes the\ndetection error tradeoff curve (DET) curve [WikipediaDET2017]_.\nQuoting Wikipedia:\n\n  \"A detection error tradeoff (DET) graph is a graphical plot of error rates\n  for binary classification systems, plotting false reject rate vs. false\n  accept rate. The x- and y-axes are scaled non-linearly by their standard\n  normal deviates (or just by logarithmic transformation), yielding tradeoff\n  curves that are more linear than ROC curves, and use most of the image area\n  to highlight the differences of importance in the critical operating region.\"\n\nDET curves are a variation of receiver operating characteristic (ROC) curves\nwhere False Negative Rate is plotted on the y-axis instead of True Positive\nRate.\nDET curves are commonly plotted in normal deviate scale by transformation with\n:math:`\\phi^{-1}` (with :math:`\\phi` being the cumulative distribution\nfunction).\nThe resulting performance curves explicitly visualize the tradeoff of error\ntypes for given classification algorithms.\nSee [Martin1997]_ for examples and further motivation.\n\nThis figure compares the ROC and DET curves of two example classifiers on the\nsame classification task:\n\n.. image:: ../auto_examples/model_selection/images/sphx_glr_plot_det_001.png\n   :target: ../auto_examples/model_selection/plot_det.html\n   :scale: 75\n   :align: center\n\n.. dropdown:: Properties\n\n  * DET curves form a linear curve in normal deviate scale if the detection\n    scores are normally (or close-to normally) distributed.\n    It was shown by [Navratil2007]_ that the reverse is not necessarily true and\n    even more general distributions are able to produce linear DET curves.\n\n  * The normal deviate scale transformation spreads out the points such that a\n    comparatively larger space of plot is occupied.\n    Therefore curves with similar classification performance might be easier to\n    distinguish on a DET plot.\n\n  * With False Negative Rate being \"inverse\" to True Positive Rate the point\n    of perfection for DET curves is the origin (in contrast to the top left\n    corner for ROC curves).\n\n.. dropdown:: Applications and limitations\n\n  DET curves are intuitive to read and hence allow quick visual assessment of a\n  classifier's performance.\n  Additionally DET curves can be consulted for threshold analysis and operating\n  point selection.\n  This is particularly helpful if a comparison of error types is required.\n\n  On the other hand DET curves do not provide their metric as a single number.\n  Therefore for either automated evaluation or comparison to other\n  classification tasks metrics like the derived area under ROC curve might be\n  better suited.\n\n.. rubric:: Examples\n\n* See :ref:`sphx_glr_auto_examples_model_selection_plot_det.py`\n  for an example comparison between receiver operating characteristic (ROC)\n  curves and Detection error tradeoff (DET) curves.\n\n.. rubric:: References\n\n.. [WikipediaDET2017] Wikipedia contributors. Detection error tradeoff.\n    Wikipedia, The Free Encyclopedia. September 4, 2017, 23:33 UTC.\n    Available at: https://en.wikipedia.org/w/index.php?title=Detection_error_tradeoff&oldid=798982054.\n    Accessed February 19, 2018.\n\n.. [Martin1997] A. Martin, G. Doddington, T. Kamm, M. Ordowski, and M. Przybocki,\n    `The DET Curve in Assessment of Detection Task Performance\n    <https://ccc.inaoep.mx/~villasen/bib/martin97det.pdf>`_, NIST 1997.\n\n.. [Navratil2007] J. Navratil and D. Klusacek,\n    `\"On Linear DETs\" <https://ieeexplore.ieee.org/document/4218079>`_,\n    2007 IEEE International Conference on Acoustics,\n    Speech and Signal Processing - ICASSP '07, Honolulu,\n    HI, 2007, pp. IV-229-IV-232.\n\n.. _zero_one_loss:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_32",
    "header": "Zero one loss",
    "text": "Zero one loss\n--------------\n\nThe :func:`zero_one_loss` function computes the sum or the average of the 0-1\nclassification loss (:math:`L_{0-1}`) over :math:`n_{\\text{samples}}`. By\ndefault, the function normalizes over the sample. To get the sum of the\n:math:`L_{0-1}`, set ``normalize`` to ``False``.\n\nIn multilabel classification, the :func:`zero_one_loss` scores a subset as\none if its labels strictly match the predictions, and as a zero if there\nare any errors.  By default, the function returns the percentage of imperfectly\npredicted subsets.  To get the count of such subsets instead, set\n``normalize`` to ``False``.\n\nIf :math:`\\hat{y}_i` is the predicted value of\nthe :math:`i`-th sample and :math:`y_i` is the corresponding true value,\nthen the 0-1 loss :math:`L_{0-1}` is defined as:\n\n.. math::\n\n   L_{0-1}(y, \\hat{y}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples}-1} 1(\\hat{y}_i \\not= y_i)\n\nwhere :math:`1(x)` is the `indicator function\n<https://en.wikipedia.org/wiki/Indicator_function>`_. The zero-one\nloss can also be computed as :math:`\\text{zero-one loss} = 1 - \\text{accuracy}`.\n\n\n  >>> from sklearn.metrics import zero_one_loss\n  >>> y_pred = [1, 2, 3, 4]\n  >>> y_true = [2, 2, 3, 4]\n  >>> zero_one_loss(y_true, y_pred)\n  0.25\n  >>> zero_one_loss(y_true, y_pred, normalize=False)\n  1.0\n\nIn the multilabel case with binary label indicators, where the first label\nset [0,1] has an error::\n\n  >>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n  0.5\n\n  >>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)),  normalize=False)\n  1.0\n\n.. rubric:: Examples\n\n* See :ref:`sphx_glr_auto_examples_feature_selection_plot_rfe_with_cross_validation.py`\n  for an example of zero one loss usage to perform recursive feature\n  elimination with cross-validation.\n\n.. _brier_score_loss:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_33",
    "header": "Brier score loss",
    "text": "Brier score loss\n----------------\n\nThe :func:`brier_score_loss` function computes the `Brier score\n<https://en.wikipedia.org/wiki/Brier_score>`_ for binary and multiclass\nprobabilistic predictions and is equivalent to the mean squared error.\nQuoting Wikipedia:\n\n    \"The Brier score is a strictly proper scoring rule that measures the accuracy of\n    probabilistic predictions. [...] [It] is applicable to tasks in which predictions\n    must assign probabilities to a set of mutually exclusive discrete outcomes or\n    classes.\"\n\nLet the true labels for a set of :math:`N` data points be encoded as a 1-of-K binary\nindicator matrix :math:`Y`, i.e., :math:`y_{i,k} = 1` if sample :math:`i` has\nlabel :math:`k` taken from a set of :math:`K` labels. Let :math:`\\hat{P}` be a matrix\nof probability estimates with elements :math:`\\hat{p}_{i,k} \\approx \\operatorname{Pr}(y_{i,k} = 1)`.\nFollowing the original definition by [Brier1950]_, the Brier score is given by:\n\n.. math::\n\n  BS(Y, \\hat{P}) = \\frac{1}{N}\\sum_{i=0}^{N-1}\\sum_{k=0}^{K-1}(y_{i,k} - \\hat{p}_{i,k})^{2}\n\nThe Brier score lies in the interval :math:`[0, 2]` and the lower the value the\nbetter the probability estimates are (the mean squared difference is smaller).\nActually, the Brier score is a strictly proper scoring rule, meaning that it\nachieves the best score only when the estimated probabilities equal the\ntrue ones.\n\nNote that in the binary case, the Brier score is usually divided by two and\nranges between :math:`[0,1]`. For binary targets :math:`y_i \\in \\{0, 1\\}` and\nprobability estimates :math:`\\hat{p}_i  \\approx \\operatorname{Pr}(y_i = 1)`\nfor the positive class, the Brier score is then equal to:\n\n.. math::\n\n   BS(y, \\hat{p}) = \\frac{1}{N} \\sum_{i=0}^{N - 1}(y_i - \\hat{p}_i)^2\n\nThe :func:`brier_score_loss` function computes the Brier score given the\nground-truth labels and predicted probabilities, as returned by an estimator's\n``predict_proba`` method. The `scale_by_half` parameter controls which of the\ntwo above definitions to follow.\n\n\n    >>> import numpy as np\n    >>> from sklearn.metrics import brier_score_loss\n    >>> y_true = np.array([0, 1, 1, 0])\n    >>> y_true_categorical = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n    >>> y_prob = np.array([0.1, 0.9, 0.8, 0.4])\n    >>> brier_score_loss(y_true, y_prob)\n    0.055\n    >>> brier_score_loss(y_true, 1 - y_prob, pos_label=0)\n    0.055\n    >>> brier_score_loss(y_true_categorical, y_prob, pos_label=\"ham\")\n    0.055\n    >>> brier_score_loss(\n    ...    [\"eggs\", \"ham\", \"spam\"],\n    ...    [[0.8, 0.1, 0.1], [0.2, 0.7, 0.1], [0.2, 0.2, 0.6]],\n    ...    labels=[\"eggs\", \"ham\", \"spam\"],\n    ... )\n    0.146\n\nThe Brier score can be used to assess how well a classifier is calibrated.\nHowever, a lower Brier score loss does not always mean a better calibration.\nThis is because, by analogy with the bias-variance decomposition of the mean\nsquared error, the Brier score loss can be decomposed as the sum of calibration\nloss and refinement loss [Bella2012]_. Calibration loss is defined as the mean\nsquared deviation from empirical probabilities derived from the slope of ROC\nsegments. Refinement loss can be defined as the expected optimal loss as\nmeasured by the area under the optimal cost curve. Refinement loss can change\nindependently from calibration loss, thus a lower Brier score loss does not\nnecessarily mean a better calibrated model. \"Only when refinement loss remains\nthe same does a lower Brier score loss always mean better calibration\"\n[Bella2012]_, [Flach2008]_.\n\n.. rubric:: Examples\n\n* See :ref:`sphx_glr_auto_examples_calibration_plot_calibration.py`\n  for an example of Brier score loss usage to perform probability\n  calibration of classifiers.\n\n.. rubric:: References\n\n.. [Brier1950] G. Brier, `Verification of forecasts expressed in terms of probability\n  <ftp://ftp.library.noaa.gov/docs.lib/htdocs/rescue/mwr/078/mwr-078-01-0001.pdf>`_,\n  Monthly weather review 78.1 (1950)\n\n.. [Bella2012] Bella, Ferri, Hern\u00e1ndez-Orallo, and Ram\u00edrez-Quintana\n  `\"Calibration of Machine Learning Models\"\n  <http://dmip.webs.upv.es/papers/BFHRHandbook2010.pdf>`_\n  in Khosrow-Pour, M. \"Machine learning: concepts, methodologies, tools\n  and applications.\" Hershey, PA: Information Science Reference (2012).\n\n.. [Flach2008] Flach, Peter, and Edson Matsubara. `\"On classification, ranking,\n  and probability estimation.\" <https://drops.dagstuhl.de/opus/volltexte/2008/1382/>`_\n  Dagstuhl Seminar Proceedings. Schloss Dagstuhl-Leibniz-Zentrum f\u00fcr Informatik (2008).\n\n.. _class_likelihood_ratios:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_34",
    "header": "Class likelihood ratios",
    "text": "Class likelihood ratios\n-----------------------\n\nThe :func:`class_likelihood_ratios` function computes the `positive and negative\nlikelihood ratios\n<https://en.wikipedia.org/wiki/Likelihood_ratios_in_diagnostic_testing>`_\n:math:`LR_\\pm` for binary classes, which can be interpreted as the ratio of\npost-test to pre-test odds as explained below. As a consequence, this metric is\ninvariant w.r.t. the class prevalence (the number of samples in the positive\nclass divided by the total number of samples) and **can be extrapolated between\npopulations regardless of any possible class imbalance.**\n\nThe :math:`LR_\\pm` metrics are therefore very useful in settings where the data\navailable to learn and evaluate a classifier is a study population with nearly\nbalanced classes, such as a case-control study, while the target application,\ni.e. the general population, has very low prevalence.\n\nThe positive likelihood ratio :math:`LR_+` is the probability of a classifier to\ncorrectly predict that a sample belongs to the positive class divided by the\nprobability of predicting the positive class for a sample belonging to the\nnegative class:\n\n.. math::\n\n   LR_+ = \\frac{\\text{PR}(P+|T+)}{\\text{PR}(P+|T-)}.\n\nThe notation here refers to predicted (:math:`P`) or true (:math:`T`) label and\nthe sign :math:`+` and :math:`-` refer to the positive and negative class,\nrespectively, e.g. :math:`P+` stands for \"predicted positive\".\n\nAnalogously, the negative likelihood ratio :math:`LR_-` is the probability of a\nsample of the positive class being classified as belonging to the negative class\ndivided by the probability of a sample of the negative class being correctly\nclassified:\n\n.. math::\n\n   LR_- = \\frac{\\text{PR}(P-|T+)}{\\text{PR}(P-|T-)}.\n\nFor classifiers above chance :math:`LR_+` above 1 **higher is better**, while\n:math:`LR_-` ranges from 0 to 1 and **lower is better**.\nValues of :math:`LR_\\pm\\approx 1` correspond to chance level.\n\nNotice that probabilities differ from counts, for instance\n:math:`\\operatorname{PR}(P+|T+)` is not equal to the number of true positive\ncounts ``tp`` (see `the wikipedia page\n<https://en.wikipedia.org/wiki/Likelihood_ratios_in_diagnostic_testing>`_ for\nthe actual formulas).\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_model_selection_plot_likelihood_ratios.py`\n\n.. dropdown:: Interpretation across varying prevalence\n\n  Both class likelihood ratios are interpretable in terms of an odds ratio\n  (pre-test and post-tests):\n\n  .. math::\n\n    \\text{post-test odds} = \\text{Likelihood ratio} \\times \\text{pre-test odds}.\n\n  Odds are in general related to probabilities via\n\n  .. math::\n\n    \\text{odds} = \\frac{\\text{probability}}{1 - \\text{probability}},\n\n  or equivalently\n\n  .. math::\n\n    \\text{probability} = \\frac{\\text{odds}}{1 + \\text{odds}}.\n\n  On a given population, the pre-test probability is given by the prevalence. By\n  converting odds to probabilities, the likelihood ratios can be translated into a\n  probability of truly belonging to either class before and after a classifier\n  prediction:\n\n  .. math::\n\n    \\text{post-test odds} = \\text{Likelihood ratio} \\times\n    \\frac{\\text{pre-test probability}}{1 - \\text{pre-test probability}},\n\n  .. math::\n\n    \\text{post-test probability} = \\frac{\\text{post-test odds}}{1 + \\text{post-test odds}}.\n\n.. dropdown:: Mathematical divergences\n\n  The positive likelihood ratio (`LR+`) is undefined when :math:`fp=0`, meaning the\n  classifier does not misclassify any negative labels as positives. This condition can\n  either indicate a perfect identification of all the negative cases or, if there are\n  also no true positive predictions (:math:`tp=0`), that the classifier does not predict\n  the positive class at all. In the first case, `LR+` can be interpreted as `np.inf`, in\n  the second case (for instance, with highly imbalanced data) it can be interpreted as\n  `np.nan`.\n\n  The negative likelihood ratio (`LR-`) is undefined when :math:`tn=0`. Such\n  divergence is invalid, as :math:`LR_- > 1.0` would indicate an increase in the odds of\n  a sample belonging to the positive class after being classified as negative, as if the\n  act of classifying caused the positive condition. This includes the case of a\n  :class:`~sklearn.dummy.DummyClassifier` that always predicts the positive class\n  (i.e. when :math:`tn=fn=0`).\n\n  Both class likelihood ratios (`LR+ and LR-`) are undefined when :math:`tp=fn=0`, which\n  means that no samples of the positive class were present in the test set. This can\n  happen when cross-validating on highly imbalanced data and also leads to a division by\n  zero.\n\n  If a division by zero occurs and `raise_warning` is set to `True` (default),\n  :func:`class_likelihood_ratios` raises an `UndefinedMetricWarning` and returns\n  `np.nan` by default to avoid pollution when averaging over cross-validation folds.\n  Users can set return values in case of a division by zero with the\n  `replace_undefined_by` param.\n\n  For a worked-out demonstration of the :func:`class_likelihood_ratios` function,\n  see the example below.\n\n.. dropdown:: References\n\n  * `Wikipedia entry for Likelihood ratios in diagnostic testing\n    <https://en.wikipedia.org/wiki/Likelihood_ratios_in_diagnostic_testing>`_\n\n  * Brenner, H., & Gefeller, O. (1997).\n    Variation of sensitivity, specificity, likelihood ratios and predictive\n    values with disease prevalence. Statistics in medicine, 16(9), 981-991.\n\n\n.. _d2_score_classification:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_35",
    "header": "D\u00b2 score for classification",
    "text": "D\u00b2 score for classification\n---------------------------\n\nThe D\u00b2 score computes the fraction of deviance explained.\nIt is a generalization of R\u00b2, where the squared error is generalized and replaced\nby a classification deviance of choice :math:`\\text{dev}(y, \\hat{y})`\n(e.g., Log loss). D\u00b2 is a form of a *skill score*.\nIt is calculated as\n\n.. math::\n\n  D^2(y, \\hat{y}) = 1 - \\frac{\\text{dev}(y, \\hat{y})}{\\text{dev}(y, y_{\\text{null}})} \\,.\n\nWhere :math:`y_{\\text{null}}` is the optimal prediction of an intercept-only model\n(e.g., the per-class proportion of `y_true` in the case of the Log loss).\n\nLike R\u00b2, the best possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always predicts\n:math:`y_{\\text{null}}`, disregarding the input features, would get a D\u00b2 score\nof 0.0.\n\n.. dropdown:: D2 log loss score\n\n  The :func:`d2_log_loss_score` function implements the special case\n  of D\u00b2 with the log loss, see :ref:`log_loss`, i.e.:\n\n  .. math::\n\n    \\text{dev}(y, \\hat{y}) = \\text{log_loss}(y, \\hat{y}).\n\n  Here are some usage examples of the :func:`d2_log_loss_score` function::\n\n    >>> from sklearn.metrics import d2_log_loss_score\n    >>> y_true = [1, 1, 2, 3]\n    >>> y_pred = [\n    ...    [0.5, 0.25, 0.25],\n    ...    [0.5, 0.25, 0.25],\n    ...    [0.5, 0.25, 0.25],\n    ...    [0.5, 0.25, 0.25],\n    ... ]\n    >>> d2_log_loss_score(y_true, y_pred)\n    0.0\n    >>> y_true = [1, 2, 3]\n    >>> y_pred = [\n    ...     [0.98, 0.01, 0.01],\n    ...     [0.01, 0.98, 0.01],\n    ...     [0.01, 0.01, 0.98],\n    ... ]\n    >>> d2_log_loss_score(y_true, y_pred)\n    0.981\n    >>> y_true = [1, 2, 3]\n    >>> y_pred = [\n    ...     [0.1, 0.6, 0.3],\n    ...     [0.1, 0.6, 0.3],\n    ...     [0.4, 0.5, 0.1],\n    ... ]\n    >>> d2_log_loss_score(y_true, y_pred)\n    -0.552\n\n\n.. _multilabel_ranking_metrics:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_36",
    "header": "Multilabel ranking metrics",
    "text": "Multilabel ranking metrics\n==========================\n\n.. currentmodule:: sklearn.metrics\n\nIn multilabel learning, each sample can have any number of ground truth labels\nassociated with it. The goal is to give high scores and better rank to\nthe ground truth labels.\n\n.. _coverage_error:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_37",
    "header": "Coverage error",
    "text": "Coverage error\n--------------\n\nThe :func:`coverage_error` function computes the average number of labels that\nhave to be included in the final prediction such that all true labels\nare predicted. This is useful if you want to know how many top-scored-labels\nyou have to predict in average without missing any true one. The best value\nof this metric is thus the average number of true labels.\n\n.. note::\n\n    Our implementation's score is 1 greater than the one given in Tsoumakas\n    et al., 2010. This extends it to handle the degenerate case in which an\n    instance has 0 true labels.\n\nFormally, given a binary indicator matrix of the ground truth labels\n:math:`y \\in \\left\\{0, 1\\right\\}^{n_\\text{samples} \\times n_\\text{labels}}` and the\nscore associated with each label\n:math:`\\hat{f} \\in \\mathbb{R}^{n_\\text{samples} \\times n_\\text{labels}}`,\nthe coverage is defined as\n\n.. math::\n  coverage(y, \\hat{f}) = \\frac{1}{n_{\\text{samples}}}\n    \\sum_{i=0}^{n_{\\text{samples}} - 1} \\max_{j:y_{ij} = 1} \\text{rank}_{ij}\n\nwith :math:`\\text{rank}_{ij} = \\left|\\left\\{k: \\hat{f}_{ik} \\geq \\hat{f}_{ij} \\right\\}\\right|`.\nGiven the rank definition, ties in ``y_scores`` are broken by giving the\nmaximal rank that would have been assigned to all tied values.\n\nHere is a small example of usage of this function::\n\n    >>> import numpy as np\n    >>> from sklearn.metrics import coverage_error\n    >>> y_true = np.array([[1, 0, 0], [0, 0, 1]])\n    >>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n    >>> coverage_error(y_true, y_score)\n    2.5\n\n.. _label_ranking_average_precision:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_38",
    "header": "Label ranking average precision",
    "text": "Label ranking average precision\n-------------------------------\n\nThe :func:`label_ranking_average_precision_score` function\nimplements label ranking average precision (LRAP). This metric is linked to\nthe :func:`average_precision_score` function, but is based on the notion of\nlabel ranking instead of precision and recall.\n\nLabel ranking average precision (LRAP) averages over the samples the answer to\nthe following question: for each ground truth label, what fraction of\nhigher-ranked labels were true labels? This performance measure will be higher\nif you are able to give better rank to the labels associated with each sample.\nThe obtained score is always strictly greater than 0, and the best value is 1.\nIf there is exactly one relevant label per sample, label ranking average\nprecision is equivalent to the `mean\nreciprocal rank <https://en.wikipedia.org/wiki/Mean_reciprocal_rank>`_.\n\nFormally, given a binary indicator matrix of the ground truth labels\n:math:`y \\in \\left\\{0, 1\\right\\}^{n_\\text{samples} \\times n_\\text{labels}}`\nand the score associated with each label\n:math:`\\hat{f} \\in \\mathbb{R}^{n_\\text{samples} \\times n_\\text{labels}}`,\nthe average precision is defined as\n\n.. math::\n  LRAP(y, \\hat{f}) = \\frac{1}{n_{\\text{samples}}}\n    \\sum_{i=0}^{n_{\\text{samples}} - 1} \\frac{1}{||y_i||_0}\n    \\sum_{j:y_{ij} = 1} \\frac{|\\mathcal{L}_{ij}|}{\\text{rank}_{ij}}\n\n\nwhere\n:math:`\\mathcal{L}_{ij} = \\left\\{k: y_{ik} = 1, \\hat{f}_{ik} \\geq \\hat{f}_{ij} \\right\\}`,\n:math:`\\text{rank}_{ij} = \\left|\\left\\{k: \\hat{f}_{ik} \\geq \\hat{f}_{ij} \\right\\}\\right|`,\n:math:`|\\cdot|` computes the cardinality of the set (i.e., the number of\nelements in the set), and :math:`||\\cdot||_0` is the :math:`\\ell_0` \"norm\"\n(which computes the number of nonzero elements in a vector).\n\nHere is a small example of usage of this function::\n\n    >>> import numpy as np\n    >>> from sklearn.metrics import label_ranking_average_precision_score\n    >>> y_true = np.array([[1, 0, 0], [0, 0, 1]])\n    >>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n    >>> label_ranking_average_precision_score(y_true, y_score)\n    0.416\n\n.. _label_ranking_loss:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_39",
    "header": "Ranking loss",
    "text": "Ranking loss\n------------\n\nThe :func:`label_ranking_loss` function computes the ranking loss which\naverages over the samples the number of label pairs that are incorrectly\nordered, i.e. true labels have a lower score than false labels, weighted by\nthe inverse of the number of ordered pairs of false and true labels.\nThe lowest achievable ranking loss is zero.\n\nFormally, given a binary indicator matrix of the ground truth labels\n:math:`y \\in \\left\\{0, 1\\right\\}^{n_\\text{samples} \\times n_\\text{labels}}` and the\nscore associated with each label\n:math:`\\hat{f} \\in \\mathbb{R}^{n_\\text{samples} \\times n_\\text{labels}}`,\nthe ranking loss is defined as\n\n.. math::\n  ranking\\_loss(y, \\hat{f}) =  \\frac{1}{n_{\\text{samples}}}\n    \\sum_{i=0}^{n_{\\text{samples}} - 1} \\frac{1}{||y_i||_0(n_\\text{labels} - ||y_i||_0)}\n    \\left|\\left\\{(k, l): \\hat{f}_{ik} \\leq \\hat{f}_{il}, y_{ik} = 1, y_{il} = 0\u00a0\\right\\}\\right|\n\nwhere :math:`|\\cdot|` computes the cardinality of the set (i.e., the number of\nelements in the set) and :math:`||\\cdot||_0` is the :math:`\\ell_0` \"norm\"\n(which computes the number of nonzero elements in a vector).\n\nHere is a small example of usage of this function::\n\n    >>> import numpy as np\n    >>> from sklearn.metrics import label_ranking_loss\n    >>> y_true = np.array([[1, 0, 0], [0, 0, 1]])\n    >>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n    >>> label_ranking_loss(y_true, y_score)\n    0.75\n    >>> # With the following prediction, we have perfect and minimal loss\n    >>> y_score = np.array([[1.0, 0.1, 0.2], [0.1, 0.2, 0.9]])\n    >>> label_ranking_loss(y_true, y_score)\n    0.0\n\n\n.. dropdown:: References\n\n  * Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010). Mining multi-label data. In\n    Data mining and knowledge discovery handbook (pp. 667-685). Springer US.\n\n\n.. _ndcg:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_40",
    "header": "Normalized Discounted Cumulative Gain",
    "text": "Normalized Discounted Cumulative Gain\n-------------------------------------\n\nDiscounted Cumulative Gain (DCG) and Normalized Discounted Cumulative Gain\n(NDCG) are ranking metrics implemented in :func:`~sklearn.metrics.dcg_score`\nand :func:`~sklearn.metrics.ndcg_score` ; they compare a predicted order to\nground-truth scores, such as the relevance of answers to a query.\n\nFrom the Wikipedia page for Discounted Cumulative Gain:\n\n\"Discounted cumulative gain (DCG) is a measure of ranking quality. In\ninformation retrieval, it is often used to measure effectiveness of web search\nengine algorithms or related applications. Using a graded relevance scale of\ndocuments in a search-engine result set, DCG measures the usefulness, or gain,\nof a document based on its position in the result list. The gain is accumulated\nfrom the top of the result list to the bottom, with the gain of each result\ndiscounted at lower ranks.\"\n\nDCG orders the true targets (e.g. relevance of query answers) in the predicted\norder, then multiplies them by a logarithmic decay and sums the result. The sum\ncan be truncated after the first :math:`K` results, in which case we call it\nDCG@K.\nNDCG, or NDCG@K is DCG divided by the DCG obtained by a perfect prediction, so\nthat it is always between 0 and 1. Usually, NDCG is preferred to DCG.\n\nCompared with the ranking loss, NDCG can take into account relevance scores,\nrather than a ground-truth ranking. So if the ground-truth consists only of an\nordering, the ranking loss should be preferred; if the ground-truth consists of\nactual usefulness scores (e.g. 0 for irrelevant, 1 for relevant, 2 for very\nrelevant), NDCG can be used.\n\nFor one sample, given the vector of continuous ground-truth values for each\ntarget :math:`y \\in \\mathbb{R}^{M}`, where :math:`M` is the number of outputs, and\nthe prediction :math:`\\hat{y}`, which induces the ranking function :math:`f`, the\nDCG score is\n\n.. math::\n   \\sum_{r=1}^{\\min(K, M)}\\frac{y_{f(r)}}{\\log(1 + r)}\n\nand the NDCG score is the DCG score divided by the DCG score obtained for\n:math:`y`.\n\n.. dropdown:: References\n\n  * `Wikipedia entry for Discounted Cumulative Gain\n    <https://en.wikipedia.org/wiki/Discounted_cumulative_gain>`_\n\n  * Jarvelin, K., & Kekalainen, J. (2002).\n    Cumulated gain-based evaluation of IR techniques. ACM Transactions on\n    Information Systems (TOIS), 20(4), 422-446.\n\n  * Wang, Y., Wang, L., Li, Y., He, D., Chen, W., & Liu, T. Y. (2013, May).\n    A theoretical analysis of NDCG ranking measures. In Proceedings of the 26th\n    Annual Conference on Learning Theory (COLT 2013)\n\n  * McSherry, F., & Najork, M. (2008, March). Computing information retrieval\n    performance measures efficiently in the presence of tied scores. In\n    European conference on information retrieval (pp. 414-421). Springer,\n    Berlin, Heidelberg.\n\n\n.. _regression_metrics:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_41",
    "header": "Regression metrics",
    "text": "Regression metrics\n===================\n\n.. currentmodule:: sklearn.metrics\n\nThe :mod:`sklearn.metrics` module implements several loss, score, and utility\nfunctions to measure regression performance. Some of those have been enhanced\nto handle the multioutput case: :func:`mean_squared_error`,\n:func:`mean_absolute_error`, :func:`r2_score`,\n:func:`explained_variance_score`, :func:`mean_pinball_loss`, :func:`d2_pinball_score`\nand :func:`d2_absolute_error_score`.\n\n\nThese functions have a ``multioutput`` keyword argument which specifies the\nway the scores or losses for each individual target should be averaged. The\ndefault is ``'uniform_average'``, which specifies a uniformly weighted mean\nover outputs. If an ``ndarray`` of shape ``(n_outputs,)`` is passed, then its\nentries are interpreted as weights and an according weighted average is\nreturned. If ``multioutput`` is ``'raw_values'``, then all unaltered\nindividual scores or losses will be returned in an array of shape\n``(n_outputs,)``.\n\n\nThe :func:`r2_score` and :func:`explained_variance_score` accept an additional\nvalue ``'variance_weighted'`` for the ``multioutput`` parameter. This option\nleads to a weighting of each individual score by the variance of the\ncorresponding target variable. This setting quantifies the globally captured\nunscaled variance. If the target variables are of different scale, then this\nscore puts more importance on explaining the higher variance variables.\n\n.. _r2_score:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_42",
    "header": "R\u00b2 score, the coefficient of determination",
    "text": "R\u00b2 score, the coefficient of determination\n-------------------------------------------\n\nThe :func:`r2_score` function computes the `coefficient of\ndetermination <https://en.wikipedia.org/wiki/Coefficient_of_determination>`_,\nusually denoted as :math:`R^2`.\n\nIt represents the proportion of variance (of y) that has been explained by the\nindependent variables in the model. It provides an indication of goodness of\nfit and therefore a measure of how well unseen samples are likely to be\npredicted by the model, through the proportion of explained variance.\n\nAs such variance is dataset dependent, :math:`R^2` may not be meaningfully comparable\nacross different datasets. Best possible score is 1.0 and it can be negative\n(because the model can be arbitrarily worse). A constant model that always\npredicts the expected (average) value of y, disregarding the input features,\nwould get an :math:`R^2` score of 0.0.\n\nNote: when the prediction residuals have zero mean, the :math:`R^2` score and\nthe :ref:`explained_variance_score` are identical.\n\nIf :math:`\\hat{y}_i` is the predicted value of the :math:`i`-th sample\nand :math:`y_i` is the corresponding true value for total :math:`n` samples,\nthe estimated :math:`R^2` is defined as:\n\n.. math::\n\n  R^2(y, \\hat{y}) = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n\nwhere :math:`\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i` and :math:`\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} \\epsilon_i^2`.\n\nNote that :func:`r2_score` calculates unadjusted :math:`R^2` without correcting for\nbias in sample variance of y.\n\nIn the particular case where the true target is constant, the :math:`R^2` score is\nnot finite: it is either ``NaN`` (perfect predictions) or ``-Inf`` (imperfect\npredictions). Such non-finite scores may prevent correct model optimization\nsuch as grid-search cross-validation to be performed correctly. For this reason\nthe default behaviour of :func:`r2_score` is to replace them with 1.0 (perfect\npredictions) or 0.0 (imperfect predictions). If ``force_finite``\nis set to ``False``, this score falls back on the original :math:`R^2` definition.\n\nHere is a small example of usage of the :func:`r2_score` function::\n\n  >>> from sklearn.metrics import r2_score\n  >>> y_true = [3, -0.5, 2, 7]\n  >>> y_pred = [2.5, 0.0, 2, 8]\n  >>> r2_score(y_true, y_pred)\n  0.948\n  >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n  >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n  >>> r2_score(y_true, y_pred, multioutput='variance_weighted')\n  0.938\n  >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n  >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n  >>> r2_score(y_true, y_pred, multioutput='uniform_average')\n  0.936\n  >>> r2_score(y_true, y_pred, multioutput='raw_values')\n  array([0.965, 0.908])\n  >>> r2_score(y_true, y_pred, multioutput=[0.3, 0.7])\n  0.925\n  >>> y_true = [-2, -2, -2]\n  >>> y_pred = [-2, -2, -2]\n  >>> r2_score(y_true, y_pred)\n  1.0\n  >>> r2_score(y_true, y_pred, force_finite=False)\n  nan\n  >>> y_true = [-2, -2, -2]\n  >>> y_pred = [-2, -2, -2 + 1e-8]\n  >>> r2_score(y_true, y_pred)\n  0.0\n  >>> r2_score(y_true, y_pred, force_finite=False)\n  -inf\n\n.. rubric:: Examples\n\n* See :ref:`sphx_glr_auto_examples_linear_model_plot_lasso_and_elasticnet.py`\n  for an example of R\u00b2 score usage to\n  evaluate Lasso and Elastic Net on sparse signals.\n\n.. _mean_absolute_error:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_43",
    "header": "Mean absolute error",
    "text": "Mean absolute error\n-------------------\n\nThe :func:`mean_absolute_error` function computes `mean absolute\nerror <https://en.wikipedia.org/wiki/Mean_absolute_error>`_, a risk\nmetric corresponding to the expected value of the absolute error loss or\n:math:`l1`-norm loss.\n\nIf :math:`\\hat{y}_i` is the predicted value of the :math:`i`-th sample,\nand :math:`y_i` is the corresponding true value, then the mean absolute error\n(MAE) estimated over :math:`n_{\\text{samples}}` is defined as\n\n.. math::\n\n  \\text{MAE}(y, \\hat{y}) = \\frac{1}{n_{\\text{samples}}} \\sum_{i=0}^{n_{\\text{samples}}-1} \\left| y_i - \\hat{y}_i \\right|.\n\nHere is a small example of usage of the :func:`mean_absolute_error` function::\n\n  >>> from sklearn.metrics import mean_absolute_error\n  >>> y_true = [3, -0.5, 2, 7]\n  >>> y_pred = [2.5, 0.0, 2, 8]\n  >>> mean_absolute_error(y_true, y_pred)\n  0.5\n  >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n  >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n  >>> mean_absolute_error(y_true, y_pred)\n  0.75\n  >>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n  array([0.5, 1. ])\n  >>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n  0.85\n\n.. _mean_squared_error:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_44",
    "header": "Mean squared error",
    "text": "Mean squared error\n-------------------\n\nThe :func:`mean_squared_error` function computes `mean squared\nerror <https://en.wikipedia.org/wiki/Mean_squared_error>`_, a risk\nmetric corresponding to the expected value of the squared (quadratic) error or\nloss.\n\nIf :math:`\\hat{y}_i` is the predicted value of the :math:`i`-th sample,\nand :math:`y_i` is the corresponding true value, then the mean squared error\n(MSE) estimated over :math:`n_{\\text{samples}}` is defined as\n\n.. math::\n\n  \\text{MSE}(y, \\hat{y}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples} - 1} (y_i - \\hat{y}_i)^2.\n\nHere is a small example of usage of the :func:`mean_squared_error`\nfunction::\n\n  >>> from sklearn.metrics import mean_squared_error\n  >>> y_true = [3, -0.5, 2, 7]\n  >>> y_pred = [2.5, 0.0, 2, 8]\n  >>> mean_squared_error(y_true, y_pred)\n  0.375\n  >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n  >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n  >>> mean_squared_error(y_true, y_pred)\n  0.7083\n\n.. rubric:: Examples\n\n* See :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_regression.py`\n  for an example of mean squared error usage to evaluate gradient boosting regression.\n\nTaking the square root of the MSE, called the root mean squared error (RMSE), is another\ncommon metric that provides a measure in the same units as the target variable. RMSE is\navailable through the :func:`root_mean_squared_error` function.\n\n.. _mean_squared_log_error:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_45",
    "header": "Mean squared logarithmic error",
    "text": "Mean squared logarithmic error\n------------------------------\n\nThe :func:`mean_squared_log_error` function computes a risk metric\ncorresponding to the expected value of the squared logarithmic (quadratic)\nerror or loss.\n\nIf :math:`\\hat{y}_i` is the predicted value of the :math:`i`-th sample,\nand :math:`y_i` is the corresponding true value, then the mean squared\nlogarithmic error (MSLE) estimated over :math:`n_{\\text{samples}}` is\ndefined as\n\n.. math::\n\n  \\text{MSLE}(y, \\hat{y}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples} - 1} (\\log_e (1 + y_i) - \\log_e (1 + \\hat{y}_i) )^2.\n\nWhere :math:`\\log_e (x)` means the natural logarithm of :math:`x`. This metric\nis best to use when targets having exponential growth, such as population\ncounts, average sales of a commodity over a span of years etc. Note that this\nmetric penalizes an under-predicted estimate greater than an over-predicted\nestimate.\n\nHere is a small example of usage of the :func:`mean_squared_log_error`\nfunction::\n\n  >>> from sklearn.metrics import mean_squared_log_error\n  >>> y_true = [3, 5, 2.5, 7]\n  >>> y_pred = [2.5, 5, 4, 8]\n  >>> mean_squared_log_error(y_true, y_pred)\n  0.0397\n  >>> y_true = [[0.5, 1], [1, 2], [7, 6]]\n  >>> y_pred = [[0.5, 2], [1, 2.5], [8, 8]]\n  >>> mean_squared_log_error(y_true, y_pred)\n  0.044\n\nThe root mean squared logarithmic error (RMSLE) is available through the\n:func:`root_mean_squared_log_error` function.\n\n.. _mean_absolute_percentage_error:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_46",
    "header": "Mean absolute percentage error",
    "text": "Mean absolute percentage error\n------------------------------\nThe :func:`mean_absolute_percentage_error` (MAPE), also known as mean absolute\npercentage deviation (MAPD), is an evaluation metric for regression problems.\nThe idea of this metric is to be sensitive to relative errors. It is for example\nnot changed by a global scaling of the target variable.\n\nIf :math:`\\hat{y}_i` is the predicted value of the :math:`i`-th sample\nand :math:`y_i` is the corresponding true value, then the mean absolute percentage\nerror (MAPE) estimated over :math:`n_{\\text{samples}}` is defined as\n\n.. math::\n\n  \\text{MAPE}(y, \\hat{y}) = \\frac{1}{n_{\\text{samples}}} \\sum_{i=0}^{n_{\\text{samples}}-1} \\frac{{}\\left| y_i - \\hat{y}_i \\right|}{\\max(\\epsilon, \\left| y_i \\right|)}\n\nwhere :math:`\\epsilon` is an arbitrary small yet strictly positive number to\navoid undefined results when y is zero.\n\nThe :func:`mean_absolute_percentage_error` function supports multioutput.\n\nHere is a small example of usage of the :func:`mean_absolute_percentage_error`\nfunction::\n\n  >>> from sklearn.metrics import mean_absolute_percentage_error\n  >>> y_true = [1, 10, 1e6]\n  >>> y_pred = [0.9, 15, 1.2e6]\n  >>> mean_absolute_percentage_error(y_true, y_pred)\n  0.2666\n\nIn above example, if we had used `mean_absolute_error`, it would have ignored\nthe small magnitude values and only reflected the error in prediction of highest\nmagnitude value. But that problem is resolved in case of MAPE because it calculates\nrelative percentage error with respect to actual output.\n\n.. note::\n\n    The MAPE formula here does not represent the common \"percentage\" definition: the\n    percentage in the range [0, 100] is converted to a relative value in the range [0,\n    1] by dividing by 100. Thus, an error of 200% corresponds to a relative error of 2.\n    The motivation here is to have a range of values that is more consistent with other\n    error metrics in scikit-learn, such as `accuracy_score`.\n\n    To obtain the mean absolute percentage error as per the Wikipedia formula,\n    multiply the `mean_absolute_percentage_error` computed here by 100.\n\n.. dropdown:: References\n\n  * `Wikipedia entry for Mean Absolute Percentage Error\n    <https://en.wikipedia.org/wiki/Mean_absolute_percentage_error>`_\n\n.. _median_absolute_error:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_47",
    "header": "Median absolute error",
    "text": "Median absolute error\n---------------------\n\nThe :func:`median_absolute_error` is particularly interesting because it is\nrobust to outliers. The loss is calculated by taking the median of all absolute\ndifferences between the target and the prediction.\n\nIf :math:`\\hat{y}_i` is the predicted value of the :math:`i`-th sample\nand :math:`y_i` is the corresponding true value, then the median absolute error\n(MedAE) estimated over :math:`n_{\\text{samples}}` is defined as\n\n.. math::\n\n  \\text{MedAE}(y, \\hat{y}) = \\text{median}(\\mid y_1 - \\hat{y}_1 \\mid, \\ldots, \\mid y_n - \\hat{y}_n \\mid).\n\nThe :func:`median_absolute_error` does not support multioutput.\n\nHere is a small example of usage of the :func:`median_absolute_error`\nfunction::\n\n  >>> from sklearn.metrics import median_absolute_error\n  >>> y_true = [3, -0.5, 2, 7]\n  >>> y_pred = [2.5, 0.0, 2, 8]\n  >>> median_absolute_error(y_true, y_pred)\n  0.5\n\n\n\n.. _max_error:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_48",
    "header": "Max error",
    "text": "Max error\n-------------------\n\nThe :func:`max_error` function computes the maximum `residual error\n<https://en.wikipedia.org/wiki/Errors_and_residuals>`_ , a metric\nthat captures the worst case error between the predicted value and\nthe true value. In a perfectly fitted single output regression\nmodel, ``max_error`` would be ``0`` on the training set and though this\nwould be highly unlikely in the real world, this metric shows the\nextent of error that the model had when it was fitted.\n\n\nIf :math:`\\hat{y}_i` is the predicted value of the :math:`i`-th sample,\nand :math:`y_i` is the corresponding true value, then the max error is\ndefined as\n\n.. math::\n\n  \\text{Max Error}(y, \\hat{y}) = \\max(| y_i - \\hat{y}_i |)\n\nHere is a small example of usage of the :func:`max_error` function::\n\n  >>> from sklearn.metrics import max_error\n  >>> y_true = [3, 2, 7, 1]\n  >>> y_pred = [9, 2, 7, 1]\n  >>> max_error(y_true, y_pred)\n  6.0\n\nThe :func:`max_error` does not support multioutput.\n\n.. _explained_variance_score:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_49",
    "header": "Explained variance score",
    "text": "Explained variance score\n-------------------------\n\nThe :func:`explained_variance_score` computes the `explained variance\nregression score <https://en.wikipedia.org/wiki/Explained_variation>`_.\n\nIf :math:`\\hat{y}` is the estimated target output, :math:`y` the corresponding\n(correct) target output, and :math:`Var` is `Variance\n<https://en.wikipedia.org/wiki/Variance>`_, the square of the standard deviation,\nthen the explained variance is estimated as follow:\n\n.. math::\n\n  explained\\_{}variance(y, \\hat{y}) = 1 - \\frac{Var\\{ y - \\hat{y}\\}}{Var\\{y\\}}\n\nThe best possible score is 1.0, lower values are worse.\n\n.. topic:: Link to :ref:`r2_score`\n\n    The difference between the explained variance score and the :ref:`r2_score`\n    is that the explained variance score does not account for\n    systematic offset in the prediction. For this reason, the\n    :ref:`r2_score` should be preferred in general.\n\nIn the particular case where the true target is constant, the Explained\nVariance score is not finite: it is either ``NaN`` (perfect predictions) or\n``-Inf`` (imperfect predictions). Such non-finite scores may prevent correct\nmodel optimization such as grid-search cross-validation to be performed\ncorrectly. For this reason the default behaviour of\n:func:`explained_variance_score` is to replace them with 1.0 (perfect\npredictions) or 0.0 (imperfect predictions). You can set the ``force_finite``\nparameter to ``False`` to prevent this fix from happening and fallback on the\noriginal Explained Variance score.\n\nHere is a small example of usage of the :func:`explained_variance_score`\nfunction::\n\n    >>> from sklearn.metrics import explained_variance_score\n    >>> y_true = [3, -0.5, 2, 7]\n    >>> y_pred = [2.5, 0.0, 2, 8]\n    >>> explained_variance_score(y_true, y_pred)\n    0.957\n    >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n    >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n    >>> explained_variance_score(y_true, y_pred, multioutput='raw_values')\n    array([0.967, 1.        ])\n    >>> explained_variance_score(y_true, y_pred, multioutput=[0.3, 0.7])\n    0.990\n    >>> y_true = [-2, -2, -2]\n    >>> y_pred = [-2, -2, -2]\n    >>> explained_variance_score(y_true, y_pred)\n    1.0\n    >>> explained_variance_score(y_true, y_pred, force_finite=False)\n    nan\n    >>> y_true = [-2, -2, -2]\n    >>> y_pred = [-2, -2, -2 + 1e-8]\n    >>> explained_variance_score(y_true, y_pred)\n    0.0\n    >>> explained_variance_score(y_true, y_pred, force_finite=False)\n    -inf\n\n\n.. _mean_tweedie_deviance:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_50",
    "header": "Mean Poisson, Gamma, and Tweedie deviances",
    "text": "Mean Poisson, Gamma, and Tweedie deviances\n------------------------------------------\nThe :func:`mean_tweedie_deviance` function computes the `mean Tweedie\ndeviance error\n<https://en.wikipedia.org/wiki/Tweedie_distribution#The_Tweedie_deviance>`_\nwith a ``power`` parameter (:math:`p`). This is a metric that elicits\npredicted expectation values of regression targets.\n\nFollowing special cases exist,\n\n- when ``power=0`` it is equivalent to :func:`mean_squared_error`.\n- when ``power=1`` it is equivalent to :func:`mean_poisson_deviance`.\n- when ``power=2`` it is equivalent to :func:`mean_gamma_deviance`.\n\nIf :math:`\\hat{y}_i` is the predicted value of the :math:`i`-th sample,\nand :math:`y_i` is the corresponding true value, then the mean Tweedie\ndeviance error (D) for power :math:`p`, estimated over :math:`n_{\\text{samples}}`\nis defined as\n\n.. math::\n\n  \\text{D}(y, \\hat{y}) = \\frac{1}{n_\\text{samples}}\n  \\sum_{i=0}^{n_\\text{samples} - 1}\n  \\begin{cases}\n  (y_i-\\hat{y}_i)^2, & \\text{for }p=0\\text{ (Normal)}\\\\\n  2(y_i \\log(y_i/\\hat{y}_i) + \\hat{y}_i - y_i),  & \\text{for }p=1\\text{ (Poisson)}\\\\\n  2(\\log(\\hat{y}_i/y_i) + y_i/\\hat{y}_i - 1),  & \\text{for }p=2\\text{ (Gamma)}\\\\\n  2\\left(\\frac{\\max(y_i,0)^{2-p}}{(1-p)(2-p)}-\n  \\frac{y_i\\,\\hat{y}_i^{1-p}}{1-p}+\\frac{\\hat{y}_i^{2-p}}{2-p}\\right),\n  & \\text{otherwise}\n  \\end{cases}\n\nTweedie deviance is a homogeneous function of degree ``2-power``.\nThus, Gamma distribution with ``power=2`` means that simultaneously scaling\n``y_true`` and ``y_pred`` has no effect on the deviance. For Poisson\ndistribution ``power=1`` the deviance scales linearly, and for Normal\ndistribution (``power=0``), quadratically.  In general, the higher\n``power`` the less weight is given to extreme deviations between true\nand predicted targets.\n\nFor instance, let's compare the two predictions 1.5 and 150 that are both\n50% larger than their corresponding true value.\n\nThe mean squared error (``power=0``) is very sensitive to the\nprediction difference of the second point,::\n\n    >>> from sklearn.metrics import mean_tweedie_deviance\n    >>> mean_tweedie_deviance([1.0], [1.5], power=0)\n    0.25\n    >>> mean_tweedie_deviance([100.], [150.], power=0)\n    2500.0\n\nIf we increase ``power`` to 1,::\n\n    >>> mean_tweedie_deviance([1.0], [1.5], power=1)\n    0.189\n    >>> mean_tweedie_deviance([100.], [150.], power=1)\n    18.9\n\nthe difference in errors decreases. Finally, by setting, ``power=2``::\n\n    >>> mean_tweedie_deviance([1.0], [1.5], power=2)\n    0.144\n    >>> mean_tweedie_deviance([100.], [150.], power=2)\n    0.144\n\nwe would get identical errors. The deviance when ``power=2`` is thus only\nsensitive to relative errors.\n\n.. _pinball_loss:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_51",
    "header": "Pinball loss",
    "text": "Pinball loss\n------------\n\nThe :func:`mean_pinball_loss` function is used to evaluate the predictive\nperformance of `quantile regression\n<https://en.wikipedia.org/wiki/Quantile_regression>`_ models.\n\n.. math::\n\n  \\text{pinball}(y, \\hat{y}) = \\frac{1}{n_{\\text{samples}}} \\sum_{i=0}^{n_{\\text{samples}}-1}  \\alpha \\max(y_i - \\hat{y}_i, 0) + (1 - \\alpha) \\max(\\hat{y}_i - y_i, 0)\n\nThe value of pinball loss is equivalent to half of :func:`mean_absolute_error` when the quantile\nparameter ``alpha`` is set to 0.5.\n\n\nHere is a small example of usage of the :func:`mean_pinball_loss` function::\n\n  >>> from sklearn.metrics import mean_pinball_loss\n  >>> y_true = [1, 2, 3]\n  >>> mean_pinball_loss(y_true, [0, 2, 3], alpha=0.1)\n  0.033\n  >>> mean_pinball_loss(y_true, [1, 2, 4], alpha=0.1)\n  0.3\n  >>> mean_pinball_loss(y_true, [0, 2, 3], alpha=0.9)\n  0.3\n  >>> mean_pinball_loss(y_true, [1, 2, 4], alpha=0.9)\n  0.033\n  >>> mean_pinball_loss(y_true, y_true, alpha=0.1)\n  0.0\n  >>> mean_pinball_loss(y_true, y_true, alpha=0.9)\n  0.0\n\nIt is possible to build a scorer object with a specific choice of ``alpha``::\n\n  >>> from sklearn.metrics import make_scorer\n  >>> mean_pinball_loss_95p = make_scorer(mean_pinball_loss, alpha=0.95)\n\nSuch a scorer can be used to evaluate the generalization performance of a\nquantile regressor via cross-validation:\n\n  >>> from sklearn.datasets import make_regression\n  >>> from sklearn.model_selection import cross_val_score\n  >>> from sklearn.ensemble import GradientBoostingRegressor\n  >>>\n  >>> X, y = make_regression(n_samples=100, random_state=0)\n  >>> estimator = GradientBoostingRegressor(\n  ...     loss=\"quantile\",\n  ...     alpha=0.95,\n  ...     random_state=0,\n  ... )\n  >>> cross_val_score(estimator, X, y, cv=5, scoring=mean_pinball_loss_95p)\n  array([13.6, 9.7, 23.3, 9.5, 10.4])\n\nIt is also possible to build scorer objects for hyper-parameter tuning. The\nsign of the loss must be switched to ensure that greater means better as\nexplained in the example linked below.\n\n.. rubric:: Examples\n\n* See :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_quantile.py`\n  for an example of using the pinball loss to evaluate and tune the\n  hyper-parameters of quantile regression models on data with non-symmetric\n  noise and outliers.\n\n.. _d2_score:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_52",
    "header": "D\u00b2 score",
    "text": "D\u00b2 score\n--------\n\nThe D\u00b2 score computes the fraction of deviance explained.\nIt is a generalization of R\u00b2, where the squared error is generalized and replaced\nby a deviance of choice :math:`\\text{dev}(y, \\hat{y})`\n(e.g., Tweedie, pinball or mean absolute error). D\u00b2 is a form of a *skill score*.\nIt is calculated as\n\n.. math::\n\n  D^2(y, \\hat{y}) = 1 - \\frac{\\text{dev}(y, \\hat{y})}{\\text{dev}(y, y_{\\text{null}})} \\,.\n\nWhere :math:`y_{\\text{null}}` is the optimal prediction of an intercept-only model\n(e.g., the mean of `y_true` for the Tweedie case, the median for absolute\nerror and the alpha-quantile for pinball loss).\n\nLike R\u00b2, the best possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always predicts\n:math:`y_{\\text{null}}`, disregarding the input features, would get a D\u00b2 score\nof 0.0.\n\n.. dropdown:: D\u00b2 Tweedie score\n\n  The :func:`d2_tweedie_score` function implements the special case of D\u00b2\n  where :math:`\\text{dev}(y, \\hat{y})` is the Tweedie deviance, see :ref:`mean_tweedie_deviance`.\n  It is also known as D\u00b2 Tweedie and is related to McFadden's likelihood ratio index.\n\n  The argument ``power`` defines the Tweedie power as for\n  :func:`mean_tweedie_deviance`. Note that for `power=0`,\n  :func:`d2_tweedie_score` equals :func:`r2_score` (for single targets).\n\n  A scorer object with a specific choice of ``power`` can be built by::\n\n    >>> from sklearn.metrics import d2_tweedie_score, make_scorer\n    >>> d2_tweedie_score_15 = make_scorer(d2_tweedie_score, power=1.5)\n\n.. dropdown:: D\u00b2 pinball score\n\n  The :func:`d2_pinball_score` function implements the special case\n  of D\u00b2 with the pinball loss, see :ref:`pinball_loss`, i.e.:\n\n  .. math::\n\n    \\text{dev}(y, \\hat{y}) = \\text{pinball}(y, \\hat{y}).\n\n  The argument ``alpha`` defines the slope of the pinball loss as for\n  :func:`mean_pinball_loss` (:ref:`pinball_loss`). It determines the\n  quantile level ``alpha`` for which the pinball loss and also D\u00b2\n  are optimal. Note that for `alpha=0.5` (the default) :func:`d2_pinball_score`\n  equals :func:`d2_absolute_error_score`.\n\n  A scorer object with a specific choice of ``alpha`` can be built by::\n\n    >>> from sklearn.metrics import d2_pinball_score, make_scorer\n    >>> d2_pinball_score_08 = make_scorer(d2_pinball_score, alpha=0.8)\n\n.. dropdown:: D\u00b2 absolute error score\n\n  The :func:`d2_absolute_error_score` function implements the special case of\n  the :ref:`mean_absolute_error`:\n\n  .. math::\n\n    \\text{dev}(y, \\hat{y}) = \\text{MAE}(y, \\hat{y}).\n\n  Here are some usage examples of the :func:`d2_absolute_error_score` function::\n\n    >>> from sklearn.metrics import d2_absolute_error_score\n    >>> y_true = [3, -0.5, 2, 7]\n    >>> y_pred = [2.5, 0.0, 2, 8]\n    >>> d2_absolute_error_score(y_true, y_pred)\n    0.764\n    >>> y_true = [1, 2, 3]\n    >>> y_pred = [1, 2, 3]\n    >>> d2_absolute_error_score(y_true, y_pred)\n    1.0\n    >>> y_true = [1, 2, 3]\n    >>> y_pred = [2, 2, 2]\n    >>> d2_absolute_error_score(y_true, y_pred)\n    0.0\n\n\n.. _visualization_regression_evaluation:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_53",
    "header": "Visual evaluation of regression models",
    "text": "Visual evaluation of regression models\n--------------------------------------\n\nAmong methods to assess the quality of regression models, scikit-learn provides\nthe :class:`~sklearn.metrics.PredictionErrorDisplay` class. It allows to\nvisually inspect the prediction errors of a model in two different manners.\n\n.. image:: ../auto_examples/model_selection/images/sphx_glr_plot_cv_predict_001.png\n   :target: ../auto_examples/model_selection/plot_cv_predict.html\n   :scale: 75\n   :align: center\n\nThe plot on the left shows the actual values vs predicted values. For a\nnoise-free regression task aiming to predict the (conditional) expectation of\n`y`, a perfect regression model would display data points on the diagonal\ndefined by predicted equal to actual values. The further away from this optimal\nline, the larger the error of the model. In a more realistic setting with\nirreducible noise, that is, when not all the variations of `y` can be explained\nby features in `X`, then the best model would lead to a cloud of points densely\narranged around the diagonal.\n\nNote that the above only holds when the predicted values is the expected value\nof `y` given `X`. This is typically the case for regression models that\nminimize the mean squared error objective function or more generally the\n:ref:`mean Tweedie deviance <mean_tweedie_deviance>` for any value of its\n\"power\" parameter.\n\nWhen plotting the predictions of an estimator that predicts a quantile\nof `y` given `X`, e.g. :class:`~sklearn.linear_model.QuantileRegressor`\nor any other model minimizing the :ref:`pinball loss <pinball_loss>`, a\nfraction of the points are either expected to lie above or below the diagonal\ndepending on the estimated quantile level.\n\nAll in all, while intuitive to read, this plot does not really inform us on\nwhat to do to obtain a better model.\n\nThe right-hand side plot shows the residuals (i.e. the difference between the\nactual and the predicted values) vs. the predicted values.\n\nThis plot makes it easier to visualize if the residuals follow and\n`homoscedastic or heteroschedastic\n<https://en.wikipedia.org/wiki/Homoscedasticity_and_heteroscedasticity>`_\ndistribution.\n\nIn particular, if the true distribution of `y|X` is Poisson or Gamma\ndistributed, it is expected that the variance of the residuals of the optimal\nmodel would grow with the predicted value of `E[y|X]` (either linearly for\nPoisson or quadratically for Gamma).\n\nWhen fitting a linear least squares regression model (see\n:class:`~sklearn.linear_model.LinearRegression` and\n:class:`~sklearn.linear_model.Ridge`), we can use this plot to check\nif some of the `model assumptions\n<https://en.wikipedia.org/wiki/Ordinary_least_squares#Assumptions>`_\nare met, in particular that the residuals should be uncorrelated, their\nexpected value should be null and that their variance should be constant\n(homoschedasticity).\n\nIf this is not the case, and in particular if the residuals plot show some\nbanana-shaped structure, this is a hint that the model is likely mis-specified\nand that non-linear feature engineering or switching to a non-linear regression\nmodel might be useful.\n\nRefer to the example below to see a model evaluation that makes use of this\ndisplay.\n\n.. rubric:: Examples\n\n* See :ref:`sphx_glr_auto_examples_compose_plot_transformed_target.py` for\n  an example on how to use :class:`~sklearn.metrics.PredictionErrorDisplay`\n  to visualize the prediction quality improvement of a regression model\n  obtained by transforming the target before learning.\n\n.. _clustering_metrics:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_54",
    "header": "Clustering metrics",
    "text": "Clustering metrics\n==================\n\n.. currentmodule:: sklearn.metrics\n\nThe :mod:`sklearn.metrics` module implements several loss, score, and utility\nfunctions to measure clustering performance. For more information see the\n:ref:`clustering_evaluation` section for instance clustering, and\n:ref:`biclustering_evaluation` for biclustering.\n\n.. _dummy_estimators:"
  },
  {
    "filename": "model_evaluation.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\model_evaluation.rst.txt",
    "id": "model_evaluation.rst.txt_chunk_55",
    "header": "Dummy estimators",
    "text": "Dummy estimators\n=================\n\n.. currentmodule:: sklearn.dummy\n\nWhen doing supervised learning, a simple sanity check consists of comparing\none's estimator against simple rules of thumb. :class:`DummyClassifier`\nimplements several such simple strategies for classification:\n\n- ``stratified`` generates random predictions by respecting the training\n  set class distribution.\n- ``most_frequent`` always predicts the most frequent label in the training set.\n- ``prior`` always predicts the class that maximizes the class prior\n  (like ``most_frequent``) and ``predict_proba`` returns the class prior.\n- ``uniform`` generates predictions uniformly at random.\n- ``constant`` always predicts a constant label that is provided by the user.\n   A major motivation of this method is F1-scoring, when the positive class\n   is in the minority.\n\nNote that with all these strategies, the ``predict`` method completely ignores\nthe input data!\n\nTo illustrate :class:`DummyClassifier`, first let's create an imbalanced\ndataset::\n\n  >>> from sklearn.datasets import load_iris\n  >>> from sklearn.model_selection import train_test_split\n  >>> X, y = load_iris(return_X_y=True)\n  >>> y[y != 1] = -1\n  >>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\nNext, let's compare the accuracy of ``SVC`` and ``most_frequent``::\n\n  >>> from sklearn.dummy import DummyClassifier\n  >>> from sklearn.svm import SVC\n  >>> clf = SVC(kernel='linear', C=1).fit(X_train, y_train)\n  >>> clf.score(X_test, y_test)\n  0.63\n  >>> clf = DummyClassifier(strategy='most_frequent', random_state=0)\n  >>> clf.fit(X_train, y_train)\n  DummyClassifier(random_state=0, strategy='most_frequent')\n  >>> clf.score(X_test, y_test)\n  0.579\n\nWe see that ``SVC`` doesn't do much better than a dummy classifier. Now, let's\nchange the kernel::\n\n  >>> clf = SVC(kernel='rbf', C=1).fit(X_train, y_train)\n  >>> clf.score(X_test, y_test)\n  0.94\n\nWe see that the accuracy was boosted to almost 100%.  A cross validation\nstrategy is recommended for a better estimate of the accuracy, if it\nis not too CPU costly. For more information see the :ref:`cross_validation`\nsection. Moreover if you want to optimize over the parameter space, it is highly\nrecommended to use an appropriate methodology; see the :ref:`grid_search`\nsection for details.\n\nMore generally, when the accuracy of a classifier is too close to random, it\nprobably means that something went wrong: features are not helpful, a\nhyperparameter is not correctly tuned, the classifier is suffering from class\nimbalance, etc...\n\n:class:`DummyRegressor` also implements four simple rules of thumb for regression:\n\n- ``mean`` always predicts the mean of the training targets.\n- ``median`` always predicts the median of the training targets.\n- ``quantile`` always predicts a user provided quantile of the training targets.\n- ``constant`` always predicts a constant value that is provided by the user.\n\nIn all these strategies, the ``predict`` method completely ignores\nthe input data."
  },
  {
    "filename": "multiclass.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\multiclass.rst.txt",
    "id": "multiclass.rst.txt_chunk_0",
    "header": "",
    "text": ".. _multiclass:\n\n====================================="
  },
  {
    "filename": "multiclass.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\multiclass.rst.txt",
    "id": "multiclass.rst.txt_chunk_1",
    "header": "Multiclass and multioutput algorithms",
    "text": "Multiclass and multioutput algorithms\n=====================================\n\nThis section of the user guide covers functionality related to multi-learning\nproblems, including :term:`multiclass`, :term:`multilabel`, and\n:term:`multioutput` classification and regression.\n\nThe modules in this section implement :term:`meta-estimators`, which require a\nbase estimator to be provided in their constructor. Meta-estimators extend the\nfunctionality of the base estimator to support multi-learning problems, which\nis accomplished by transforming the multi-learning problem into a set of\nsimpler problems, then fitting one estimator per problem.\n\nThis section covers two modules: :mod:`sklearn.multiclass` and\n:mod:`sklearn.multioutput`. The chart below demonstrates the problem types\nthat each module is responsible for, and the corresponding meta-estimators\nthat each module provides.\n\n.. image:: ../images/multi_org_chart.png\n   :align: center\n\nThe table below provides a quick reference on the differences between problem\ntypes. More detailed explanations can be found in subsequent sections of this\nguide.\n\n+------------------------------+-----------------------+-------------------------+--------------------------------------------------+\n|                              | Number of targets     | Target cardinality      | Valid                                            |\n|                              |                       |                         | :func:`~sklearn.utils.multiclass.type_of_target` |\n+==============================+=======================+=========================+==================================================+\n| Multiclass                   |  1                    | >2                      | 'multiclass'                                     |\n| classification               |                       |                         |                                                  |\n+------------------------------+-----------------------+-------------------------+--------------------------------------------------+\n| Multilabel                   | >1                    |  2 (0 or 1)             | 'multilabel-indicator'                           |\n| classification               |                       |                         |                                                  |\n+------------------------------+-----------------------+-------------------------+--------------------------------------------------+\n| Multiclass-multioutput       | >1                    | >2                      | 'multiclass-multioutput'                         |\n| classification               |                       |                         |                                                  |\n+------------------------------+-----------------------+-------------------------+--------------------------------------------------+\n| Multioutput                  | >1                    | Continuous              | 'continuous-multioutput'                         |\n| regression                   |                       |                         |                                                  |\n+------------------------------+-----------------------+-------------------------+--------------------------------------------------+\n\nBelow is a summary of scikit-learn estimators that have multi-learning support\nbuilt-in, grouped by strategy. You don't need the meta-estimators provided by\nthis section if you're using one of these estimators. However, meta-estimators\ncan provide additional strategies beyond what is built-in:\n\n.. currentmodule:: sklearn\n\n- **Inherently multiclass:**\n\n  - :class:`naive_bayes.BernoulliNB`\n  - :class:`tree.DecisionTreeClassifier`\n  - :class:`tree.ExtraTreeClassifier`\n  - :class:`ensemble.ExtraTreesClassifier`\n  - :class:`naive_bayes.GaussianNB`\n  - :class:`neighbors.KNeighborsClassifier`\n  - :class:`semi_supervised.LabelPropagation`\n  - :class:`semi_supervised.LabelSpreading`\n  - :class:`discriminant_analysis.LinearDiscriminantAnalysis`\n  - :class:`svm.LinearSVC` (setting multi_class=\"crammer_singer\")\n  - :class:`linear_model.LogisticRegression` (with most solvers)\n  - :class:`linear_model.LogisticRegressionCV` (with most solvers)\n  - :class:`neural_network.MLPClassifier`\n  - :class:`neighbors.NearestCentroid`\n  - :class:`discriminant_analysis.QuadraticDiscriminantAnalysis`\n  - :class:`neighbors.RadiusNeighborsClassifier`\n  - :class:`ensemble.RandomForestClassifier`\n  - :class:`linear_model.RidgeClassifier`\n  - :class:`linear_model.RidgeClassifierCV`\n\n\n- **Multiclass as One-Vs-One:**\n\n  - :class:`svm.NuSVC`\n  - :class:`svm.SVC`.\n  - :class:`gaussian_process.GaussianProcessClassifier` (setting multi_class = \"one_vs_one\")\n\n\n- **Multiclass as One-Vs-The-Rest:**\n\n  - :class:`ensemble.GradientBoostingClassifier`\n  - :class:`gaussian_process.GaussianProcessClassifier` (setting multi_class = \"one_vs_rest\")\n  - :class:`svm.LinearSVC` (setting multi_class=\"ovr\")\n  - :class:`linear_model.LogisticRegression` (most solvers)\n  - :class:`linear_model.LogisticRegressionCV` (most solvers)\n  - :class:`linear_model.SGDClassifier`\n  - :class:`linear_model.Perceptron`\n  - :class:`linear_model.PassiveAggressiveClassifier`\n\n\n- **Support multilabel:**\n\n  - :class:`tree.DecisionTreeClassifier`\n  - :class:`tree.ExtraTreeClassifier`\n  - :class:`ensemble.ExtraTreesClassifier`\n  - :class:`neighbors.KNeighborsClassifier`\n  - :class:`neural_network.MLPClassifier`\n  - :class:`neighbors.RadiusNeighborsClassifier`\n  - :class:`ensemble.RandomForestClassifier`\n  - :class:`linear_model.RidgeClassifier`\n  - :class:`linear_model.RidgeClassifierCV`\n\n\n- **Support multiclass-multioutput:**\n\n  - :class:`tree.DecisionTreeClassifier`\n  - :class:`tree.ExtraTreeClassifier`\n  - :class:`ensemble.ExtraTreesClassifier`\n  - :class:`neighbors.KNeighborsClassifier`\n  - :class:`neighbors.RadiusNeighborsClassifier`\n  - :class:`ensemble.RandomForestClassifier`\n\n.. _multiclass_classification:"
  },
  {
    "filename": "multiclass.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\multiclass.rst.txt",
    "id": "multiclass.rst.txt_chunk_2",
    "header": "Multiclass classification",
    "text": "Multiclass classification\n=========================\n\n.. warning::\n    All classifiers in scikit-learn do multiclass classification\n    out-of-the-box. You don't need to use the :mod:`sklearn.multiclass` module\n    unless you want to experiment with different multiclass strategies.\n\n**Multiclass classification** is a classification task with more than two\nclasses. Each sample can only be labeled as one class.\n\nFor example, classification using features extracted from a set of images of\nfruit, where each image may either be of an orange, an apple, or a pear.\nEach image is one sample and is labeled as one of the 3 possible classes.\nMulticlass classification makes the assumption that each sample is assigned\nto one and only one label - one sample cannot, for example, be both a pear\nand an apple.\n\nWhile all scikit-learn classifiers are capable of multiclass classification,\nthe meta-estimators offered by :mod:`sklearn.multiclass`\npermit changing the way they handle more than two classes\nbecause this may have an effect on classifier performance\n(either in terms of generalization error or required computational resources)."
  },
  {
    "filename": "multiclass.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\multiclass.rst.txt",
    "id": "multiclass.rst.txt_chunk_3",
    "header": "Target format",
    "text": "Target format\n-------------\n\nValid :term:`multiclass` representations for\n:func:`~sklearn.utils.multiclass.type_of_target` (`y`) are:\n\n- 1d or column vector containing more than two discrete values. An\n  example of a vector ``y`` for 4 samples:\n\n    >>> import numpy as np\n    >>> y = np.array(['apple', 'pear', 'apple', 'orange'])\n    >>> print(y)\n    ['apple' 'pear' 'apple' 'orange']\n\n- Dense or sparse :term:`binary` matrix of shape ``(n_samples, n_classes)``\n  with a single sample per row, where each column represents one class. An\n  example of both a dense and sparse :term:`binary` matrix ``y`` for 4\n  samples, where the columns, in order, are apple, orange, and pear:\n\n    >>> import numpy as np\n    >>> from sklearn.preprocessing import LabelBinarizer\n    >>> y = np.array(['apple', 'pear', 'apple', 'orange'])\n    >>> y_dense = LabelBinarizer().fit_transform(y)\n    >>> print(y_dense)\n    [[1 0 0]\n     [0 0 1]\n     [1 0 0]\n     [0 1 0]]\n    >>> from scipy import sparse\n    >>> y_sparse = sparse.csr_matrix(y_dense)\n    >>> print(y_sparse)\n    <Compressed Sparse Row sparse matrix of dtype 'int64'\n      with 4 stored elements and shape (4, 3)>\n      Coords Values\n      (0, 0) 1\n      (1, 2) 1\n      (2, 0) 1\n      (3, 1) 1\n\nFor more information about :class:`~sklearn.preprocessing.LabelBinarizer`,\nrefer to :ref:`preprocessing_targets`.\n\n.. _ovr_classification:"
  },
  {
    "filename": "multiclass.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\multiclass.rst.txt",
    "id": "multiclass.rst.txt_chunk_4",
    "header": "OneVsRestClassifier",
    "text": "OneVsRestClassifier\n-------------------\n\nThe **one-vs-rest** strategy, also known as **one-vs-all**, is implemented in\n:class:`~sklearn.multiclass.OneVsRestClassifier`.  The strategy consists in\nfitting one classifier per class. For each classifier, the class is fitted\nagainst all the other classes. In addition to its computational efficiency\n(only `n_classes` classifiers are needed), one advantage of this approach is\nits interpretability. Since each class is represented by one and only one\nclassifier, it is possible to gain knowledge about the class by inspecting its\ncorresponding classifier. This is the most commonly used strategy and is a fair\ndefault choice.\n\nBelow is an example of multiclass learning using OvR::\n\n  >>> from sklearn import datasets\n  >>> from sklearn.multiclass import OneVsRestClassifier\n  >>> from sklearn.svm import LinearSVC\n  >>> X, y = datasets.load_iris(return_X_y=True)\n  >>> OneVsRestClassifier(LinearSVC(random_state=0)).fit(X, y).predict(X)\n  array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2,\n         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n\n\n:class:`~sklearn.multiclass.OneVsRestClassifier` also supports multilabel\nclassification. To use this feature, feed the classifier an indicator matrix,\nin which cell [i, j] indicates the presence of label j in sample i.\n\n\n.. figure:: ../auto_examples/miscellaneous/images/sphx_glr_plot_multilabel_001.png\n    :target: ../auto_examples/miscellaneous/plot_multilabel.html\n    :align: center\n    :scale: 75%\n\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_miscellaneous_plot_multilabel.py`\n* :ref:`sphx_glr_auto_examples_classification_plot_classification_probability.py`\n* :ref:`sphx_glr_auto_examples_linear_model_plot_logistic_multinomial.py`\n\n.. _ovo_classification:"
  },
  {
    "filename": "multiclass.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\multiclass.rst.txt",
    "id": "multiclass.rst.txt_chunk_5",
    "header": "OneVsOneClassifier",
    "text": "OneVsOneClassifier\n------------------\n\n:class:`~sklearn.multiclass.OneVsOneClassifier` constructs one classifier per\npair of classes. At prediction time, the class which received the most votes\nis selected. In the event of a tie (among two classes with an equal number of\nvotes), it selects the class with the highest aggregate classification\nconfidence by summing over the pair-wise classification confidence levels\ncomputed by the underlying binary classifiers.\n\nSince it requires to fit ``n_classes * (n_classes - 1) / 2`` classifiers,\nthis method is usually slower than one-vs-the-rest, due to its\nO(n_classes^2) complexity. However, this method may be advantageous for\nalgorithms such as kernel algorithms which don't scale well with\n``n_samples``. This is because each individual learning problem only involves\na small subset of the data whereas, with one-vs-the-rest, the complete\ndataset is used ``n_classes`` times. The decision function is the result\nof a monotonic transformation of the one-versus-one classification.\n\nBelow is an example of multiclass learning using OvO::\n\n  >>> from sklearn import datasets\n  >>> from sklearn.multiclass import OneVsOneClassifier\n  >>> from sklearn.svm import LinearSVC\n  >>> X, y = datasets.load_iris(return_X_y=True)\n  >>> OneVsOneClassifier(LinearSVC(random_state=0)).fit(X, y).predict(X)\n  array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n\n\n.. rubric:: References\n\n* \"Pattern Recognition and Machine Learning. Springer\",\n  Christopher M. Bishop, page 183, (First Edition)\n\n.. _ecoc:"
  },
  {
    "filename": "multiclass.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\multiclass.rst.txt",
    "id": "multiclass.rst.txt_chunk_6",
    "header": "OutputCodeClassifier",
    "text": "OutputCodeClassifier\n--------------------\n\nError-Correcting Output Code-based strategies are fairly different from\none-vs-the-rest and one-vs-one. With these strategies, each class is\nrepresented in a Euclidean space, where each dimension can only be 0 or 1.\nAnother way to put it is that each class is represented by a binary code (an\narray of 0 and 1). The matrix which keeps track of the location/code of each\nclass is called the code book. The code size is the dimensionality of the\naforementioned space. Intuitively, each class should be represented by a code\nas unique as possible and a good code book should be designed to optimize\nclassification accuracy. In this implementation, we simply use a\nrandomly-generated code book as advocated in [3]_ although more elaborate\nmethods may be added in the future.\n\nAt fitting time, one binary classifier per bit in the code book is fitted.\nAt prediction time, the classifiers are used to project new points in the\nclass space and the class closest to the points is chosen.\n\nIn :class:`~sklearn.multiclass.OutputCodeClassifier`, the ``code_size``\nattribute allows the user to control the number of classifiers which will be\nused. It is a percentage of the total number of classes.\n\nA number between 0 and 1 will require fewer classifiers than\none-vs-the-rest. In theory, ``log2(n_classes) / n_classes`` is sufficient to\nrepresent each class unambiguously. However, in practice, it may not lead to\ngood accuracy since ``log2(n_classes)`` is much smaller than `n_classes`.\n\nA number greater than 1 will require more classifiers than\none-vs-the-rest. In this case, some classifiers will in theory correct for\nthe mistakes made by other classifiers, hence the name \"error-correcting\".\nIn practice, however, this may not happen as classifier mistakes will\ntypically be correlated. The error-correcting output codes have a similar\neffect to bagging.\n\nBelow is an example of multiclass learning using Output-Codes::\n\n  >>> from sklearn import datasets\n  >>> from sklearn.multiclass import OutputCodeClassifier\n  >>> from sklearn.svm import LinearSVC\n  >>> X, y = datasets.load_iris(return_X_y=True)\n  >>> clf = OutputCodeClassifier(LinearSVC(random_state=0), code_size=2, random_state=0)\n  >>> clf.fit(X, y).predict(X)\n  array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,\n         1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n         2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2,\n         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n\n.. rubric:: References\n\n* \"Solving multiclass learning problems via error-correcting output codes\",\n  Dietterich T., Bakiri G., Journal of Artificial Intelligence Research 2, 1995.\n\n.. [3] \"The error coding method and PICTs\", James G., Hastie T.,\n  Journal of Computational and Graphical statistics 7, 1998.\n\n* \"The Elements of Statistical Learning\",\n  Hastie T., Tibshirani R., Friedman J., page 606 (second-edition), 2008.\n\n.. _multilabel_classification:"
  },
  {
    "filename": "multiclass.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\multiclass.rst.txt",
    "id": "multiclass.rst.txt_chunk_7",
    "header": "Multilabel classification",
    "text": "Multilabel classification\n=========================\n\n**Multilabel classification** (closely related to **multioutput**\n**classification**) is a classification task labeling each sample with ``m``\nlabels from ``n_classes`` possible classes, where ``m`` can be 0 to\n``n_classes`` inclusive. This can be thought of as predicting properties of a\nsample that are not mutually exclusive. Formally, a binary output is assigned\nto each class, for every sample. Positive classes are indicated with 1 and\nnegative classes with 0 or -1. It is thus comparable to running ``n_classes``\nbinary classification tasks, for example with\n:class:`~sklearn.multioutput.MultiOutputClassifier`. This approach treats\neach label independently whereas multilabel classifiers *may* treat the\nmultiple classes simultaneously, accounting for correlated behavior among\nthem.\n\nFor example, prediction of the topics relevant to a text document or video.\nThe document or video may be about one of 'religion', 'politics', 'finance'\nor 'education', several of the topic classes or all of the topic classes."
  },
  {
    "filename": "multiclass.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\multiclass.rst.txt",
    "id": "multiclass.rst.txt_chunk_8",
    "header": "Target format",
    "text": "Target format\n-------------\n\nA valid representation of :term:`multilabel` `y` is an either dense or sparse\n:term:`binary` matrix of shape ``(n_samples, n_classes)``. Each column\nrepresents a class. The ``1``'s in each row denote the positive classes a\nsample has been labeled with. An example of a dense matrix ``y`` for 3\nsamples:\n\n  >>> y = np.array([[1, 0, 0, 1], [0, 0, 1, 1], [0, 0, 0, 0]])\n  >>> print(y)\n  [[1 0 0 1]\n   [0 0 1 1]\n   [0 0 0 0]]\n\nDense binary matrices can also be created using\n:class:`~sklearn.preprocessing.MultiLabelBinarizer`. For more information,\nrefer to :ref:`preprocessing_targets`.\n\nAn example of the same ``y`` in sparse matrix form:\n\n  >>> y_sparse = sparse.csr_matrix(y)\n  >>> print(y_sparse)\n  <Compressed Sparse Row sparse matrix of dtype 'int64'\n    with 4 stored elements and shape (3, 4)>\n    Coords Values\n    (0, 0) 1\n    (0, 3) 1\n    (1, 2) 1\n    (1, 3) 1\n\n.. _multioutputclassfier:"
  },
  {
    "filename": "multiclass.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\multiclass.rst.txt",
    "id": "multiclass.rst.txt_chunk_9",
    "header": "MultiOutputClassifier",
    "text": "MultiOutputClassifier\n---------------------\n\nMultilabel classification support can be added to any classifier with\n:class:`~sklearn.multioutput.MultiOutputClassifier`. This strategy consists of\nfitting one classifier per target.  This allows multiple target variable\nclassifications. The purpose of this class is to extend estimators\nto be able to estimate a series of target functions (f1,f2,f3...,fn)\nthat are trained on a single X predictor matrix to predict a series\nof responses (y1,y2,y3...,yn).\n\nYou can find a usage example for\n:class:`~sklearn.multioutput.MultiOutputClassifier`\nas part of the section on :ref:`multiclass_multioutput_classification`\nsince it is a generalization of multilabel classification to\nmulticlass outputs instead of binary outputs.\n\n.. _classifierchain:"
  },
  {
    "filename": "multiclass.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\multiclass.rst.txt",
    "id": "multiclass.rst.txt_chunk_10",
    "header": "ClassifierChain",
    "text": "ClassifierChain\n---------------\n\nClassifier chains (see :class:`~sklearn.multioutput.ClassifierChain`) are a way\nof combining a number of binary classifiers into a single multi-label model\nthat is capable of exploiting correlations among targets.\n\nFor a multi-label classification problem with N classes, N binary\nclassifiers are assigned an integer between 0 and N-1. These integers\ndefine the order of models in the chain. Each classifier is then fit on the\navailable training data plus the true labels of the classes whose\nmodels were assigned a lower number.\n\nWhen predicting, the true labels will not be available. Instead the\npredictions of each model are passed on to the subsequent models in the\nchain to be used as features.\n\nClearly the order of the chain is important. The first model in the chain\nhas no information about the other labels while the last model in the chain\nhas features indicating the presence of all of the other labels. In general\none does not know the optimal ordering of the models in the chain so\ntypically many randomly ordered chains are fit and their predictions are\naveraged together.\n\n.. rubric:: References\n\n* Jesse Read, Bernhard Pfahringer, Geoff Holmes, Eibe Frank,\n  \"Classifier Chains for Multi-label Classification\", 2009.\n\n.. _multiclass_multioutput_classification:"
  },
  {
    "filename": "multiclass.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\multiclass.rst.txt",
    "id": "multiclass.rst.txt_chunk_11",
    "header": "Multiclass-multioutput classification",
    "text": "Multiclass-multioutput classification\n=====================================\n\n**Multiclass-multioutput classification**\n(also known as **multitask classification**) is a\nclassification task which labels each sample with a set of **non-binary**\nproperties. Both the number of properties and the number of\nclasses per property is greater than 2. A single estimator thus\nhandles several joint classification tasks. This is both a generalization of\nthe multi\\ *label* classification task, which only considers binary\nattributes, as well as a generalization of the multi\\ *class* classification\ntask, where only one property is considered.\n\nFor example, classification of the properties \"type of fruit\" and \"colour\"\nfor a set of images of fruit. The property \"type of fruit\" has the possible\nclasses: \"apple\", \"pear\" and \"orange\". The property \"colour\" has the\npossible classes: \"green\", \"red\", \"yellow\" and \"orange\". Each sample is an\nimage of a fruit, a label is output for both properties and each label is\none of the possible classes of the corresponding property.\n\nNote that all classifiers handling multiclass-multioutput (also known as\nmultitask classification) tasks, support the multilabel classification task\nas a special case. Multitask classification is similar to the multioutput\nclassification task with different model formulations. For more information,\nsee the relevant estimator documentation.\n\nBelow is an example of multiclass-multioutput classification:\n\n    >>> from sklearn.datasets import make_classification\n    >>> from sklearn.multioutput import MultiOutputClassifier\n    >>> from sklearn.ensemble import RandomForestClassifier\n    >>> from sklearn.utils import shuffle\n    >>> import numpy as np\n    >>> X, y1 = make_classification(n_samples=10, n_features=100,\n    ...                             n_informative=30, n_classes=3,\n    ...                             random_state=1)\n    >>> y2 = shuffle(y1, random_state=1)\n    >>> y3 = shuffle(y1, random_state=2)\n    >>> Y = np.vstack((y1, y2, y3)).T\n    >>> n_samples, n_features = X.shape # 10,100\n    >>> n_outputs = Y.shape[1] # 3\n    >>> n_classes = 3\n    >>> forest = RandomForestClassifier(random_state=1)\n    >>> multi_target_forest = MultiOutputClassifier(forest, n_jobs=2)\n    >>> multi_target_forest.fit(X, Y).predict(X)\n    array([[2, 2, 0],\n           [1, 2, 1],\n           [2, 1, 0],\n           [0, 0, 2],\n           [0, 2, 1],\n           [0, 0, 2],\n           [1, 1, 0],\n           [1, 1, 1],\n           [0, 0, 2],\n           [2, 0, 0]])\n\n.. warning::\n    At present, no metric in :mod:`sklearn.metrics`\n    supports the multiclass-multioutput classification task."
  },
  {
    "filename": "multiclass.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\multiclass.rst.txt",
    "id": "multiclass.rst.txt_chunk_12",
    "header": "Target format",
    "text": "Target format\n-------------\n\nA valid representation of :term:`multioutput` `y` is a dense matrix of shape\n``(n_samples, n_classes)`` of class labels. A column wise concatenation of 1d\n:term:`multiclass` variables. An example of ``y`` for 3 samples:\n\n  >>> y = np.array([['apple', 'green'], ['orange', 'orange'], ['pear', 'green']])\n  >>> print(y)\n  [['apple' 'green']\n   ['orange' 'orange']\n   ['pear' 'green']]\n\n.. _multioutput_regression:"
  },
  {
    "filename": "multiclass.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\multiclass.rst.txt",
    "id": "multiclass.rst.txt_chunk_13",
    "header": "Multioutput regression",
    "text": "Multioutput regression\n======================\n\n**Multioutput regression** predicts multiple numerical properties for each\nsample. Each property is a numerical variable and the number of properties\nto be predicted for each sample is greater than or equal to 2. Some estimators\nthat support multioutput regression are faster than just running ``n_output``\nestimators.\n\nFor example, prediction of both wind speed and wind direction, in degrees,\nusing data obtained at a certain location. Each sample would be data\nobtained at one location and both wind speed and direction would be\noutput for each sample.\n\nThe following regressors natively support multioutput regression:\n\n- :class:`cross_decomposition.CCA`\n- :class:`tree.DecisionTreeRegressor`\n- :class:`dummy.DummyRegressor`\n- :class:`linear_model.ElasticNet`\n- :class:`tree.ExtraTreeRegressor`\n- :class:`ensemble.ExtraTreesRegressor`\n- :class:`gaussian_process.GaussianProcessRegressor`\n- :class:`neighbors.KNeighborsRegressor`\n- :class:`kernel_ridge.KernelRidge`\n- :class:`linear_model.Lars`\n- :class:`linear_model.Lasso`\n- :class:`linear_model.LassoLars`\n- :class:`linear_model.LinearRegression`\n- :class:`multioutput.MultiOutputRegressor`\n- :class:`linear_model.MultiTaskElasticNet`\n- :class:`linear_model.MultiTaskElasticNetCV`\n- :class:`linear_model.MultiTaskLasso`\n- :class:`linear_model.MultiTaskLassoCV`\n- :class:`linear_model.OrthogonalMatchingPursuit`\n- :class:`cross_decomposition.PLSCanonical`\n- :class:`cross_decomposition.PLSRegression`\n- :class:`linear_model.RANSACRegressor`\n- :class:`neighbors.RadiusNeighborsRegressor`\n- :class:`ensemble.RandomForestRegressor`\n- :class:`multioutput.RegressorChain`\n- :class:`linear_model.Ridge`\n- :class:`linear_model.RidgeCV`\n- :class:`compose.TransformedTargetRegressor`"
  },
  {
    "filename": "multiclass.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\multiclass.rst.txt",
    "id": "multiclass.rst.txt_chunk_14",
    "header": "Target format",
    "text": "Target format\n-------------\n\nA valid representation of :term:`multioutput` `y` is a dense matrix of shape\n``(n_samples, n_output)`` of floats. A column wise concatenation of\n:term:`continuous` variables. An example of ``y`` for 3 samples:\n\n  >>> y = np.array([[31.4, 94], [40.5, 109], [25.0, 30]])\n  >>> print(y)\n  [[ 31.4  94. ]\n   [ 40.5 109. ]\n   [ 25.   30. ]]\n\n.. _multioutputregressor:"
  },
  {
    "filename": "multiclass.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\multiclass.rst.txt",
    "id": "multiclass.rst.txt_chunk_15",
    "header": "MultiOutputRegressor",
    "text": "MultiOutputRegressor\n--------------------\n\nMultioutput regression support can be added to any regressor with\n:class:`~sklearn.multioutput.MultiOutputRegressor`.  This strategy consists of\nfitting one regressor per target. Since each target is represented by exactly\none regressor it is possible to gain knowledge about the target by\ninspecting its corresponding regressor. As\n:class:`~sklearn.multioutput.MultiOutputRegressor` fits one regressor per\ntarget it can not take advantage of correlations between targets.\n\nBelow is an example of multioutput regression:\n\n  >>> from sklearn.datasets import make_regression\n  >>> from sklearn.multioutput import MultiOutputRegressor\n  >>> from sklearn.ensemble import GradientBoostingRegressor\n  >>> X, y = make_regression(n_samples=10, n_targets=3, random_state=1)\n  >>> MultiOutputRegressor(GradientBoostingRegressor(random_state=0)).fit(X, y).predict(X)\n  array([[-154.75474165, -147.03498585,  -50.03812219],\n         [   7.12165031,    5.12914884,  -81.46081961],\n         [-187.8948621 , -100.44373091,   13.88978285],\n         [-141.62745778,   95.02891072, -191.48204257],\n         [  97.03260883,  165.34867495,  139.52003279],\n         [ 123.92529176,   21.25719016,   -7.84253   ],\n         [-122.25193977,  -85.16443186, -107.12274212],\n         [ -30.170388  ,  -94.80956739,   12.16979946],\n         [ 140.72667194,  176.50941682,  -17.50447799],\n         [ 149.37967282,  -81.15699552,   -5.72850319]])\n\n.. _regressorchain:"
  },
  {
    "filename": "multiclass.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\multiclass.rst.txt",
    "id": "multiclass.rst.txt_chunk_16",
    "header": "RegressorChain",
    "text": "RegressorChain\n--------------\n\nRegressor chains (see :class:`~sklearn.multioutput.RegressorChain`) is\nanalogous to :class:`~sklearn.multioutput.ClassifierChain` as a way of\ncombining a number of regressions into a single multi-target model that is\ncapable of exploiting correlations among targets."
  },
  {
    "filename": "naive_bayes.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\naive_bayes.rst.txt",
    "id": "naive_bayes.rst.txt_chunk_0",
    "header": ".. _naive_bayes:",
    "text": ".. _naive_bayes:\n\n==========="
  },
  {
    "filename": "naive_bayes.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\naive_bayes.rst.txt",
    "id": "naive_bayes.rst.txt_chunk_1",
    "header": "Naive Bayes",
    "text": "Naive Bayes\n===========\n\n.. currentmodule:: sklearn.naive_bayes\n\n\nNaive Bayes methods are a set of supervised learning algorithms\nbased on applying Bayes' theorem with the \"naive\" assumption of\nconditional independence between every pair of features given the\nvalue of the class variable. Bayes' theorem states the following\nrelationship, given class variable :math:`y` and dependent feature\nvector :math:`x_1` through :math:`x_n`, :\n\n.. math::\n\n   P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) P(x_1, \\dots, x_n \\mid y)}\n                                    {P(x_1, \\dots, x_n)}\n\nUsing the naive conditional independence assumption that\n\n.. math::\n\n   P(x_i | y, x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_n) = P(x_i | y),\n\nfor all :math:`i`, this relationship is simplified to\n\n.. math::\n\n   P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) \\prod_{i=1}^{n} P(x_i \\mid y)}\n                                    {P(x_1, \\dots, x_n)}\n\nSince :math:`P(x_1, \\dots, x_n)` is constant given the input,\nwe can use the following classification rule:\n\n.. math::\n\n   P(y \\mid x_1, \\dots, x_n) \\propto P(y) \\prod_{i=1}^{n} P(x_i \\mid y)\n\n   \\Downarrow\n\n   \\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n} P(x_i \\mid y),\n\nand we can use Maximum A Posteriori (MAP) estimation to estimate\n:math:`P(y)` and :math:`P(x_i \\mid y)`;\nthe former is then the relative frequency of class :math:`y`\nin the training set.\n\nThe different naive Bayes classifiers differ mainly by the assumptions they\nmake regarding the distribution of :math:`P(x_i \\mid y)`.\n\nIn spite of their apparently over-simplified assumptions, naive Bayes\nclassifiers have worked quite well in many real-world situations, famously\ndocument classification and spam filtering. They require a small amount\nof training data to estimate the necessary parameters. (For theoretical\nreasons why naive Bayes works well, and on which types of data it does, see\nthe references below.)\n\nNaive Bayes learners and classifiers can be extremely fast compared to more\nsophisticated methods.\nThe decoupling of the class conditional feature distributions means that each\ndistribution can be independently estimated as a one dimensional distribution.\nThis in turn helps to alleviate problems stemming from the curse of\ndimensionality.\n\nOn the flip side, although naive Bayes is known as a decent classifier,\nit is known to be a bad estimator, so the probability outputs from\n``predict_proba`` are not to be taken too seriously.\n\n.. dropdown:: References\n\n   * H. Zhang (2004). `The optimality of Naive Bayes.\n     <https://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf>`_\n     Proc. FLAIRS.\n\n.. _gaussian_naive_bayes:"
  },
  {
    "filename": "naive_bayes.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\naive_bayes.rst.txt",
    "id": "naive_bayes.rst.txt_chunk_2",
    "header": "Gaussian Naive Bayes",
    "text": "Gaussian Naive Bayes\n--------------------\n\n:class:`GaussianNB` implements the Gaussian Naive Bayes algorithm for\nclassification. The likelihood of the features is assumed to be Gaussian:\n\n.. math::\n\n   P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}\\right)\n\nThe parameters :math:`\\sigma_y` and :math:`\\mu_y`\nare estimated using maximum likelihood.\n\n   >>> from sklearn.datasets import load_iris\n   >>> from sklearn.model_selection import train_test_split\n   >>> from sklearn.naive_bayes import GaussianNB\n   >>> X, y = load_iris(return_X_y=True)\n   >>> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n   >>> gnb = GaussianNB()\n   >>> y_pred = gnb.fit(X_train, y_train).predict(X_test)\n   >>> print(\"Number of mislabeled points out of a total %d points : %d\"\n   ...       % (X_test.shape[0], (y_test != y_pred).sum()))\n   Number of mislabeled points out of a total 75 points : 4\n\n.. _multinomial_naive_bayes:"
  },
  {
    "filename": "naive_bayes.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\naive_bayes.rst.txt",
    "id": "naive_bayes.rst.txt_chunk_3",
    "header": "Multinomial Naive Bayes",
    "text": "Multinomial Naive Bayes\n-----------------------\n\n:class:`MultinomialNB` implements the naive Bayes algorithm for multinomially\ndistributed data, and is one of the two classic naive Bayes variants used in\ntext classification (where the data are typically represented as word vector\ncounts, although tf-idf vectors are also known to work well in practice).\nThe distribution is parametrized by vectors\n:math:`\\theta_y = (\\theta_{y1},\\ldots,\\theta_{yn})`\nfor each class :math:`y`, where :math:`n` is the number of features\n(in text classification, the size of the vocabulary)\nand :math:`\\theta_{yi}` is the probability :math:`P(x_i \\mid y)`\nof feature :math:`i` appearing in a sample belonging to class :math:`y`.\n\nThe parameters :math:`\\theta_y` are estimated by a smoothed\nversion of maximum likelihood, i.e. relative frequency counting:\n\n.. math::\n\n    \\hat{\\theta}_{yi} = \\frac{ N_{yi} + \\alpha}{N_y + \\alpha n}\n\nwhere :math:`N_{yi} = \\sum_{x \\in T} x_i` is\nthe number of times feature :math:`i` appears in all samples of class :math:`y`\nin the training set :math:`T`,\nand :math:`N_{y} = \\sum_{i=1}^{n} N_{yi}` is the total count of\nall features for class :math:`y`.\n\nThe smoothing priors :math:`\\alpha \\ge 0` account for\nfeatures not present in the learning samples and prevent zero probabilities\nin further computations.\nSetting :math:`\\alpha = 1` is called Laplace smoothing,\nwhile :math:`\\alpha < 1` is called Lidstone smoothing.\n\n.. _complement_naive_bayes:"
  },
  {
    "filename": "naive_bayes.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\naive_bayes.rst.txt",
    "id": "naive_bayes.rst.txt_chunk_4",
    "header": "Complement Naive Bayes",
    "text": "Complement Naive Bayes\n----------------------\n\n:class:`ComplementNB` implements the complement naive Bayes (CNB) algorithm.\nCNB is an adaptation of the standard multinomial naive Bayes (MNB) algorithm\nthat is particularly suited for imbalanced data sets. Specifically, CNB uses\nstatistics from the *complement* of each class to compute the model's weights.\nThe inventors of CNB show empirically that the parameter estimates for CNB are\nmore stable than those for MNB. Further, CNB regularly outperforms MNB (often\nby a considerable margin) on text classification tasks.\n\n.. dropdown:: Weights calculation\n\n   The procedure for calculating the weights is as follows:\n\n   .. math::\n\n      \\hat{\\theta}_{ci} = \\frac{\\alpha_i + \\sum_{j:y_j \\neq c} d_{ij}}\n                              {\\alpha + \\sum_{j:y_j \\neq c} \\sum_{k} d_{kj}}\n\n      w_{ci} = \\log \\hat{\\theta}_{ci}\n\n      w_{ci} = \\frac{w_{ci}}{\\sum_{j} |w_{cj}|}\n\n   where the summations are over all documents :math:`j` not in class :math:`c`,\n   :math:`d_{ij}` is either the count or tf-idf value of term :math:`i` in document\n   :math:`j`, :math:`\\alpha_i` is a smoothing hyperparameter like that found in\n   MNB, and :math:`\\alpha = \\sum_{i} \\alpha_i`. The second normalization addresses\n   the tendency for longer documents to dominate parameter estimates in MNB. The\n   classification rule is:\n\n   .. math::\n\n      \\hat{c} = \\arg\\min_c \\sum_{i} t_i w_{ci}\n\n   i.e., a document is assigned to the class that is the *poorest* complement\n   match.\n\n.. dropdown:: References\n\n   * Rennie, J. D., Shih, L., Teevan, J., & Karger, D. R. (2003).\n     `Tackling the poor assumptions of naive bayes text classifiers.\n     <https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf>`_\n     In ICML (Vol. 3, pp. 616-623).\n\n\n.. _bernoulli_naive_bayes:"
  },
  {
    "filename": "naive_bayes.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\naive_bayes.rst.txt",
    "id": "naive_bayes.rst.txt_chunk_5",
    "header": "Bernoulli Naive Bayes",
    "text": "Bernoulli Naive Bayes\n---------------------\n\n:class:`BernoulliNB` implements the naive Bayes training and classification\nalgorithms for data that is distributed according to multivariate Bernoulli\ndistributions; i.e., there may be multiple features but each one is assumed\nto be a binary-valued (Bernoulli, boolean) variable.\nTherefore, this class requires samples to be represented as binary-valued\nfeature vectors; if handed any other kind of data, a :class:`BernoulliNB` instance\nmay binarize its input (depending on the ``binarize`` parameter).\n\nThe decision rule for Bernoulli naive Bayes is based on\n\n.. math::\n\n    P(x_i \\mid y) = P(x_i = 1 \\mid y) x_i + (1 - P(x_i = 1 \\mid y)) (1 - x_i)\n\nwhich differs from multinomial NB's rule\nin that it explicitly penalizes the non-occurrence of a feature :math:`i`\nthat is an indicator for class :math:`y`,\nwhere the multinomial variant would simply ignore a non-occurring feature.\n\nIn the case of text classification, word occurrence vectors (rather than word\ncount vectors) may be used to train and use this classifier. :class:`BernoulliNB`\nmight perform better on some datasets, especially those with shorter documents.\nIt is advisable to evaluate both models, if time permits.\n\n.. dropdown:: References\n\n   * C.D. Manning, P. Raghavan and H. Sch\u00fctze (2008). Introduction to\n     Information Retrieval. Cambridge University Press, pp. 234-265.\n\n   * A. McCallum and K. Nigam (1998).\n     `A comparison of event models for Naive Bayes text classification.\n     <https://citeseerx.ist.psu.edu/doc_view/pid/04ce064505b1635583fa0d9cc07cac7e9ea993cc>`_\n     Proc. AAAI/ICML-98 Workshop on Learning for Text Categorization, pp. 41-48.\n\n   * V. Metsis, I. Androutsopoulos and G. Paliouras (2006).\n     `Spam filtering with Naive Bayes -- Which Naive Bayes?\n     <https://citeseerx.ist.psu.edu/doc_view/pid/8bd0934b366b539ec95e683ae39f8abb29ccc757>`_\n     3rd Conf. on Email and Anti-Spam (CEAS).\n\n\n.. _categorical_naive_bayes:"
  },
  {
    "filename": "naive_bayes.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\naive_bayes.rst.txt",
    "id": "naive_bayes.rst.txt_chunk_6",
    "header": "Categorical Naive Bayes",
    "text": "Categorical Naive Bayes\n-----------------------\n\n:class:`CategoricalNB` implements the categorical naive Bayes\nalgorithm for categorically distributed data. It assumes that each feature,\nwhich is described by the index :math:`i`, has its own categorical\ndistribution.\n\nFor each feature :math:`i` in the training set :math:`X`,\n:class:`CategoricalNB` estimates a categorical distribution for each feature i\nof X conditioned on the class y. The index set of the samples is defined as\n:math:`J = \\{ 1, \\dots, m \\}`, with :math:`m` as the number of samples.\n\n.. dropdown:: Probability calculation\n\n   The probability of category :math:`t` in feature :math:`i` given class\n   :math:`c` is estimated as:\n\n   .. math::\n\n      P(x_i = t \\mid y = c \\: ;\\, \\alpha) = \\frac{ N_{tic} + \\alpha}{N_{c} +\n                                             \\alpha n_i},\n\n   where :math:`N_{tic} = |\\{j \\in J \\mid x_{ij} = t, y_j = c\\}|` is the number\n   of times category :math:`t` appears in the samples :math:`x_{i}`, which belong\n   to class :math:`c`, :math:`N_{c} = |\\{ j \\in J\\mid y_j = c\\}|` is the number\n   of samples with class c, :math:`\\alpha` is a smoothing parameter and\n   :math:`n_i` is the number of available categories of feature :math:`i`.\n\n:class:`CategoricalNB` assumes that the sample matrix :math:`X` is encoded (for\ninstance with the help of :class:`~sklearn.preprocessing.OrdinalEncoder`) such\nthat all categories for each feature :math:`i` are represented with numbers\n:math:`0, ..., n_i - 1` where :math:`n_i` is the number of available categories\nof feature :math:`i`."
  },
  {
    "filename": "naive_bayes.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\naive_bayes.rst.txt",
    "id": "naive_bayes.rst.txt_chunk_7",
    "header": "Out-of-core naive Bayes model fitting",
    "text": "Out-of-core naive Bayes model fitting\n-------------------------------------\n\nNaive Bayes models can be used to tackle large scale classification problems\nfor which the full training set might not fit in memory. To handle this case,\n:class:`MultinomialNB`, :class:`BernoulliNB`, and :class:`GaussianNB`\nexpose a ``partial_fit`` method that can be used\nincrementally as done with other classifiers as demonstrated in\n:ref:`sphx_glr_auto_examples_applications_plot_out_of_core_classification.py`. All naive Bayes\nclassifiers support sample weighting.\n\nContrary to the ``fit`` method, the first call to ``partial_fit`` needs to be\npassed the list of all the expected class labels.\n\nFor an overview of available strategies in scikit-learn, see also the\n:ref:`out-of-core learning <scaling_strategies>` documentation.\n\n.. note::\n\n   The ``partial_fit`` method call of naive Bayes models introduces some\n   computational overhead. It is recommended to use data chunk sizes that are as\n   large as possible, that is as the available RAM allows."
  },
  {
    "filename": "neighbors.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neighbors.rst.txt",
    "id": "neighbors.rst.txt_chunk_0",
    "header": ".. _neighbors:",
    "text": ".. _neighbors:\n\n================="
  },
  {
    "filename": "neighbors.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neighbors.rst.txt",
    "id": "neighbors.rst.txt_chunk_1",
    "header": "Nearest Neighbors",
    "text": "Nearest Neighbors\n=================\n\n.. sectionauthor:: Jake Vanderplas <vanderplas@astro.washington.edu>\n\n.. currentmodule:: sklearn.neighbors\n\n:mod:`sklearn.neighbors` provides functionality for unsupervised and\nsupervised neighbors-based learning methods.  Unsupervised nearest neighbors\nis the foundation of many other learning methods,\nnotably manifold learning and spectral clustering.  Supervised neighbors-based\nlearning comes in two flavors: `classification`_ for data with\ndiscrete labels, and `regression`_ for data with continuous labels.\n\nThe principle behind nearest neighbor methods is to find a predefined number\nof training samples closest in distance to the new point, and\npredict the label from these.  The number of samples can be a user-defined\nconstant (k-nearest neighbor learning), or vary based\non the local density of points (radius-based neighbor learning).\nThe distance can, in general, be any metric measure: standard Euclidean\ndistance is the most common choice.\nNeighbors-based methods are known as *non-generalizing* machine\nlearning methods, since they simply \"remember\" all of its training data\n(possibly transformed into a fast indexing structure such as a\n:ref:`Ball Tree <ball_tree>` or :ref:`KD Tree <kd_tree>`).\n\nDespite its simplicity, nearest neighbors has been successful in a\nlarge number of classification and regression problems, including\nhandwritten digits and satellite image scenes. Being a non-parametric method,\nit is often successful in classification situations where the decision\nboundary is very irregular.\n\nThe classes in :mod:`sklearn.neighbors` can handle either NumPy arrays or\n`scipy.sparse` matrices as input.  For dense matrices, a large number of\npossible distance metrics are supported.  For sparse matrices, arbitrary\nMinkowski metrics are supported for searches.\n\nThere are many learning routines which rely on nearest neighbors at their\ncore.  One example is :ref:`kernel density estimation <kernel_density>`,\ndiscussed in the :ref:`density estimation <density_estimation>` section.\n\n\n.. _unsupervised_neighbors:"
  },
  {
    "filename": "neighbors.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neighbors.rst.txt",
    "id": "neighbors.rst.txt_chunk_2",
    "header": "Unsupervised Nearest Neighbors",
    "text": "Unsupervised Nearest Neighbors\n==============================\n\n:class:`NearestNeighbors` implements unsupervised nearest neighbors learning.\nIt acts as a uniform interface to three different nearest neighbors\nalgorithms: :class:`BallTree`, :class:`KDTree`, and a\nbrute-force algorithm based on routines in :mod:`sklearn.metrics.pairwise`.\nThe choice of neighbors search algorithm is controlled through the keyword\n``'algorithm'``, which must be one of\n``['auto', 'ball_tree', 'kd_tree', 'brute']``.  When the default value\n``'auto'`` is passed, the algorithm attempts to determine the best approach\nfrom the training data.  For a discussion of the strengths and weaknesses\nof each option, see `Nearest Neighbor Algorithms`_.\n\n.. warning::\n\n    Regarding the Nearest Neighbors algorithms, if two\n    neighbors :math:`k+1` and :math:`k` have identical distances\n    but different labels, the result will depend on the ordering of the\n    training data."
  },
  {
    "filename": "neighbors.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neighbors.rst.txt",
    "id": "neighbors.rst.txt_chunk_3",
    "header": "Finding the Nearest Neighbors",
    "text": "Finding the Nearest Neighbors\n-----------------------------\nFor the simple task of finding the nearest neighbors between two sets of\ndata, the unsupervised algorithms within :mod:`sklearn.neighbors` can be\nused:\n\n    >>> from sklearn.neighbors import NearestNeighbors\n    >>> import numpy as np\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    >>> nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X)\n    >>> distances, indices = nbrs.kneighbors(X)\n    >>> indices\n    array([[0, 1],\n           [1, 0],\n           [2, 1],\n           [3, 4],\n           [4, 3],\n           [5, 4]]...)\n    >>> distances\n    array([[0.        , 1.        ],\n           [0.        , 1.        ],\n           [0.        , 1.41421356],\n           [0.        , 1.        ],\n           [0.        , 1.        ],\n           [0.        , 1.41421356]])\n\nBecause the query set matches the training set, the nearest neighbor of each\npoint is the point itself, at a distance of zero.\n\nIt is also possible to efficiently produce a sparse graph showing the\nconnections between neighboring points:\n\n    >>> nbrs.kneighbors_graph(X).toarray()\n    array([[1., 1., 0., 0., 0., 0.],\n           [1., 1., 0., 0., 0., 0.],\n           [0., 1., 1., 0., 0., 0.],\n           [0., 0., 0., 1., 1., 0.],\n           [0., 0., 0., 1., 1., 0.],\n           [0., 0., 0., 0., 1., 1.]])\n\nThe dataset is structured such that points nearby in index order are nearby\nin parameter space, leading to an approximately block-diagonal matrix of\nK-nearest neighbors.  Such a sparse graph is useful in a variety of\ncircumstances which make use of spatial relationships between points for\nunsupervised learning: in particular, see :class:`~sklearn.manifold.Isomap`,\n:class:`~sklearn.manifold.LocallyLinearEmbedding`, and\n:class:`~sklearn.cluster.SpectralClustering`."
  },
  {
    "filename": "neighbors.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neighbors.rst.txt",
    "id": "neighbors.rst.txt_chunk_4",
    "header": "KDTree and BallTree Classes",
    "text": "KDTree and BallTree Classes\n---------------------------\nAlternatively, one can use the :class:`KDTree` or :class:`BallTree` classes\ndirectly to find nearest neighbors.  This is the functionality wrapped by\nthe :class:`NearestNeighbors` class used above.  The Ball Tree and KD Tree\nhave the same interface; we'll show an example of using the KD Tree here:\n\n    >>> from sklearn.neighbors import KDTree\n    >>> import numpy as np\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    >>> kdt = KDTree(X, leaf_size=30, metric='euclidean')\n    >>> kdt.query(X, k=2, return_distance=False)\n    array([[0, 1],\n           [1, 0],\n           [2, 1],\n           [3, 4],\n           [4, 3],\n           [5, 4]]...)\n\nRefer to the :class:`KDTree` and :class:`BallTree` class documentation\nfor more information on the options available for nearest neighbors searches,\nincluding specification of query strategies, distance metrics, etc. For a list\nof valid metrics use `KDTree.valid_metrics` and `BallTree.valid_metrics`:\n\n    >>> from sklearn.neighbors import KDTree, BallTree\n    >>> KDTree.valid_metrics\n    ['euclidean', 'l2', 'minkowski', 'p', 'manhattan', 'cityblock', 'l1', 'chebyshev', 'infinity']\n    >>> BallTree.valid_metrics\n    ['euclidean', 'l2', 'minkowski', 'p', 'manhattan', 'cityblock', 'l1', 'chebyshev', 'infinity', 'seuclidean', 'mahalanobis', 'hamming', 'canberra', 'braycurtis', 'jaccard', 'dice', 'rogerstanimoto', 'russellrao', 'sokalmichener', 'sokalsneath', 'haversine', 'pyfunc']\n\n.. _classification:"
  },
  {
    "filename": "neighbors.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neighbors.rst.txt",
    "id": "neighbors.rst.txt_chunk_5",
    "header": "Nearest Neighbors Classification",
    "text": "Nearest Neighbors Classification\n================================\n\nNeighbors-based classification is a type of *instance-based learning* or\n*non-generalizing learning*: it does not attempt to construct a general\ninternal model, but simply stores instances of the training data.\nClassification is computed from a simple majority vote of the nearest\nneighbors of each point: a query point is assigned the data class which\nhas the most representatives within the nearest neighbors of the point.\n\nscikit-learn implements two different nearest neighbors classifiers:\n:class:`KNeighborsClassifier` implements learning based on the :math:`k`\nnearest neighbors of each query point, where :math:`k` is an integer value\nspecified by the user.  :class:`RadiusNeighborsClassifier` implements learning\nbased on the number of neighbors within a fixed radius :math:`r` of each\ntraining point, where :math:`r` is a floating-point value specified by\nthe user.\n\nThe :math:`k`-neighbors classification in :class:`KNeighborsClassifier`\nis the most commonly used technique. The optimal choice of the value :math:`k`\nis highly data-dependent: in general a larger :math:`k` suppresses the effects\nof noise, but makes the classification boundaries less distinct.\n\nIn cases where the data is not uniformly sampled, radius-based neighbors\nclassification in :class:`RadiusNeighborsClassifier` can be a better choice.\nThe user specifies a fixed radius :math:`r`, such that points in sparser\nneighborhoods use fewer nearest neighbors for the classification.  For\nhigh-dimensional parameter spaces, this method becomes less effective due\nto the so-called \"curse of dimensionality\".\n\nThe basic nearest neighbors classification uses uniform weights: that is, the\nvalue assigned to a query point is computed from a simple majority vote of\nthe nearest neighbors.  Under some circumstances, it is better to weight the\nneighbors such that nearer neighbors contribute more to the fit.  This can\nbe accomplished through the ``weights`` keyword.  The default value,\n``weights = 'uniform'``, assigns uniform weights to each neighbor.\n``weights = 'distance'`` assigns weights proportional to the inverse of the\ndistance from the query point.  Alternatively, a user-defined function of the\ndistance can be supplied to compute the weights.\n\n.. |classification_1| image:: ../auto_examples/neighbors/images/sphx_glr_plot_classification_001.png\n   :target: ../auto_examples/neighbors/plot_classification.html\n   :scale: 75\n\n.. centered:: |classification_1|\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_neighbors_plot_classification.py`: an example of\n  classification using nearest neighbors.\n\n.. _regression:"
  },
  {
    "filename": "neighbors.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neighbors.rst.txt",
    "id": "neighbors.rst.txt_chunk_6",
    "header": "Nearest Neighbors Regression",
    "text": "Nearest Neighbors Regression\n============================\n\nNeighbors-based regression can be used in cases where the data labels are\ncontinuous rather than discrete variables.  The label assigned to a query\npoint is computed based on the mean of the labels of its nearest neighbors.\n\nscikit-learn implements two different neighbors regressors:\n:class:`KNeighborsRegressor` implements learning based on the :math:`k`\nnearest neighbors of each query point, where :math:`k` is an integer\nvalue specified by the user.  :class:`RadiusNeighborsRegressor` implements\nlearning based on the neighbors within a fixed radius :math:`r` of the\nquery point, where :math:`r` is a floating-point value specified by the\nuser.\n\nThe basic nearest neighbors regression uses uniform weights: that is,\neach point in the local neighborhood contributes uniformly to the\nclassification of a query point.  Under some circumstances, it can be\nadvantageous to weight points such that nearby points contribute more\nto the regression than faraway points.  This can be accomplished through\nthe ``weights`` keyword.  The default value, ``weights = 'uniform'``,\nassigns equal weights to all points.  ``weights = 'distance'`` assigns\nweights proportional to the inverse of the distance from the query point.\nAlternatively, a user-defined function of the distance can be supplied,\nwhich will be used to compute the weights.\n\n.. figure:: ../auto_examples/neighbors/images/sphx_glr_plot_regression_001.png\n   :target: ../auto_examples/neighbors/plot_regression.html\n   :align: center\n   :scale: 75\n\nThe use of multi-output nearest neighbors for regression is demonstrated in\n:ref:`sphx_glr_auto_examples_miscellaneous_plot_multioutput_face_completion.py`. In this example, the inputs\nX are the pixels of the upper half of faces and the outputs Y are the pixels of\nthe lower half of those faces.\n\n.. figure:: ../auto_examples/miscellaneous/images/sphx_glr_plot_multioutput_face_completion_001.png\n   :target: ../auto_examples/miscellaneous/plot_multioutput_face_completion.html\n   :scale: 75\n   :align: center\n\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_neighbors_plot_regression.py`: an example of regression\n  using nearest neighbors.\n\n* :ref:`sphx_glr_auto_examples_miscellaneous_plot_multioutput_face_completion.py`:\n  an example of multi-output regression using nearest neighbors."
  },
  {
    "filename": "neighbors.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neighbors.rst.txt",
    "id": "neighbors.rst.txt_chunk_7",
    "header": "Nearest Neighbor Algorithms",
    "text": "Nearest Neighbor Algorithms\n===========================\n\n.. _brute_force:"
  },
  {
    "filename": "neighbors.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neighbors.rst.txt",
    "id": "neighbors.rst.txt_chunk_8",
    "header": "Brute Force",
    "text": "Brute Force\n-----------\n\nFast computation of nearest neighbors is an active area of research in\nmachine learning. The most naive neighbor search implementation involves\nthe brute-force computation of distances between all pairs of points in the\ndataset: for :math:`N` samples in :math:`D` dimensions, this approach scales\nas :math:`O[D N^2]`.  Efficient brute-force neighbors searches can be very\ncompetitive for small data samples.\nHowever, as the number of samples :math:`N` grows, the brute-force\napproach quickly becomes infeasible.  In the classes within\n:mod:`sklearn.neighbors`, brute-force neighbors searches are specified\nusing the keyword ``algorithm = 'brute'``, and are computed using the\nroutines available in :mod:`sklearn.metrics.pairwise`.\n\n.. _kd_tree:"
  },
  {
    "filename": "neighbors.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neighbors.rst.txt",
    "id": "neighbors.rst.txt_chunk_9",
    "header": "K-D Tree",
    "text": "K-D Tree\n--------\n\nTo address the computational inefficiencies of the brute-force approach, a\nvariety of tree-based data structures have been invented.  In general, these\nstructures attempt to reduce the required number of distance calculations\nby efficiently encoding aggregate distance information for the sample.\nThe basic idea is that if point :math:`A` is very distant from point\n:math:`B`, and point :math:`B` is very close to point :math:`C`,\nthen we know that points :math:`A` and :math:`C`\nare very distant, *without having to explicitly calculate their distance*.\nIn this way, the computational cost of a nearest neighbors search can be\nreduced to :math:`O[D N \\log(N)]` or better. This is a significant\nimprovement over brute-force for large :math:`N`.\n\nAn early approach to taking advantage of this aggregate information was\nthe *KD tree* data structure (short for *K-dimensional tree*), which\ngeneralizes two-dimensional *Quad-trees* and 3-dimensional *Oct-trees*\nto an arbitrary number of dimensions.  The KD tree is a binary tree\nstructure which recursively partitions the parameter space along the data\naxes, dividing it into nested orthotropic regions into which data points\nare filed.  The construction of a KD tree is very fast: because partitioning\nis performed only along the data axes, no :math:`D`-dimensional distances\nneed to be computed. Once constructed, the nearest neighbor of a query\npoint can be determined with only :math:`O[\\log(N)]` distance computations.\nThough the KD tree approach is very fast for low-dimensional (:math:`D < 20`)\nneighbors searches, it becomes inefficient as :math:`D` grows very large:\nthis is one manifestation of the so-called \"curse of dimensionality\".\nIn scikit-learn, KD tree neighbors searches are specified using the\nkeyword ``algorithm = 'kd_tree'``, and are computed using the class\n:class:`KDTree`.\n\n\n.. dropdown:: References\n\n  * `\"Multidimensional binary search trees used for associative searching\"\n    <https://dl.acm.org/citation.cfm?doid=361002.361007>`_,\n    Bentley, J.L., Communications of the ACM (1975)\n\n\n.. _ball_tree:"
  },
  {
    "filename": "neighbors.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neighbors.rst.txt",
    "id": "neighbors.rst.txt_chunk_10",
    "header": "Ball Tree",
    "text": "Ball Tree\n---------\n\nTo address the inefficiencies of KD Trees in higher dimensions, the *ball tree*\ndata structure was developed.  Where KD trees partition data along\nCartesian axes, ball trees partition data in a series of nesting\nhyper-spheres.  This makes tree construction more costly than that of the\nKD tree, but results in a data structure which can be very efficient on\nhighly structured data, even in very high dimensions.\n\nA ball tree recursively divides the data into\nnodes defined by a centroid :math:`C` and radius :math:`r`, such that each\npoint in the node lies within the hyper-sphere defined by :math:`r` and\n:math:`C`. The number of candidate points for a neighbor search\nis reduced through use of the *triangle inequality*:\n\n.. math::   |x+y| \\leq |x| + |y|\n\nWith this setup, a single distance calculation between a test point and\nthe centroid is sufficient to determine a lower and upper bound on the\ndistance to all points within the node.\nBecause of the spherical geometry of the ball tree nodes, it can out-perform\na *KD-tree* in high dimensions, though the actual performance is highly\ndependent on the structure of the training data.\nIn scikit-learn, ball-tree-based\nneighbors searches are specified using the keyword ``algorithm = 'ball_tree'``,\nand are computed using the class :class:`BallTree`.\nAlternatively, the user can work with the :class:`BallTree` class directly.\n\n\n.. dropdown:: References\n\n  * `\"Five Balltree Construction Algorithms\"\n    <https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=17ac002939f8e950ffb32ec4dc8e86bdd8cb5ff1>`_,\n    Omohundro, S.M., International Computer Science Institute\n    Technical Report (1989)\n\n.. dropdown:: Choice of Nearest Neighbors Algorithm\n\n  The optimal algorithm for a given dataset is a complicated choice, and\n  depends on a number of factors:\n\n  * number of samples :math:`N` (i.e. ``n_samples``) and dimensionality\n    :math:`D` (i.e. ``n_features``).\n\n    * *Brute force* query time grows as :math:`O[D N]`\n    * *Ball tree* query time grows as approximately :math:`O[D \\log(N)]`\n    * *KD tree* query time changes with :math:`D` in a way that is difficult\n      to precisely characterise.  For small :math:`D` (less than 20 or so)\n      the cost is approximately :math:`O[D\\log(N)]`, and the KD tree\n      query can be very efficient.\n      For larger :math:`D`, the cost increases to nearly :math:`O[DN]`, and\n      the overhead due to the tree\n      structure can lead to queries which are slower than brute force.\n\n    For small data sets (:math:`N` less than 30 or so), :math:`\\log(N)` is\n    comparable to :math:`N`, and brute force algorithms can be more efficient\n    than a tree-based approach.  Both :class:`KDTree` and :class:`BallTree`\n    address this through providing a *leaf size* parameter: this controls the\n    number of samples at which a query switches to brute-force.  This allows both\n    algorithms to approach the efficiency of a brute-force computation for small\n    :math:`N`.\n\n  * data structure: *intrinsic dimensionality* of the data and/or *sparsity*\n    of the data. Intrinsic dimensionality refers to the dimension\n    :math:`d \\le D` of a manifold on which the data lies, which can be linearly\n    or non-linearly embedded in the parameter space. Sparsity refers to the\n    degree to which the data fills the parameter space (this is to be\n    distinguished from the concept as used in \"sparse\" matrices.  The data\n    matrix may have no zero entries, but the **structure** can still be\n    \"sparse\" in this sense).\n\n    * *Brute force* query time is unchanged by data structure.\n    * *Ball tree* and *KD tree* query times can be greatly influenced\n      by data structure.  In general, sparser data with a smaller intrinsic\n      dimensionality leads to faster query times.  Because the KD tree\n      internal representation is aligned with the parameter axes, it will not\n      generally show as much improvement as ball tree for arbitrarily\n      structured data.\n\n    Datasets used in machine learning tend to be very structured, and are\n    very well-suited for tree-based queries.\n\n  * number of neighbors :math:`k` requested for a query point.\n\n    * *Brute force* query time is largely unaffected by the value of :math:`k`\n    * *Ball tree* and *KD tree* query time will become slower as :math:`k`\n      increases.  This is due to two effects: first, a larger :math:`k` leads\n      to the necessity to search a larger portion of the parameter space.\n      Second, using :math:`k > 1` requires internal queueing of results\n      as the tree is traversed.\n\n    As :math:`k` becomes large compared to :math:`N`, the ability to prune\n    branches in a tree-based query is reduced.  In this situation, Brute force\n    queries can be more efficient.\n\n  * number of query points.  Both the ball tree and the KD Tree\n    require a construction phase.  The cost of this construction becomes\n    negligible when amortized over many queries.  If only a small number of\n    queries will be performed, however, the construction can make up\n    a significant fraction of the total cost.  If very few query points\n    will be required, brute force is better than a tree-based method.\n\n  Currently, ``algorithm = 'auto'`` selects ``'brute'`` if any of the following\n  conditions are verified:\n\n  * input data is sparse\n  * ``metric = 'precomputed'``\n  * :math:`D > 15`\n  * :math:`k >= N/2`\n  * ``effective_metric_`` isn't in the ``VALID_METRICS`` list for either\n    ``'kd_tree'`` or ``'ball_tree'``\n\n  Otherwise, it selects the first out of ``'kd_tree'`` and ``'ball_tree'`` that\n  has ``effective_metric_`` in its ``VALID_METRICS`` list. This heuristic is\n  based on the following assumptions:\n\n  * the number of query points is at least the same order as the number of\n    training points\n  * ``leaf_size`` is close to its default value of ``30``\n  * when :math:`D > 15`, the intrinsic dimensionality of the data is generally\n    too high for tree-based methods\n\n.. dropdown:: Effect of ``leaf_size``\n\n  As noted above, for small sample sizes a brute force search can be more\n  efficient than a tree-based query.  This fact is accounted for in the ball\n  tree and KD tree by internally switching to brute force searches within\n  leaf nodes.  The level of this switch can be specified with the parameter\n  ``leaf_size``.  This parameter choice has many effects:\n\n  **construction time**\n    A larger ``leaf_size`` leads to a faster tree construction time, because\n    fewer nodes need to be created\n\n  **query time**\n    Both a large or small ``leaf_size`` can lead to suboptimal query cost.\n    For ``leaf_size`` approaching 1, the overhead involved in traversing\n    nodes can significantly slow query times.  For ``leaf_size`` approaching\n    the size of the training set, queries become essentially brute force.\n    A good compromise between these is ``leaf_size = 30``, the default value\n    of the parameter.\n\n  **memory**\n    As ``leaf_size`` increases, the memory required to store a tree structure\n    decreases.  This is especially important in the case of ball tree, which\n    stores a :math:`D`-dimensional centroid for each node.  The required\n    storage space for :class:`BallTree` is approximately ``1 / leaf_size`` times\n    the size of the training set.\n\n  ``leaf_size`` is not referenced for brute force queries.\n\n.. dropdown:: Valid Metrics for Nearest Neighbor Algorithms\n\n  For a list of available metrics, see the documentation of the\n  :class:`~sklearn.metrics.DistanceMetric` class and the metrics listed in\n  `sklearn.metrics.pairwise.PAIRWISE_DISTANCE_FUNCTIONS`. Note that the \"cosine\"\n  metric uses :func:`~sklearn.metrics.pairwise.cosine_distances`.\n\n  A list of valid metrics for any of the above algorithms can be obtained by using their\n  ``valid_metric`` attribute. For example, valid metrics for ``KDTree`` can be generated by:\n\n      >>> from sklearn.neighbors import KDTree\n      >>> print(sorted(KDTree.valid_metrics))\n      ['chebyshev', 'cityblock', 'euclidean', 'infinity', 'l1', 'l2', 'manhattan', 'minkowski', 'p']\n\n.. _nearest_centroid_classifier:"
  },
  {
    "filename": "neighbors.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neighbors.rst.txt",
    "id": "neighbors.rst.txt_chunk_11",
    "header": "Nearest Centroid Classifier",
    "text": "Nearest Centroid Classifier\n===========================\n\nThe :class:`NearestCentroid` classifier is a simple algorithm that represents\neach class by the centroid of its members. In effect, this makes it\nsimilar to the label updating phase of the :class:`~sklearn.cluster.KMeans` algorithm.\nIt also has no parameters to choose, making it a good baseline classifier. It\ndoes, however, suffer on non-convex classes, as well as when classes have\ndrastically different variances, as equal variance in all dimensions is\nassumed. See Linear Discriminant Analysis (:class:`~sklearn.discriminant_analysis.LinearDiscriminantAnalysis`)\nand Quadratic Discriminant Analysis (:class:`~sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis`)\nfor more complex methods that do not make this assumption. Usage of the default\n:class:`NearestCentroid` is simple:\n\n    >>> from sklearn.neighbors import NearestCentroid\n    >>> import numpy as np\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    >>> y = np.array([1, 1, 1, 2, 2, 2])\n    >>> clf = NearestCentroid()\n    >>> clf.fit(X, y)\n    NearestCentroid()\n    >>> print(clf.predict([[-0.8, -1]]))\n    [1]"
  },
  {
    "filename": "neighbors.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neighbors.rst.txt",
    "id": "neighbors.rst.txt_chunk_12",
    "header": "Nearest Shrunken Centroid",
    "text": "Nearest Shrunken Centroid\n-------------------------\n\nThe :class:`NearestCentroid` classifier has a ``shrink_threshold`` parameter,\nwhich implements the nearest shrunken centroid classifier. In effect, the value\nof each feature for each centroid is divided by the within-class variance of\nthat feature. The feature values are then reduced by ``shrink_threshold``. Most\nnotably, if a particular feature value crosses zero, it is set\nto zero. In effect, this removes the feature from affecting the classification.\nThis is useful, for example, for removing noisy features.\n\nIn the example below, using a small shrink threshold increases the accuracy of\nthe model from 0.81 to 0.82.\n\n.. |nearest_centroid_1| image:: ../auto_examples/neighbors/images/sphx_glr_plot_nearest_centroid_001.png\n   :target: ../auto_examples/neighbors/plot_nearest_centroid.html\n   :scale: 50\n\n.. |nearest_centroid_2| image:: ../auto_examples/neighbors/images/sphx_glr_plot_nearest_centroid_002.png\n   :target: ../auto_examples/neighbors/plot_nearest_centroid.html\n   :scale: 50\n\n.. centered:: |nearest_centroid_1| |nearest_centroid_2|\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_neighbors_plot_nearest_centroid.py`: an example of\n  classification using nearest centroid with different shrink thresholds.\n\n.. _neighbors_transformer:"
  },
  {
    "filename": "neighbors.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neighbors.rst.txt",
    "id": "neighbors.rst.txt_chunk_13",
    "header": "Nearest Neighbors Transformer",
    "text": "Nearest Neighbors Transformer\n=============================\n\nMany scikit-learn estimators rely on nearest neighbors: Several classifiers and\nregressors such as :class:`KNeighborsClassifier` and\n:class:`KNeighborsRegressor`, but also some clustering methods such as\n:class:`~sklearn.cluster.DBSCAN` and\n:class:`~sklearn.cluster.SpectralClustering`, and some manifold embeddings such\nas :class:`~sklearn.manifold.TSNE` and :class:`~sklearn.manifold.Isomap`.\n\nAll these estimators can compute internally the nearest neighbors, but most of\nthem also accept precomputed nearest neighbors :term:`sparse graph`,\nas given by :func:`~sklearn.neighbors.kneighbors_graph` and\n:func:`~sklearn.neighbors.radius_neighbors_graph`. With mode\n`mode='connectivity'`, these functions return a binary adjacency sparse graph\nas required, for instance, in :class:`~sklearn.cluster.SpectralClustering`.\nWhereas with `mode='distance'`, they return a distance sparse graph as required,\nfor instance, in :class:`~sklearn.cluster.DBSCAN`. To include these functions in\na scikit-learn pipeline, one can also use the corresponding classes\n:class:`KNeighborsTransformer` and :class:`RadiusNeighborsTransformer`.\nThe benefits of this sparse graph API are multiple.\n\nFirst, the precomputed graph can be reused multiple times, for instance while\nvarying a parameter of the estimator. This can be done manually by the user, or\nusing the caching properties of the scikit-learn pipeline:\n\n    >>> import tempfile\n    >>> from sklearn.manifold import Isomap\n    >>> from sklearn.neighbors import KNeighborsTransformer\n    >>> from sklearn.pipeline import make_pipeline\n    >>> from sklearn.datasets import make_regression\n    >>> cache_path = tempfile.gettempdir()  # we use a temporary folder here\n    >>> X, _ = make_regression(n_samples=50, n_features=25, random_state=0)\n    >>> estimator = make_pipeline(\n    ...     KNeighborsTransformer(mode='distance'),\n    ...     Isomap(n_components=3, metric='precomputed'),\n    ...     memory=cache_path)\n    >>> X_embedded = estimator.fit_transform(X)\n    >>> X_embedded.shape\n    (50, 3)\n\nSecond, precomputing the graph can give finer control on the nearest neighbors\nestimation, for instance enabling multiprocessing though the parameter\n`n_jobs`, which might not be available in all estimators.\n\nFinally, the precomputation can be performed by custom estimators to use\ndifferent implementations, such as approximate nearest neighbors methods, or\nimplementation with special data types. The precomputed neighbors\n:term:`sparse graph` needs to be formatted as in\n:func:`~sklearn.neighbors.radius_neighbors_graph` output:\n\n* a CSR matrix (although COO, CSC or LIL will be accepted).\n* only explicitly store nearest neighborhoods of each sample with respect to the\n  training data. This should include those at 0 distance from a query point,\n  including the matrix diagonal when computing the nearest neighborhoods\n  between the training data and itself.\n* each row's `data` should store the distance in increasing order (optional.\n  Unsorted data will be stable-sorted, adding a computational overhead).\n* all values in data should be non-negative.\n* there should be no duplicate `indices` in any row\n  (see https://github.com/scipy/scipy/issues/5807).\n* if the algorithm being passed the precomputed matrix uses k nearest neighbors\n  (as opposed to radius neighborhood), at least k neighbors must be stored in\n  each row (or k+1, as explained in the following note).\n\n.. note::\n  When a specific number of neighbors is queried (using\n  :class:`KNeighborsTransformer`), the definition of `n_neighbors` is ambiguous\n  since it can either include each training point as its own neighbor, or\n  exclude them. Neither choice is perfect, since including them leads to a\n  different number of non-self neighbors during training and testing, while\n  excluding them leads to a difference between `fit(X).transform(X)` and\n  `fit_transform(X)`, which is against scikit-learn API.\n  In :class:`KNeighborsTransformer` we use the definition which includes each\n  training point as its own neighbor in the count of `n_neighbors`. However,\n  for compatibility reasons with other estimators which use the other\n  definition, one extra neighbor will be computed when `mode == 'distance'`.\n  To maximise compatibility with all estimators, a safe choice is to always\n  include one extra neighbor in a custom nearest neighbors estimator, since\n  unnecessary neighbors will be filtered by following estimators.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_neighbors_approximate_nearest_neighbors.py`:\n  an example of pipelining :class:`KNeighborsTransformer` and\n  :class:`~sklearn.manifold.TSNE`. Also proposes two custom nearest neighbors\n  estimators based on external packages.\n\n* :ref:`sphx_glr_auto_examples_neighbors_plot_caching_nearest_neighbors.py`:\n  an example of pipelining :class:`KNeighborsTransformer` and\n  :class:`KNeighborsClassifier` to enable caching of the neighbors graph\n  during a hyper-parameter grid-search.\n\n.. _nca:"
  },
  {
    "filename": "neighbors.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neighbors.rst.txt",
    "id": "neighbors.rst.txt_chunk_14",
    "header": "Neighborhood Components Analysis",
    "text": "Neighborhood Components Analysis\n================================\n\n.. sectionauthor:: William de Vazelhes <william.de-vazelhes@inria.fr>\n\nNeighborhood Components Analysis (NCA, :class:`NeighborhoodComponentsAnalysis`)\nis a distance metric learning algorithm which aims to improve the accuracy of\nnearest neighbors classification compared to the standard Euclidean distance.\nThe algorithm directly maximizes a stochastic variant of the leave-one-out\nk-nearest neighbors (KNN) score on the training set. It can also learn a\nlow-dimensional linear projection of data that can be used for data\nvisualization and fast classification.\n\n.. |nca_illustration_1| image:: ../auto_examples/neighbors/images/sphx_glr_plot_nca_illustration_001.png\n   :target: ../auto_examples/neighbors/plot_nca_illustration.html\n   :scale: 50\n\n.. |nca_illustration_2| image:: ../auto_examples/neighbors/images/sphx_glr_plot_nca_illustration_002.png\n   :target: ../auto_examples/neighbors/plot_nca_illustration.html\n   :scale: 50\n\n.. centered:: |nca_illustration_1| |nca_illustration_2|\n\nIn the above illustrating figure, we consider some points from a randomly\ngenerated dataset. We focus on the stochastic KNN classification of point no.\n3. The thickness of a link between sample 3 and another point is proportional\nto their distance, and can be seen as the relative weight (or probability) that\na stochastic nearest neighbor prediction rule would assign to this point. In\nthe original space, sample 3 has many stochastic neighbors from various\nclasses, so the right class is not very likely. However, in the projected space\nlearned by NCA, the only stochastic neighbors with non-negligible weight are\nfrom the same class as sample 3, guaranteeing that the latter will be well\nclassified. See the :ref:`mathematical formulation <nca_mathematical_formulation>`\nfor more details."
  },
  {
    "filename": "neighbors.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neighbors.rst.txt",
    "id": "neighbors.rst.txt_chunk_15",
    "header": "Classification",
    "text": "Classification\n--------------\n\nCombined with a nearest neighbors classifier (:class:`KNeighborsClassifier`),\nNCA is attractive for classification because it can naturally handle\nmulti-class problems without any increase in the model size, and does not\nintroduce additional parameters that require fine-tuning by the user.\n\nNCA classification has been shown to work well in practice for data sets of\nvarying size and difficulty. In contrast to related methods such as Linear\nDiscriminant Analysis, NCA does not make any assumptions about the class\ndistributions. The nearest neighbor classification can naturally produce highly\nirregular decision boundaries.\n\nTo use this model for classification, one needs to combine a\n:class:`NeighborhoodComponentsAnalysis` instance that learns the optimal\ntransformation with a :class:`KNeighborsClassifier` instance that performs the\nclassification in the projected space. Here is an example using the two\nclasses:\n\n    >>> from sklearn.neighbors import (NeighborhoodComponentsAnalysis,\n    ... KNeighborsClassifier)\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn.model_selection import train_test_split\n    >>> from sklearn.pipeline import Pipeline\n    >>> X, y = load_iris(return_X_y=True)\n    >>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n    ... stratify=y, test_size=0.7, random_state=42)\n    >>> nca = NeighborhoodComponentsAnalysis(random_state=42)\n    >>> knn = KNeighborsClassifier(n_neighbors=3)\n    >>> nca_pipe = Pipeline([('nca', nca), ('knn', knn)])\n    >>> nca_pipe.fit(X_train, y_train)\n    Pipeline(...)\n    >>> print(nca_pipe.score(X_test, y_test))\n    0.96190476...\n\n.. |nca_classification_1| image:: ../auto_examples/neighbors/images/sphx_glr_plot_nca_classification_001.png\n   :target: ../auto_examples/neighbors/plot_nca_classification.html\n   :scale: 50\n\n.. |nca_classification_2| image:: ../auto_examples/neighbors/images/sphx_glr_plot_nca_classification_002.png\n   :target: ../auto_examples/neighbors/plot_nca_classification.html\n   :scale: 50\n\n.. centered:: |nca_classification_1| |nca_classification_2|\n\nThe plot shows decision boundaries for Nearest Neighbor Classification and\nNeighborhood Components Analysis classification on the iris dataset, when\ntraining and scoring on only two features, for visualisation purposes.\n\n.. _nca_dim_reduction:"
  },
  {
    "filename": "neighbors.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neighbors.rst.txt",
    "id": "neighbors.rst.txt_chunk_16",
    "header": "Dimensionality reduction",
    "text": "Dimensionality reduction\n------------------------\n\nNCA can be used to perform supervised dimensionality reduction. The input data\nare projected onto a linear subspace consisting of the directions which\nminimize the NCA objective. The desired dimensionality can be set using the\nparameter ``n_components``. For instance, the following figure shows a\ncomparison of dimensionality reduction with Principal Component Analysis\n(:class:`~sklearn.decomposition.PCA`), Linear Discriminant Analysis\n(:class:`~sklearn.discriminant_analysis.LinearDiscriminantAnalysis`) and\nNeighborhood Component Analysis (:class:`NeighborhoodComponentsAnalysis`) on\nthe Digits dataset, a dataset with size :math:`n_{samples} = 1797` and\n:math:`n_{features} = 64`. The data set is split into a training and a test set\nof equal size, then standardized. For evaluation the 3-nearest neighbor\nclassification accuracy is computed on the 2-dimensional projected points found\nby each method. Each data sample belongs to one of 10 classes.\n\n.. |nca_dim_reduction_1| image:: ../auto_examples/neighbors/images/sphx_glr_plot_nca_dim_reduction_001.png\n   :target: ../auto_examples/neighbors/plot_nca_dim_reduction.html\n   :width: 32%\n\n.. |nca_dim_reduction_2| image:: ../auto_examples/neighbors/images/sphx_glr_plot_nca_dim_reduction_002.png\n   :target: ../auto_examples/neighbors/plot_nca_dim_reduction.html\n   :width: 32%\n\n.. |nca_dim_reduction_3| image:: ../auto_examples/neighbors/images/sphx_glr_plot_nca_dim_reduction_003.png\n   :target: ../auto_examples/neighbors/plot_nca_dim_reduction.html\n   :width: 32%\n\n.. centered:: |nca_dim_reduction_1| |nca_dim_reduction_2| |nca_dim_reduction_3|\n\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_neighbors_plot_nca_classification.py`\n* :ref:`sphx_glr_auto_examples_neighbors_plot_nca_dim_reduction.py`\n* :ref:`sphx_glr_auto_examples_manifold_plot_lle_digits.py`\n\n.. _nca_mathematical_formulation:"
  },
  {
    "filename": "neighbors.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neighbors.rst.txt",
    "id": "neighbors.rst.txt_chunk_17",
    "header": "Mathematical formulation",
    "text": "Mathematical formulation\n------------------------\n\nThe goal of NCA is to learn an optimal linear transformation matrix of size\n``(n_components, n_features)``, which maximises the sum over all samples\n:math:`i` of the probability :math:`p_i` that :math:`i` is correctly\nclassified, i.e.:\n\n.. math::\n\n  \\underset{L}{\\arg\\max} \\sum\\limits_{i=0}^{N - 1} p_{i}\n\nwith :math:`N` = ``n_samples`` and :math:`p_i` the probability of sample\n:math:`i` being correctly classified according to a stochastic nearest\nneighbors rule in the learned embedded space:\n\n.. math::\n\n  p_{i}=\\sum\\limits_{j \\in C_i}{p_{i j}}\n\nwhere :math:`C_i` is the set of points in the same class as sample :math:`i`,\nand :math:`p_{i j}` is the softmax over Euclidean distances in the embedded\nspace:\n\n.. math::\n\n  p_{i j} = \\frac{\\exp(-||L x_i - L x_j||^2)}{\\sum\\limits_{k \\ne\n            i} {\\exp{-(||L x_i - L x_k||^2)}}} , \\quad p_{i i} = 0\n\n.. dropdown:: Mahalanobis distance\n\n  NCA can be seen as learning a (squared) Mahalanobis distance metric:\n\n  .. math::\n\n      || L(x_i - x_j)||^2 = (x_i - x_j)^TM(x_i - x_j),\n\n  where :math:`M = L^T L` is a symmetric positive semi-definite matrix of size\n  ``(n_features, n_features)``."
  },
  {
    "filename": "neighbors.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neighbors.rst.txt",
    "id": "neighbors.rst.txt_chunk_18",
    "header": "Implementation",
    "text": "Implementation\n--------------\n\nThis implementation follows what is explained in the original paper [1]_. For\nthe optimisation method, it currently uses scipy's L-BFGS-B with a full\ngradient computation at each iteration, to avoid to tune the learning rate and\nprovide stable learning.\n\nSee the examples below and the docstring of\n:meth:`NeighborhoodComponentsAnalysis.fit` for further information."
  },
  {
    "filename": "neighbors.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neighbors.rst.txt",
    "id": "neighbors.rst.txt_chunk_19",
    "header": "Complexity",
    "text": "Complexity\n----------"
  },
  {
    "filename": "neighbors.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neighbors.rst.txt",
    "id": "neighbors.rst.txt_chunk_20",
    "header": "Training",
    "text": "Training\n^^^^^^^^\nNCA stores a matrix of pairwise distances, taking ``n_samples ** 2`` memory.\nTime complexity depends on the number of iterations done by the optimisation\nalgorithm. However, one can set the maximum number of iterations with the\nargument ``max_iter``. For each iteration, time complexity is\n``O(n_components x n_samples x min(n_samples, n_features))``."
  },
  {
    "filename": "neighbors.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neighbors.rst.txt",
    "id": "neighbors.rst.txt_chunk_21",
    "header": "Transform",
    "text": "Transform\n^^^^^^^^^\nHere the ``transform`` operation returns :math:`LX^T`, therefore its time\ncomplexity equals ``n_components * n_features * n_samples_test``. There is no\nadded space complexity in the operation.\n\n\n.. rubric:: References\n\n.. [1] `\"Neighbourhood Components Analysis\"\n  <https://papers.nips.cc/paper_files/paper/2004/file/42fe880812925e520249e808937738d2-Paper.pdf>`_,\n  J. Goldberger, S. Roweis, G. Hinton, R. Salakhutdinov, Advances in\n  Neural Information Processing Systems, Vol. 17, May 2005, pp. 513-520.\n\n* `Wikipedia entry on Neighborhood Components Analysis\n  <https://en.wikipedia.org/wiki/Neighbourhood_components_analysis>`_"
  },
  {
    "filename": "neural_networks_supervised.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neural_networks_supervised.rst.txt",
    "id": "neural_networks_supervised.rst.txt_chunk_0",
    "header": ".. _neural_networks_supervised:",
    "text": ".. _neural_networks_supervised:\n\n=================================="
  },
  {
    "filename": "neural_networks_supervised.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neural_networks_supervised.rst.txt",
    "id": "neural_networks_supervised.rst.txt_chunk_1",
    "header": "Neural network models (supervised)",
    "text": "Neural network models (supervised)\n==================================\n\n.. currentmodule:: sklearn.neural_network\n\n\n.. warning::\n\n    This implementation is not intended for large-scale applications. In particular,\n    scikit-learn offers no GPU support. For much faster, GPU-based implementations,\n    as well as frameworks offering much more flexibility to build deep learning\n    architectures, see  :ref:`related_projects`.\n\n.. _multilayer_perceptron:"
  },
  {
    "filename": "neural_networks_supervised.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neural_networks_supervised.rst.txt",
    "id": "neural_networks_supervised.rst.txt_chunk_2",
    "header": "Multi-layer Perceptron",
    "text": "Multi-layer Perceptron\n======================\n\n**Multi-layer Perceptron (MLP)** is a supervised learning algorithm that learns\na function :math:`f: R^m \\rightarrow R^o` by training on a dataset,\nwhere :math:`m` is the number of dimensions for input and :math:`o` is the\nnumber of dimensions for output. Given a set of features :math:`X = \\{x_1, x_2, ..., x_m\\}`\nand a target :math:`y`, it can learn a non-linear function approximator for either\nclassification or regression. It is different from logistic regression, in that\nbetween the input and the output layer, there can be one or more non-linear\nlayers, called hidden layers. Figure 1 shows a one hidden layer MLP with scalar\noutput.\n\n.. figure:: ../images/multilayerperceptron_network.png\n   :align: center\n   :scale: 60%\n\n   **Figure 1 : One hidden layer MLP.**\n\nThe leftmost layer, known as the input layer, consists of a set of neurons\n:math:`\\{x_i | x_1, x_2, ..., x_m\\}` representing the input features. Each\nneuron in the hidden layer transforms the values from the previous layer with\na weighted linear summation :math:`w_1x_1 + w_2x_2 + ... + w_mx_m`, followed\nby a non-linear activation function :math:`g(\\cdot):R \\rightarrow R` - like\nthe hyperbolic tan function. The output layer receives the values from the\nlast hidden layer and transforms them into output values.\n\nThe module contains the public attributes ``coefs_`` and ``intercepts_``.\n``coefs_`` is a list of weight matrices, where weight matrix at index\n:math:`i` represents the weights between layer :math:`i` and layer\n:math:`i+1`. ``intercepts_`` is a list of bias vectors, where the vector\nat index :math:`i` represents the bias values added to layer :math:`i+1`.\n\n.. dropdown:: Advantages and disadvantages of Multi-layer Perceptron\n\n  The advantages of Multi-layer Perceptron are:\n\n  + Capability to learn non-linear models.\n\n  + Capability to learn models in real-time (on-line learning)\n    using ``partial_fit``.\n\n\n  The disadvantages of Multi-layer Perceptron (MLP) include:\n\n  + MLP with hidden layers has a non-convex loss function where there exists\n    more than one local minimum. Therefore, different random weight\n    initializations can lead to different validation accuracy.\n\n  + MLP requires tuning a number of hyperparameters such as the number of\n    hidden neurons, layers, and iterations.\n\n  + MLP is sensitive to feature scaling.\n\n  Please see :ref:`Tips on Practical Use <mlp_tips>` section that addresses\n  some of these disadvantages."
  },
  {
    "filename": "neural_networks_supervised.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neural_networks_supervised.rst.txt",
    "id": "neural_networks_supervised.rst.txt_chunk_3",
    "header": "Classification",
    "text": "Classification\n==============\n\nClass :class:`MLPClassifier` implements a multi-layer perceptron (MLP) algorithm\nthat trains using `Backpropagation <http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/#backpropagation_algorithm>`_.\n\nMLP trains on two arrays: array X of size (n_samples, n_features), which holds\nthe training samples represented as floating point feature vectors; and array\ny of size (n_samples,), which holds the target values (class labels) for the\ntraining samples::\n\n    >>> from sklearn.neural_network import MLPClassifier\n    >>> X = [[0., 0.], [1., 1.]]\n    >>> y = [0, 1]\n    >>> clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n    ...                     hidden_layer_sizes=(5, 2), random_state=1)\n    ...\n    >>> clf.fit(X, y)\n    MLPClassifier(alpha=1e-05, hidden_layer_sizes=(5, 2), random_state=1,\n                  solver='lbfgs')\n\nAfter fitting (training), the model can predict labels for new samples::\n\n    >>> clf.predict([[2., 2.], [-1., -2.]])\n    array([1, 0])\n\nMLP can fit a non-linear model to the training data. ``clf.coefs_``\ncontains the weight matrices that constitute the model parameters::\n\n    >>> [coef.shape for coef in clf.coefs_]\n    [(2, 5), (5, 2), (2, 1)]\n\nCurrently, :class:`MLPClassifier` supports only the\nCross-Entropy loss function, which allows probability estimates by running the\n``predict_proba`` method.\n\nMLP trains using Backpropagation. More precisely, it trains using some form of\ngradient descent and the gradients are calculated using Backpropagation. For\nclassification, it minimizes the Cross-Entropy loss function, giving a vector\nof probability estimates :math:`P(y|x)` per sample :math:`x`::\n\n    >>> clf.predict_proba([[2., 2.], [1., 2.]])\n    array([[1.967e-04, 9.998e-01],\n           [1.967e-04, 9.998e-01]])\n\n:class:`MLPClassifier` supports multi-class classification by\napplying `Softmax <https://en.wikipedia.org/wiki/Softmax_activation_function>`_\nas the output function.\n\nFurther, the model supports :ref:`multi-label classification <multiclass>`\nin which a sample can belong to more than one class. For each class, the raw\noutput passes through the logistic function. Values larger or equal to `0.5`\nare rounded to `1`, otherwise to `0`. For a predicted output of a sample, the\nindices where the value is `1` represent the assigned classes of that sample::\n\n    >>> X = [[0., 0.], [1., 1.]]\n    >>> y = [[0, 1], [1, 1]]\n    >>> clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n    ...                     hidden_layer_sizes=(15,), random_state=1)\n    ...\n    >>> clf.fit(X, y)\n    MLPClassifier(alpha=1e-05, hidden_layer_sizes=(15,), random_state=1,\n                  solver='lbfgs')\n    >>> clf.predict([[1., 2.]])\n    array([[1, 1]])\n    >>> clf.predict([[0., 0.]])\n    array([[0, 1]])\n\nSee the examples below and the docstring of\n:meth:`MLPClassifier.fit` for further information.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_neural_networks_plot_mlp_training_curves.py`\n* See :ref:`sphx_glr_auto_examples_neural_networks_plot_mnist_filters.py` for\n  visualized representation of trained weights."
  },
  {
    "filename": "neural_networks_supervised.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neural_networks_supervised.rst.txt",
    "id": "neural_networks_supervised.rst.txt_chunk_4",
    "header": "Regression",
    "text": "Regression\n==========\n\nClass :class:`MLPRegressor` implements a multi-layer perceptron (MLP) that\ntrains using backpropagation with no activation function in the output layer,\nwhich can also be seen as using the identity function as activation function.\nTherefore, it uses the square error as the loss function, and the output is a\nset of continuous values.\n\n:class:`MLPRegressor` also supports multi-output regression, in\nwhich a sample can have more than one target."
  },
  {
    "filename": "neural_networks_supervised.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neural_networks_supervised.rst.txt",
    "id": "neural_networks_supervised.rst.txt_chunk_5",
    "header": "Regularization",
    "text": "Regularization\n==============\n\nBoth :class:`MLPRegressor` and :class:`MLPClassifier` use parameter ``alpha``\nfor regularization (L2 regularization) term which helps in avoiding overfitting\nby penalizing weights with large magnitudes. Following plot displays varying\ndecision function with value of alpha.\n\n.. figure:: ../auto_examples/neural_networks/images/sphx_glr_plot_mlp_alpha_001.png\n   :target: ../auto_examples/neural_networks/plot_mlp_alpha.html\n   :align: center\n   :scale: 75\n\nSee the examples below for further information.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_neural_networks_plot_mlp_alpha.py`"
  },
  {
    "filename": "neural_networks_supervised.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neural_networks_supervised.rst.txt",
    "id": "neural_networks_supervised.rst.txt_chunk_6",
    "header": "Algorithms",
    "text": "Algorithms\n==========\n\nMLP trains using `Stochastic Gradient Descent\n<https://en.wikipedia.org/wiki/Stochastic_gradient_descent>`_,\n:arxiv:`Adam <1412.6980>`, or\n`L-BFGS <https://en.wikipedia.org/wiki/Limited-memory_BFGS>`__.\nStochastic Gradient Descent (SGD) updates parameters using the gradient of the\nloss function with respect to a parameter that needs adaptation, i.e.\n\n.. math::\n\n    w \\leftarrow w - \\eta (\\alpha \\frac{\\partial R(w)}{\\partial w}\n    + \\frac{\\partial Loss}{\\partial w})\n\nwhere :math:`\\eta` is the learning rate which controls the step-size in\nthe parameter space search.  :math:`Loss` is the loss function used\nfor the network.\n\nMore details can be found in the documentation of\n`SGD <https://scikit-learn.org/stable/modules/sgd.html>`_\n\nAdam is similar to SGD in a sense that it is a stochastic optimizer, but it can\nautomatically adjust the amount to update parameters based on adaptive estimates\nof lower-order moments.\n\nWith SGD or Adam, training supports online and mini-batch learning.\n\nL-BFGS is a solver that approximates the Hessian matrix which represents the\nsecond-order partial derivative of a function. Further it approximates the\ninverse of the Hessian matrix to perform parameter updates. The implementation\nuses the Scipy version of `L-BFGS\n<https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html>`_.\n\nIf the selected solver is 'L-BFGS', training does not support online nor\nmini-batch learning."
  },
  {
    "filename": "neural_networks_supervised.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neural_networks_supervised.rst.txt",
    "id": "neural_networks_supervised.rst.txt_chunk_7",
    "header": "Complexity",
    "text": "Complexity\n==========\n\nSuppose there are :math:`n` training samples, :math:`m` features, :math:`k`\nhidden layers, each containing :math:`h` neurons - for simplicity, and :math:`o`\noutput neurons.  The time complexity of backpropagation is\n:math:`O(i \\cdot n \\cdot (m \\cdot h + (k - 1) \\cdot h \\cdot h + h \\cdot o))`, where :math:`i` is the number\nof iterations. Since backpropagation has a high time complexity, it is advisable\nto start with smaller number of hidden neurons and few hidden layers for\ntraining.\n\n.. dropdown:: Mathematical formulation\n\n  Given a set of training examples :math:`\\{(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\}`\n  where :math:`x_i \\in \\mathbf{R}^n` and :math:`y_i \\in \\{0, 1\\}`, a one hidden\n  layer one hidden neuron MLP learns the function :math:`f(x) = W_2 g(W_1^T x + b_1) + b_2`\n  where :math:`W_1 \\in \\mathbf{R}^m` and :math:`W_2, b_1, b_2 \\in \\mathbf{R}` are\n  model parameters. :math:`W_1, W_2` represent the weights of the input layer and\n  hidden layer, respectively; and :math:`b_1, b_2` represent the bias added to\n  the hidden layer and the output layer, respectively.\n  :math:`g(\\cdot) : R \\rightarrow R` is the activation function, set by default as\n  the hyperbolic tan. It is given as,\n\n  .. math::\n        g(z)= \\frac{e^z-e^{-z}}{e^z+e^{-z}}\n\n  For binary classification, :math:`f(x)` passes through the logistic function\n  :math:`g(z)=1/(1+e^{-z})` to obtain output values between zero and one. A\n  threshold, set to 0.5, would assign samples of outputs larger or equal 0.5\n  to the positive class, and the rest to the negative class.\n\n  If there are more than two classes, :math:`f(x)` itself would be a vector of\n  size (n_classes,). Instead of passing through logistic function, it passes\n  through the softmax function, which is written as,\n\n  .. math::\n        \\text{softmax}(z)_i = \\frac{\\exp(z_i)}{\\sum_{l=1}^k\\exp(z_l)}\n\n  where :math:`z_i` represents the :math:`i` th element of the input to softmax,\n  which corresponds to class :math:`i`, and :math:`K` is the number of classes.\n  The result is a vector containing the probabilities that sample :math:`x`\n  belongs to each class. The output is the class with the highest probability.\n\n  In regression, the output remains as :math:`f(x)`; therefore, output activation\n  function is just the identity function.\n\n  MLP uses different loss functions depending on the problem type. The loss\n  function for classification is Average Cross-Entropy, which in binary case is\n  given as,\n\n  .. math::\n\n      Loss(\\hat{y},y,W) = -\\dfrac{1}{n}\\sum_{i=0}^n(y_i \\ln {\\hat{y_i}} + (1-y_i) \\ln{(1-\\hat{y_i})}) + \\dfrac{\\alpha}{2n} ||W||_2^2\n\n  where :math:`\\alpha ||W||_2^2` is an L2-regularization term (aka penalty)\n  that penalizes complex models; and :math:`\\alpha > 0` is a non-negative\n  hyperparameter that controls the magnitude of the penalty.\n\n  For regression, MLP uses the Mean Square Error loss function; written as,\n\n  .. math::\n\n      Loss(\\hat{y},y,W) = \\frac{1}{2n}\\sum_{i=0}^n||\\hat{y}_i - y_i ||_2^2 + \\frac{\\alpha}{2n} ||W||_2^2\n\n  Starting from initial random weights, multi-layer perceptron (MLP) minimizes\n  the loss function by repeatedly updating these weights. After computing the\n  loss, a backward pass propagates it from the output layer to the previous\n  layers, providing each weight parameter with an update value meant to decrease\n  the loss.\n\n  In gradient descent, the gradient :math:`\\nabla Loss_{W}` of the loss with respect\n  to the weights is computed and deducted from :math:`W`.\n  More formally, this is expressed as,\n\n  .. math::\n      W^{i+1} = W^i - \\epsilon \\nabla {Loss}_{W}^{i}\n\n  where :math:`i` is the iteration step, and :math:`\\epsilon` is the learning rate\n  with a value larger than 0.\n\n  The algorithm stops when it reaches a preset maximum number of iterations; or\n  when the improvement in loss is below a certain, small number.\n\n\n.. _mlp_tips:"
  },
  {
    "filename": "neural_networks_supervised.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neural_networks_supervised.rst.txt",
    "id": "neural_networks_supervised.rst.txt_chunk_8",
    "header": "Tips on Practical Use",
    "text": "Tips on Practical Use\n=====================\n\n* Multi-layer Perceptron is sensitive to feature scaling, so it\n  is highly recommended to scale your data. For example, scale each\n  attribute on the input vector X to [0, 1] or [-1, +1], or standardize\n  it to have mean 0 and variance 1. Note that you must apply the *same*\n  scaling to the test set for meaningful results.\n  You can use :class:`~sklearn.preprocessing.StandardScaler` for standardization.\n\n    >>> from sklearn.preprocessing import StandardScaler  # doctest: +SKIP\n    >>> scaler = StandardScaler()  # doctest: +SKIP\n    >>> # Don't cheat - fit only on training data\n    >>> scaler.fit(X_train)  # doctest: +SKIP\n    >>> X_train = scaler.transform(X_train)  # doctest: +SKIP\n    >>> # apply same transformation to test data\n    >>> X_test = scaler.transform(X_test)  # doctest: +SKIP\n\n  An alternative and recommended approach is to use\n  :class:`~sklearn.preprocessing.StandardScaler` in a\n  :class:`~sklearn.pipeline.Pipeline`\n\n* Finding a reasonable regularization parameter :math:`\\alpha` is best done\n  using :class:`~sklearn.model_selection.GridSearchCV`, usually in the range\n  ``10.0 ** -np.arange(1, 7)``.\n\n* Empirically, we observed that `L-BFGS` converges faster and\n  with better solutions on small datasets. For relatively large\n  datasets, however, `Adam` is very robust. It usually converges\n  quickly and gives pretty good performance. `SGD` with momentum or\n  nesterov's momentum, on the other hand, can perform better than\n  those two algorithms if learning rate is correctly tuned."
  },
  {
    "filename": "neural_networks_supervised.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neural_networks_supervised.rst.txt",
    "id": "neural_networks_supervised.rst.txt_chunk_9",
    "header": "More control with warm_start",
    "text": "More control with warm_start\n============================\nIf you want more control over stopping criteria or learning rate in SGD,\nor want to do additional monitoring, using ``warm_start=True`` and\n``max_iter=1`` and iterating yourself can be helpful::\n\n    >>> X = [[0., 0.], [1., 1.]]\n    >>> y = [0, 1]\n    >>> clf = MLPClassifier(hidden_layer_sizes=(15,), random_state=1, max_iter=1, warm_start=True)\n    >>> for i in range(10):\n    ...     clf.fit(X, y)\n    ...     # additional monitoring / inspection\n    MLPClassifier(...\n\n.. dropdown:: References\n\n  * `\"Learning representations by back-propagating errors.\"\n    <https://www.iro.umontreal.ca/~pift6266/A06/refs/backprop_old.pdf>`_\n    Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams.\n\n  * `\"Stochastic Gradient Descent\" <https://leon.bottou.org/projects/sgd>`_ L. Bottou - Website, 2010.\n\n  * `\"Backpropagation\" <http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm>`_\n    Andrew Ng, Jiquan Ngiam, Chuan Yu Foo, Yifan Mai, Caroline Suen - Website, 2011.\n\n  * `\"Efficient BackProp\" <http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf>`_\n    Y. LeCun, L. Bottou, G. Orr, K. M\u00fcller - In Neural Networks: Tricks of the Trade 1998.\n\n  * :arxiv:`\"Adam: A method for stochastic optimization.\" <1412.6980>`\n    Kingma, Diederik, and Jimmy Ba (2014)"
  },
  {
    "filename": "neural_networks_unsupervised.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neural_networks_unsupervised.rst.txt",
    "id": "neural_networks_unsupervised.rst.txt_chunk_0",
    "header": ".. _neural_networks_unsupervised:",
    "text": ".. _neural_networks_unsupervised:\n\n===================================="
  },
  {
    "filename": "neural_networks_unsupervised.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neural_networks_unsupervised.rst.txt",
    "id": "neural_networks_unsupervised.rst.txt_chunk_1",
    "header": "Neural network models (unsupervised)",
    "text": "Neural network models (unsupervised)\n====================================\n\n.. currentmodule:: sklearn.neural_network\n\n\n.. _rbm:"
  },
  {
    "filename": "neural_networks_unsupervised.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neural_networks_unsupervised.rst.txt",
    "id": "neural_networks_unsupervised.rst.txt_chunk_2",
    "header": "Restricted Boltzmann machines",
    "text": "Restricted Boltzmann machines\n=============================\n\nRestricted Boltzmann machines (RBM) are unsupervised nonlinear feature learners\nbased on a probabilistic model. The features extracted by an RBM or a hierarchy\nof RBMs often give good results when fed into a linear classifier such as a\nlinear SVM or a perceptron.\n\nThe model makes assumptions regarding the distribution of inputs. At the moment,\nscikit-learn only provides :class:`BernoulliRBM`, which assumes the inputs are\neither binary values or values between 0 and 1, each encoding the probability\nthat the specific feature would be turned on.\n\nThe RBM tries to maximize the likelihood of the data using a particular\ngraphical model. The parameter learning algorithm used (:ref:`Stochastic\nMaximum Likelihood <sml>`) prevents the representations from straying far\nfrom the input data, which makes them capture interesting regularities, but\nmakes the model less useful for small datasets, and usually not useful for\ndensity estimation.\n\nThe method gained popularity for initializing deep neural networks with the\nweights of independent RBMs. This method is known as unsupervised pre-training.\n\n.. figure:: ../auto_examples/neural_networks/images/sphx_glr_plot_rbm_logistic_classification_001.png\n   :target: ../auto_examples/neural_networks/plot_rbm_logistic_classification.html\n   :align: center\n   :scale: 100%\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_neural_networks_plot_rbm_logistic_classification.py`"
  },
  {
    "filename": "neural_networks_unsupervised.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neural_networks_unsupervised.rst.txt",
    "id": "neural_networks_unsupervised.rst.txt_chunk_3",
    "header": "Graphical model and parametrization",
    "text": "Graphical model and parametrization\n-----------------------------------\n\nThe graphical model of an RBM is a fully-connected bipartite graph.\n\n.. image:: ../images/rbm_graph.png\n   :align: center\n\nThe nodes are random variables whose states depend on the state of the other\nnodes they are connected to. The model is therefore parameterized by the\nweights of the connections, as well as one intercept (bias) term for each\nvisible and hidden unit, omitted from the image for simplicity.\n\nThe energy function measures the quality of a joint assignment:\n\n.. math::\n\n   E(\\mathbf{v}, \\mathbf{h}) = -\\sum_i \\sum_j w_{ij}v_ih_j - \\sum_i b_iv_i\n     - \\sum_j c_jh_j\n\nIn the formula above, :math:`\\mathbf{b}` and :math:`\\mathbf{c}` are the\nintercept vectors for the visible and hidden layers, respectively. The\njoint probability of the model is defined in terms of the energy:\n\n.. math::\n\n   P(\\mathbf{v}, \\mathbf{h}) = \\frac{e^{-E(\\mathbf{v}, \\mathbf{h})}}{Z}\n\n\nThe word *restricted* refers to the bipartite structure of the model, which\nprohibits direct interaction between hidden units, or between visible units.\nThis means that the following conditional independencies are assumed:\n\n.. math::\n\n   h_i \\bot h_j | \\mathbf{v} \\\\\n   v_i \\bot v_j | \\mathbf{h}\n\nThe bipartite structure allows for the use of efficient block Gibbs sampling for\ninference."
  },
  {
    "filename": "neural_networks_unsupervised.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neural_networks_unsupervised.rst.txt",
    "id": "neural_networks_unsupervised.rst.txt_chunk_4",
    "header": "Bernoulli Restricted Boltzmann machines",
    "text": "Bernoulli Restricted Boltzmann machines\n---------------------------------------\n\nIn the :class:`BernoulliRBM`, all units are binary stochastic units. This\nmeans that the input data should either be binary, or real-valued between 0 and\n1 signifying the probability that the visible unit would turn on or off. This\nis a good model for character recognition, where the interest is on which\npixels are active and which aren't. For images of natural scenes it no longer\nfits because of background, depth and the tendency of neighbouring pixels to\ntake the same values.\n\nThe conditional probability distribution of each unit is given by the\nlogistic sigmoid activation function of the input it receives:\n\n.. math::\n\n   P(v_i=1|\\mathbf{h}) = \\sigma(\\sum_j w_{ij}h_j + b_i) \\\\\n   P(h_i=1|\\mathbf{v}) = \\sigma(\\sum_i w_{ij}v_i + c_j)\n\nwhere :math:`\\sigma` is the logistic sigmoid function:\n\n.. math::\n\n   \\sigma(x) = \\frac{1}{1 + e^{-x}}\n\n.. _sml:"
  },
  {
    "filename": "neural_networks_unsupervised.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\neural_networks_unsupervised.rst.txt",
    "id": "neural_networks_unsupervised.rst.txt_chunk_5",
    "header": "Stochastic Maximum Likelihood learning",
    "text": "Stochastic Maximum Likelihood learning\n--------------------------------------\n\nThe training algorithm implemented in :class:`BernoulliRBM` is known as\nStochastic Maximum Likelihood (SML) or Persistent Contrastive Divergence\n(PCD). Optimizing maximum likelihood directly is infeasible because of\nthe form of the data likelihood:\n\n.. math::\n\n   \\log P(v) = \\log \\sum_h e^{-E(v, h)} - \\log \\sum_{x, y} e^{-E(x, y)}\n\nFor simplicity the equation above is written for a single training example.\nThe gradient with respect to the weights is formed of two terms corresponding to\nthe ones above. They are usually known as the positive gradient and the negative\ngradient, because of their respective signs.  In this implementation, the\ngradients are estimated over mini-batches of samples.\n\nIn maximizing the log-likelihood, the positive gradient makes the model prefer\nhidden states that are compatible with the observed training data. Because of\nthe bipartite structure of RBMs, it can be computed efficiently. The\nnegative gradient, however, is intractable. Its goal is to lower the energy of\njoint states that the model prefers, therefore making it stay true to the data.\nIt can be approximated by Markov chain Monte Carlo using block Gibbs sampling by\niteratively sampling each of :math:`v` and :math:`h` given the other, until the\nchain mixes. Samples generated in this way are sometimes referred as fantasy\nparticles. This is inefficient and it is difficult to determine whether the\nMarkov chain mixes.\n\nThe Contrastive Divergence method suggests to stop the chain after a small\nnumber of iterations, :math:`k`, usually even 1. This method is fast and has\nlow variance, but the samples are far from the model distribution.\n\nPersistent Contrastive Divergence addresses this. Instead of starting a new\nchain each time the gradient is needed, and performing only one Gibbs sampling\nstep, in PCD we keep a number of chains (fantasy particles) that are updated\n:math:`k` Gibbs steps after each weight update. This allows the particles to\nexplore the space more thoroughly.\n\n.. rubric:: References\n\n* `\"A fast learning algorithm for deep belief nets\"\n  <https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf>`_,\n  G. Hinton, S. Osindero, Y.-W. Teh, 2006\n\n* `\"Training Restricted Boltzmann Machines using Approximations to\n  the Likelihood Gradient\"\n  <https://www.cs.toronto.edu/~tijmen/pcd/pcd.pdf>`_,\n  T. Tieleman, 2008"
  },
  {
    "filename": "outlier_detection.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\outlier_detection.rst.txt",
    "id": "outlier_detection.rst.txt_chunk_0",
    "header": ".. _outlier_detection:",
    "text": ".. _outlier_detection:\n\n==================================================="
  },
  {
    "filename": "outlier_detection.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\outlier_detection.rst.txt",
    "id": "outlier_detection.rst.txt_chunk_1",
    "header": "Novelty and Outlier Detection",
    "text": "Novelty and Outlier Detection\n===================================================\n\n.. currentmodule:: sklearn\n\nMany applications require being able to decide whether a new observation\nbelongs to the same distribution as existing observations (it is an\n*inlier*), or should be considered as different (it is an *outlier*).\nOften, this ability is used to clean real data sets. Two important\ndistinctions must be made:\n\n:outlier detection:\n  The training data contains outliers which are defined as observations that\n  are far from the others. Outlier detection estimators thus try to fit the\n  regions where the training data is the most concentrated, ignoring the\n  deviant observations.\n\n:novelty detection:\n  The training data is not polluted by outliers and we are interested in\n  detecting whether a **new** observation is an outlier. In this context an\n  outlier is also called a novelty.\n\nOutlier detection and novelty detection are both used for anomaly\ndetection, where one is interested in detecting abnormal or unusual\nobservations. Outlier detection is then also known as unsupervised anomaly\ndetection and novelty detection as semi-supervised anomaly detection. In the\ncontext of outlier detection, the outliers/anomalies cannot form a\ndense cluster as available estimators assume that the outliers/anomalies are\nlocated in low density regions. On the contrary, in the context of novelty\ndetection, novelties/anomalies can form a dense cluster as long as they are in\na low density region of the training data, considered as normal in this\ncontext.\n\nThe scikit-learn project provides a set of machine learning tools that\ncan be used both for novelty or outlier detection. This strategy is\nimplemented with objects learning in an unsupervised way from the data::\n\n    estimator.fit(X_train)\n\nnew observations can then be sorted as inliers or outliers with a\n``predict`` method::\n\n    estimator.predict(X_test)\n\nInliers are labeled 1, while outliers are labeled -1. The predict method\nmakes use of a threshold on the raw scoring function computed by the\nestimator. This scoring function is accessible through the ``score_samples``\nmethod, while the threshold can be controlled by the ``contamination``\nparameter.\n\nThe ``decision_function`` method is also defined from the scoring function,\nin such a way that negative values are outliers and non-negative ones are\ninliers::\n\n    estimator.decision_function(X_test)\n\nNote that :class:`neighbors.LocalOutlierFactor` does not support\n``predict``, ``decision_function`` and ``score_samples`` methods by default\nbut only a ``fit_predict`` method, as this estimator was originally meant to\nbe applied for outlier detection. The scores of abnormality of the training\nsamples are accessible through the ``negative_outlier_factor_`` attribute.\n\nIf you really want to use :class:`neighbors.LocalOutlierFactor` for novelty\ndetection, i.e. predict labels or compute the score of abnormality of new\nunseen data, you can instantiate the estimator with the ``novelty`` parameter\nset to ``True`` before fitting the estimator. In this case, ``fit_predict`` is\nnot available.\n\n.. warning:: **Novelty detection with Local Outlier Factor**\n\n  When ``novelty`` is set to ``True`` be aware that you must only use\n  ``predict``, ``decision_function`` and ``score_samples`` on new unseen data\n  and not on the training samples as this would lead to wrong results.\n  I.e., the result of ``predict`` will not be the same as ``fit_predict``.\n  The scores of abnormality of the training samples are always accessible\n  through the ``negative_outlier_factor_`` attribute.\n\nThe behavior of :class:`neighbors.LocalOutlierFactor` is summarized in the\nfollowing table.\n\n============================ ================================ =====================\nMethod                       Outlier detection                Novelty detection\n============================ ================================ =====================\n``fit_predict``              OK                               Not available\n``predict``                  Not available                    Use only on new data\n``decision_function``        Not available                    Use only on new data\n``score_samples``            Use ``negative_outlier_factor_`` Use only on new data\n``negative_outlier_factor_`` OK                               OK\n============================ ================================ ====================="
  },
  {
    "filename": "outlier_detection.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\outlier_detection.rst.txt",
    "id": "outlier_detection.rst.txt_chunk_2",
    "header": "Overview of outlier detection methods",
    "text": "Overview of outlier detection methods\n=====================================\n\nA comparison of the outlier detection algorithms in scikit-learn. Local\nOutlier Factor (LOF) does not show a decision boundary in black as it\nhas no predict method to be applied on new data when it is used for outlier\ndetection.\n\n.. figure:: ../auto_examples/miscellaneous/images/sphx_glr_plot_anomaly_comparison_001.png\n   :target: ../auto_examples/miscellaneous/plot_anomaly_comparison.html\n   :align: center\n   :scale: 50\n\n:class:`ensemble.IsolationForest` and :class:`neighbors.LocalOutlierFactor`\nperform reasonably well on the data sets considered here.\nThe :class:`svm.OneClassSVM` is known to be sensitive to outliers and thus\ndoes not perform very well for outlier detection. That being said, outlier\ndetection in high-dimension, or without any assumptions on the distribution\nof the inlying data is very challenging. :class:`svm.OneClassSVM` may still\nbe used with outlier detection but requires fine-tuning of its hyperparameter\n`nu` to handle outliers and prevent overfitting.\n:class:`linear_model.SGDOneClassSVM` provides an implementation of a\nlinear One-Class SVM with a linear complexity in the number of samples. This\nimplementation is here used with a kernel approximation technique to obtain\nresults similar to :class:`svm.OneClassSVM` which uses a Gaussian kernel\nby default. Finally, :class:`covariance.EllipticEnvelope` assumes the data is\nGaussian and learns an ellipse. For more details on the different estimators\nrefer to the example\n:ref:`sphx_glr_auto_examples_miscellaneous_plot_anomaly_comparison.py` and the\nsections hereunder.\n\n.. rubric:: Examples\n\n* See :ref:`sphx_glr_auto_examples_miscellaneous_plot_anomaly_comparison.py`\n  for a comparison of the :class:`svm.OneClassSVM`, the\n  :class:`ensemble.IsolationForest`, the\n  :class:`neighbors.LocalOutlierFactor` and\n  :class:`covariance.EllipticEnvelope`.\n\n* See :ref:`sphx_glr_auto_examples_miscellaneous_plot_outlier_detection_bench.py`\n  for an example showing how to evaluate outlier detection estimators,\n  the :class:`neighbors.LocalOutlierFactor` and the\n  :class:`ensemble.IsolationForest`, using ROC curves from\n  :class:`metrics.RocCurveDisplay`."
  },
  {
    "filename": "outlier_detection.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\outlier_detection.rst.txt",
    "id": "outlier_detection.rst.txt_chunk_3",
    "header": "Novelty Detection",
    "text": "Novelty Detection\n=================\n\nConsider a data set of :math:`n` observations from the same\ndistribution described by :math:`p` features.  Consider now that we\nadd one more observation to that data set. Is the new observation so\ndifferent from the others that we can doubt it is regular? (i.e. does\nit come from the same distribution?) Or on the contrary, is it so\nsimilar to the other that we cannot distinguish it from the original\nobservations? This is the question addressed by the novelty detection\ntools and methods.\n\nIn general, it is about to learn a rough, close frontier delimiting\nthe contour of the initial observations distribution, plotted in\nembedding :math:`p`-dimensional space. Then, if further observations\nlay within the frontier-delimited subspace, they are considered as\ncoming from the same population as the initial\nobservations. Otherwise, if they lay outside the frontier, we can say\nthat they are abnormal with a given confidence in our assessment.\n\nThe One-Class SVM has been introduced by Sch\u00f6lkopf et al. for that purpose\nand implemented in the :ref:`svm` module in the\n:class:`svm.OneClassSVM` object. It requires the choice of a\nkernel and a scalar parameter to define a frontier.  The RBF kernel is\nusually chosen although there exists no exact formula or algorithm to\nset its bandwidth parameter. This is the default in the scikit-learn\nimplementation. The `nu` parameter, also known as the margin of\nthe One-Class SVM, corresponds to the probability of finding a new,\nbut regular, observation outside the frontier.\n\n.. rubric:: References\n\n* `Estimating the support of a high-dimensional distribution\n  <https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-99-87.pdf>`_\n  Sch\u00f6lkopf, Bernhard, et al. Neural computation 13.7 (2001): 1443-1471.\n\n.. rubric:: Examples\n\n* See :ref:`sphx_glr_auto_examples_svm_plot_oneclass.py` for visualizing the\n  frontier learned around some data by a :class:`svm.OneClassSVM` object.\n\n* :ref:`sphx_glr_auto_examples_applications_plot_species_distribution_modeling.py`\n\n.. figure:: ../auto_examples/svm/images/sphx_glr_plot_oneclass_001.png\n   :target: ../auto_examples/svm/plot_oneclass.html\n   :align: center\n   :scale: 75%"
  },
  {
    "filename": "outlier_detection.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\outlier_detection.rst.txt",
    "id": "outlier_detection.rst.txt_chunk_4",
    "header": "Scaling up the One-Class SVM",
    "text": "Scaling up the One-Class SVM\n----------------------------\n\nAn online linear version of the One-Class SVM is implemented in\n:class:`linear_model.SGDOneClassSVM`. This implementation scales linearly with\nthe number of samples and can be used with a kernel approximation to\napproximate the solution of a kernelized :class:`svm.OneClassSVM` whose\ncomplexity is at best quadratic in the number of samples. See section\n:ref:`sgd_online_one_class_svm` for more details.\n\n.. rubric:: Examples\n\n* See :ref:`sphx_glr_auto_examples_linear_model_plot_sgdocsvm_vs_ocsvm.py`\n  for an illustration of the approximation of a kernelized One-Class SVM\n  with the `linear_model.SGDOneClassSVM` combined with kernel approximation."
  },
  {
    "filename": "outlier_detection.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\outlier_detection.rst.txt",
    "id": "outlier_detection.rst.txt_chunk_5",
    "header": "Outlier Detection",
    "text": "Outlier Detection\n=================\n\nOutlier detection is similar to novelty detection in the sense that\nthe goal is to separate a core of regular observations from some\npolluting ones, called *outliers*. Yet, in the case of outlier\ndetection, we don't have a clean data set representing the population\nof regular observations that can be used to train any tool."
  },
  {
    "filename": "outlier_detection.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\outlier_detection.rst.txt",
    "id": "outlier_detection.rst.txt_chunk_6",
    "header": "Fitting an elliptic envelope",
    "text": "Fitting an elliptic envelope\n----------------------------\n\nOne common way of performing outlier detection is to assume that the\nregular data come from a known distribution (e.g. data are Gaussian\ndistributed). From this assumption, we generally try to define the\n\"shape\" of the data, and can define outlying observations as\nobservations which stand far enough from the fit shape.\n\nThe scikit-learn provides an object\n:class:`covariance.EllipticEnvelope` that fits a robust covariance\nestimate to the data, and thus fits an ellipse to the central data\npoints, ignoring points outside the central mode.\n\nFor instance, assuming that the inlier data are Gaussian distributed, it\nwill estimate the inlier location and covariance in a robust way (i.e.\nwithout being influenced by outliers). The Mahalanobis distances\nobtained from this estimate are used to derive a measure of outlyingness.\nThis strategy is illustrated below.\n\n.. figure:: ../auto_examples/covariance/images/sphx_glr_plot_mahalanobis_distances_001.png\n   :target: ../auto_examples/covariance/plot_mahalanobis_distances.html\n   :align: center\n   :scale: 75%\n\n.. rubric:: Examples\n\n* See :ref:`sphx_glr_auto_examples_covariance_plot_mahalanobis_distances.py` for\n  an illustration of the difference between using a standard\n  (:class:`covariance.EmpiricalCovariance`) or a robust estimate\n  (:class:`covariance.MinCovDet`) of location and covariance to\n  assess the degree of outlyingness of an observation.\n\n* See :ref:`sphx_glr_auto_examples_applications_plot_outlier_detection_wine.py`\n  for an example of robust covariance estimation on a real data set.\n\n\n.. rubric:: References\n\n* Rousseeuw, P.J., Van Driessen, K. \"A fast algorithm for the minimum\n  covariance determinant estimator\" Technometrics 41(3), 212 (1999)\n\n.. _isolation_forest:"
  },
  {
    "filename": "outlier_detection.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\outlier_detection.rst.txt",
    "id": "outlier_detection.rst.txt_chunk_7",
    "header": "Isolation Forest",
    "text": "Isolation Forest\n----------------------------\n\nOne efficient way of performing outlier detection in high-dimensional datasets\nis to use random forests.\nThe :class:`ensemble.IsolationForest` 'isolates' observations by randomly selecting\na feature and then randomly selecting a split value between the maximum and\nminimum values of the selected feature.\n\nSince recursive partitioning can be represented by a tree structure, the\nnumber of splittings required to isolate a sample is equivalent to the path\nlength from the root node to the terminating node.\n\nThis path length, averaged over a forest of such random trees, is a\nmeasure of normality and our decision function.\n\nRandom partitioning produces noticeably shorter paths for anomalies.\nHence, when a forest of random trees collectively produce shorter path\nlengths for particular samples, they are highly likely to be anomalies.\n\nThe implementation of :class:`ensemble.IsolationForest` is based on an ensemble\nof :class:`tree.ExtraTreeRegressor`. Following Isolation Forest original paper,\nthe maximum depth of each tree is set to :math:`\\lceil \\log_2(n) \\rceil` where\n:math:`n` is the number of samples used to build the tree (see (Liu et al.,\n2008) for more details).\n\nThis algorithm is illustrated below.\n\n.. figure:: ../auto_examples/ensemble/images/sphx_glr_plot_isolation_forest_003.png\n   :target: ../auto_examples/ensemble/plot_isolation_forest.html\n   :align: center\n   :scale: 75%\n\n.. _iforest_warm_start:\n\nThe :class:`ensemble.IsolationForest` supports ``warm_start=True`` which\nallows you to add more trees to an already fitted model::\n\n  >>> from sklearn.ensemble import IsolationForest\n  >>> import numpy as np\n  >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [0, 0], [-20, 50], [3, 5]])\n  >>> clf = IsolationForest(n_estimators=10, warm_start=True)\n  >>> clf.fit(X)  # fit 10 trees  # doctest: +SKIP\n  >>> clf.set_params(n_estimators=20)  # add 10 more trees  # doctest: +SKIP\n  >>> clf.fit(X)  # fit the added trees  # doctest: +SKIP\n\n.. rubric:: Examples\n\n* See :ref:`sphx_glr_auto_examples_ensemble_plot_isolation_forest.py` for\n  an illustration of the use of IsolationForest.\n\n* See :ref:`sphx_glr_auto_examples_miscellaneous_plot_anomaly_comparison.py`\n  for a comparison of :class:`ensemble.IsolationForest` with\n  :class:`neighbors.LocalOutlierFactor`,\n  :class:`svm.OneClassSVM` (tuned to perform like an outlier detection\n  method), :class:`linear_model.SGDOneClassSVM`, and a covariance-based\n  outlier detection with :class:`covariance.EllipticEnvelope`.\n\n.. rubric:: References\n\n* Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation forest.\"\n  Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on.\n\n.. _local_outlier_factor:"
  },
  {
    "filename": "outlier_detection.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\outlier_detection.rst.txt",
    "id": "outlier_detection.rst.txt_chunk_8",
    "header": "Local Outlier Factor",
    "text": "Local Outlier Factor\n--------------------\nAnother efficient way to perform outlier detection on moderately high dimensional\ndatasets is to use the Local Outlier Factor (LOF) algorithm.\n\nThe :class:`neighbors.LocalOutlierFactor` (LOF) algorithm computes a score\n(called local outlier factor) reflecting the degree of abnormality of the\nobservations.\nIt measures the local density deviation of a given data point with respect to\nits neighbors. The idea is to detect the samples that have a substantially\nlower density than their neighbors.\n\nIn practice the local density is obtained from the k-nearest neighbors.\nThe LOF score of an observation is equal to the ratio of the\naverage local density of its k-nearest neighbors, and its own local density:\na normal instance is expected to have a local density similar to that of its\nneighbors, while abnormal data are expected to have much smaller local density.\n\nThe number k of neighbors considered, (alias parameter `n_neighbors`) is\ntypically chosen 1) greater than the minimum number of objects a cluster has to\ncontain, so that other objects can be local outliers relative to this cluster,\nand 2) smaller than the maximum number of close by objects that can potentially\nbe local outliers. In practice, such information is generally not available, and\ntaking `n_neighbors=20` appears to work well in general. When the proportion of\noutliers is high (i.e. greater than 10 \\%, as in the example below),\n`n_neighbors` should be greater (`n_neighbors=35` in the example below).\n\nThe strength of the LOF algorithm is that it takes both local and global\nproperties of datasets into consideration: it can perform well even in datasets\nwhere abnormal samples have different underlying densities.\nThe question is not, how isolated the sample is, but how isolated it is\nwith respect to the surrounding neighborhood.\n\nWhen applying LOF for outlier detection, there are no ``predict``,\n``decision_function`` and ``score_samples`` methods but only a ``fit_predict``\nmethod. The scores of abnormality of the training samples are accessible\nthrough the ``negative_outlier_factor_`` attribute.\nNote that ``predict``, ``decision_function`` and ``score_samples`` can be used\non new unseen data when LOF is applied for novelty detection, i.e. when the\n``novelty`` parameter is set to ``True``, but the result of ``predict`` may\ndiffer from that of ``fit_predict``. See :ref:`novelty_with_lof`.\n\n\nThis strategy is illustrated below.\n\n.. figure:: ../auto_examples/neighbors/images/sphx_glr_plot_lof_outlier_detection_001.png\n   :target: ../auto_examples/neighbors/plot_lof_outlier_detection.html\n   :align: center\n   :scale: 75%\n\n.. rubric:: Examples\n\n* See :ref:`sphx_glr_auto_examples_neighbors_plot_lof_outlier_detection.py`\n  for an illustration of the use of :class:`neighbors.LocalOutlierFactor`.\n\n* See :ref:`sphx_glr_auto_examples_miscellaneous_plot_anomaly_comparison.py`\n  for a comparison with other anomaly detection methods.\n\n.. rubric:: References\n\n* Breunig, Kriegel, Ng, and Sander (2000)\n  `LOF: identifying density-based local outliers.\n  <https://www.dbs.ifi.lmu.de/Publikationen/Papers/LOF.pdf>`_\n  Proc. ACM SIGMOD\n\n.. _novelty_with_lof:"
  },
  {
    "filename": "outlier_detection.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\outlier_detection.rst.txt",
    "id": "outlier_detection.rst.txt_chunk_9",
    "header": "Novelty detection with Local Outlier Factor",
    "text": "Novelty detection with Local Outlier Factor\n===========================================\n\nTo use :class:`neighbors.LocalOutlierFactor` for novelty detection, i.e.\npredict labels or compute the score of abnormality of new unseen data, you\nneed to instantiate the estimator with the ``novelty`` parameter\nset to ``True`` before fitting the estimator::\n\n  lof = LocalOutlierFactor(novelty=True)\n  lof.fit(X_train)\n\nNote that ``fit_predict`` is not available in this case to avoid inconsistencies.\n\n.. warning:: **Novelty detection with Local Outlier Factor**\n\n  When ``novelty`` is set to ``True`` be aware that you must only use\n  ``predict``, ``decision_function`` and ``score_samples`` on new unseen data\n  and not on the training samples as this would lead to wrong results.\n  I.e., the result of ``predict`` will not be the same as ``fit_predict``.\n  The scores of abnormality of the training samples are always accessible\n  through the ``negative_outlier_factor_`` attribute.\n\nNovelty detection with :class:`neighbors.LocalOutlierFactor` is illustrated below\n(see :ref:`sphx_glr_auto_examples_neighbors_plot_lof_novelty_detection.py`).\n\n.. figure:: ../auto_examples/neighbors/images/sphx_glr_plot_lof_novelty_detection_001.png\n    :target: ../auto_examples/neighbors/plot_lof_novelty_detection.html\n    :align: center\n    :scale: 75%"
  },
  {
    "filename": "partial_dependence.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\partial_dependence.rst.txt",
    "id": "partial_dependence.rst.txt_chunk_0",
    "header": "",
    "text": ".. _partial_dependence:\n\n==============================================================="
  },
  {
    "filename": "partial_dependence.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\partial_dependence.rst.txt",
    "id": "partial_dependence.rst.txt_chunk_1",
    "header": "Partial Dependence and Individual Conditional Expectation plots",
    "text": "Partial Dependence and Individual Conditional Expectation plots\n===============================================================\n\n.. currentmodule:: sklearn.inspection\n\nPartial dependence plots (PDP) and individual conditional expectation (ICE)\nplots can be used to visualize and analyze interaction between the target\nresponse [1]_ and a set of input features of interest.\n\nBoth PDPs [H2009]_ and ICEs [G2015]_ assume that the input features of interest\nare independent from the complement features, and this assumption is often\nviolated in practice. Thus, in the case of correlated features, we will\ncreate absurd data points to compute the PDP/ICE [M2019]_."
  },
  {
    "filename": "partial_dependence.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\partial_dependence.rst.txt",
    "id": "partial_dependence.rst.txt_chunk_2",
    "header": "Partial dependence plots",
    "text": "Partial dependence plots\n========================\n\nPartial dependence plots (PDP) show the dependence between the target response\nand a set of input features of interest, marginalizing over the values\nof all other input features (the 'complement' features). Intuitively, we can\ninterpret the partial dependence as the expected target response as a\nfunction of the input features of interest.\n\nDue to the limits of human perception, the size of the set of input features of\ninterest must be small (usually, one or two) thus the input features of interest\nare usually chosen among the most important features.\n\nThe figure below shows two one-way and one two-way partial dependence plots for\nthe bike sharing dataset, with a\n:class:`~sklearn.ensemble.HistGradientBoostingRegressor`:\n\n.. figure:: ../auto_examples/inspection/images/sphx_glr_plot_partial_dependence_006.png\n   :target: ../auto_examples/inspection/plot_partial_dependence.html\n   :align: center\n   :scale: 70\n\nOne-way PDPs tell us about the interaction between the target response and an input\nfeature of interest (e.g. linear, non-linear). The left plot in the above figure\nshows the effect of the temperature on the number of bike rentals; we can clearly see\nthat a higher temperature is related with a higher number of bike rentals. Similarly, we\ncould analyze the effect of the humidity on the number of bike rentals (middle plot).\nThus, these interpretations are marginal, considering a feature at a time.\n\nPDPs with two input features of interest show the interactions among the two features.\nFor example, the two-variable PDP in the above figure shows the dependence of the number\nof bike rentals on joint values of temperature and humidity. We can clearly see an\ninteraction between the two features: with a temperature higher than 20 degrees Celsius,\nmainly the humidity has a strong impact on the number of bike rentals. For lower\ntemperatures, both the temperature and the humidity have an impact on the number of bike\nrentals.\n\nThe :mod:`sklearn.inspection` module provides a convenience function\n:func:`~PartialDependenceDisplay.from_estimator` to create one-way and two-way partial\ndependence plots. In the below example we show how to create a grid of\npartial dependence plots: two one-way PDPs for the features ``0`` and ``1``\nand a two-way PDP between the two features::\n\n    >>> from sklearn.datasets import make_hastie_10_2\n    >>> from sklearn.ensemble import GradientBoostingClassifier\n    >>> from sklearn.inspection import PartialDependenceDisplay\n\n    >>> X, y = make_hastie_10_2(random_state=0)\n    >>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n    ...     max_depth=1, random_state=0).fit(X, y)\n    >>> features = [0, 1, (0, 1)]\n    >>> PartialDependenceDisplay.from_estimator(clf, X, features)\n    <...>\n\nYou can access the newly created figure and Axes objects using ``plt.gcf()``\nand ``plt.gca()``.\n\nTo make a partial dependence plot with categorical features, you need to specify\nwhich features are categorical using the parameter `categorical_features`. This\nparameter takes a list of indices, names of the categorical features or a boolean\nmask. The graphical representation of partial dependence for categorical features is\na bar plot or a 2D heatmap.\n\n.. dropdown:: PDPs for multi-class classification\n\n    For multi-class classification, you need to set the class label for which\n    the PDPs should be created via the ``target`` argument::\n\n        >>> from sklearn.datasets import load_iris\n        >>> iris = load_iris()\n        >>> mc_clf = GradientBoostingClassifier(n_estimators=10,\n        ...     max_depth=1).fit(iris.data, iris.target)\n        >>> features = [3, 2, (3, 2)]\n        >>> PartialDependenceDisplay.from_estimator(mc_clf, X, features, target=0)\n        <...>\n\n    The same parameter ``target`` is used to specify the target in multi-output\n    regression settings.\n\nIf you need the raw values of the partial dependence function rather than\nthe plots, you can use the\n:func:`sklearn.inspection.partial_dependence` function::\n\n    >>> from sklearn.inspection import partial_dependence\n\n    >>> results = partial_dependence(clf, X, [0])\n    >>> results[\"average\"]\n    array([[ 2.466...,  2.466..., ...\n    >>> results[\"grid_values\"]\n    [array([-1.624..., -1.592..., ...\n\nThe values at which the partial dependence should be evaluated are directly\ngenerated from ``X``. For 2-way partial dependence, a 2D-grid of values is\ngenerated. The ``values`` field returned by\n:func:`sklearn.inspection.partial_dependence` gives the actual values\nused in the grid for each input feature of interest. They also correspond to\nthe axis of the plots.\n\n.. _individual_conditional:"
  },
  {
    "filename": "partial_dependence.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\partial_dependence.rst.txt",
    "id": "partial_dependence.rst.txt_chunk_3",
    "header": "Individual conditional expectation (ICE) plot",
    "text": "Individual conditional expectation (ICE) plot\n=============================================\n\nSimilar to a PDP, an individual conditional expectation (ICE) plot\nshows the dependence between the target function and an input feature of\ninterest. However, unlike a PDP, which shows the average effect of the input\nfeature, an ICE plot visualizes the dependence of the prediction on a\nfeature for each sample separately with one line per sample.\nDue to the limits of human perception, only one input feature of interest is\nsupported for ICE plots.\n\nThe figures below show two ICE plots for the bike sharing dataset,\nwith a :class:`~sklearn.ensemble.HistGradientBoostingRegressor`. The figures plot\nthe corresponding PD line overlaid on ICE lines.\n\n.. figure:: ../auto_examples/inspection/images/sphx_glr_plot_partial_dependence_004.png\n   :target: ../auto_examples/inspection/plot_partial_dependence.html\n   :align: center\n   :scale: 70\n\nWhile the PDPs are good at showing the average effect of the target features,\nthey can obscure a heterogeneous relationship created by interactions.\nWhen interactions are present the ICE plot will provide many more insights.\nFor example, we see that the ICE for the temperature feature gives us some\nadditional information: some of the ICE lines are flat while some others\nshow a decrease of the dependence for temperature above 35 degrees Celsius.\nWe observe a similar pattern for the humidity feature: some of the ICE\nlines show a sharp decrease when the humidity is above 80%.\n\nThe :mod:`sklearn.inspection` module's :meth:`PartialDependenceDisplay.from_estimator`\nconvenience function can be used to create ICE plots by setting\n``kind='individual'``. In the example below, we show how to create a grid of\nICE plots:\n\n    >>> from sklearn.datasets import make_hastie_10_2\n    >>> from sklearn.ensemble import GradientBoostingClassifier\n    >>> from sklearn.inspection import PartialDependenceDisplay\n\n    >>> X, y = make_hastie_10_2(random_state=0)\n    >>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n    ...     max_depth=1, random_state=0).fit(X, y)\n    >>> features = [0, 1]\n    >>> PartialDependenceDisplay.from_estimator(clf, X, features,\n    ...     kind='individual')\n    <...>\n\nIn ICE plots it might not be easy to see the average effect of the input\nfeature of interest. Hence, it is recommended to use ICE plots alongside\nPDPs. They can be plotted together with\n``kind='both'``.\n\n    >>> PartialDependenceDisplay.from_estimator(clf, X, features,\n    ...     kind='both')\n    <...>\n\nIf there are too many lines in an ICE plot, it can be difficult to see\ndifferences between individual samples and interpret the model. Centering the\nICE at the first value on the x-axis, produces centered Individual Conditional\nExpectation (cICE) plots [G2015]_. This puts emphasis on the divergence of\nindividual conditional expectations from the mean line, thus making it easier\nto explore heterogeneous relationships. cICE plots can be plotted by setting\n`centered=True`:\n\n    >>> PartialDependenceDisplay.from_estimator(clf, X, features,\n    ...     kind='both', centered=True)\n    <...>"
  },
  {
    "filename": "partial_dependence.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\partial_dependence.rst.txt",
    "id": "partial_dependence.rst.txt_chunk_4",
    "header": "Mathematical Definition",
    "text": "Mathematical Definition\n=======================\n\nLet :math:`X_S` be the set of input features of interest (i.e. the `features`\nparameter) and let :math:`X_C` be its complement.\n\nThe partial dependence of the response :math:`f` at a point :math:`x_S` is\ndefined as:\n\n.. math::\n\n    pd_{X_S}(x_S) &\\overset{def}{=} \\mathbb{E}_{X_C}\\left[ f(x_S, X_C) \\right]\\\\\n                  &= \\int f(x_S, x_C) p(x_C) dx_C,\n\nwhere :math:`f(x_S, x_C)` is the response function (:term:`predict`,\n:term:`predict_proba` or :term:`decision_function`) for a given sample whose\nvalues are defined by :math:`x_S` for the features in :math:`X_S`, and by\n:math:`x_C` for the features in :math:`X_C`. Note that :math:`x_S` and\n:math:`x_C` may be tuples.\n\nComputing this integral for various values of :math:`x_S` produces a PDP plot\nas above. An ICE line is defined as a single :math:`f(x_{S}, x_{C}^{(i)})`\nevaluated at :math:`x_{S}`."
  },
  {
    "filename": "partial_dependence.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\partial_dependence.rst.txt",
    "id": "partial_dependence.rst.txt_chunk_5",
    "header": "Computation methods",
    "text": "Computation methods\n===================\n\nThere are two main methods to approximate the integral above, namely the\n`'brute'` and `'recursion'` methods. The `method` parameter controls which method\nto use.\n\nThe `'brute'` method is a generic method that works with any estimator. Note that\ncomputing ICE plots is only supported with the `'brute'` method. It\napproximates the above integral by computing an average over the data `X`:\n\n.. math::\n\n    pd_{X_S}(x_S) \\approx \\frac{1}{n_\\text{samples}} \\sum_{i=1}^n f(x_S, x_C^{(i)}),\n\nwhere :math:`x_C^{(i)}` is the value of the i-th sample for the features in\n:math:`X_C`. For each value of :math:`x_S`, this method requires a full pass\nover the dataset `X` which is computationally intensive.\n\nEach of the :math:`f(x_{S}, x_{C}^{(i)})` corresponds to one ICE line evaluated\nat :math:`x_{S}`. Computing this for multiple values of :math:`x_{S}`, one\nobtains a full ICE line. As one can see, the average of the ICE lines\ncorresponds to the partial dependence line.\n\nThe `'recursion'` method is faster than the `'brute'` method, but it is only\nsupported for PDP plots by some tree-based estimators. It is computed as\nfollows. For a given point :math:`x_S`, a weighted tree traversal is performed:\nif a split node involves an input feature of interest, the corresponding left\nor right branch is followed; otherwise both branches are followed, each branch\nbeing weighted by the fraction of training samples that entered that branch.\nFinally, the partial dependence is given by a weighted average of all the\nvisited leaves' values.\n\nWith the `'brute'` method, the parameter `X` is used both for generating the\ngrid of values :math:`x_S` and the complement feature values :math:`x_C`.\nHowever with the 'recursion' method, `X` is only used for the grid values:\nimplicitly, the :math:`x_C` values are those of the training data.\n\nBy default, the `'recursion'` method is used for plotting PDPs on tree-based\nestimators that support it, and 'brute' is used for the rest.\n\n.. _pdp_method_differences:\n\n.. note::\n\n    While both methods should be close in general, they might differ in some\n    specific settings. The `'brute'` method assumes the existence of the\n    data points :math:`(x_S, x_C^{(i)})`. When the features are correlated,\n    such artificial samples may have a very low probability mass. The `'brute'`\n    and `'recursion'` methods will likely disagree regarding the value of the\n    partial dependence, because they will treat these unlikely\n    samples differently. Remember, however, that the primary assumption for\n    interpreting PDPs is that the features should be independent.\n\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_inspection_plot_partial_dependence.py`\n\n.. rubric:: Footnotes\n\n.. [1] For classification, the target response may be the probability of a\n   class (the positive class for binary classification), or the decision\n   function.\n\n.. rubric:: References\n\n.. [H2009] T. Hastie, R. Tibshirani and J. Friedman,\n    `The Elements of Statistical Learning\n    <https://web.stanford.edu/~hastie/ElemStatLearn//>`_,\n    Second Edition, Section 10.13.2, Springer, 2009.\n\n.. [M2019] C. Molnar,\n    `Interpretable Machine Learning\n    <https://christophm.github.io/interpretable-ml-book/>`_,\n    Section 5.1, 2019.\n\n.. [G2015] :arxiv:`A. Goldstein, A. Kapelner, J. Bleich, and E. Pitkin,\n    \"Peeking Inside the Black Box: Visualizing Statistical\n    Learning With Plots of Individual Conditional Expectation\"\n    Journal of Computational and Graphical Statistics,\n    24(1): 44-65, Springer, 2015. <1309.6392>`"
  },
  {
    "filename": "permutation_importance.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\permutation_importance.rst.txt",
    "id": "permutation_importance.rst.txt_chunk_0",
    "header": "",
    "text": ".. _permutation_importance:"
  },
  {
    "filename": "permutation_importance.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\permutation_importance.rst.txt",
    "id": "permutation_importance.rst.txt_chunk_1",
    "header": "Permutation feature importance",
    "text": "Permutation feature importance\n==============================\n\n.. currentmodule:: sklearn.inspection\n\nPermutation feature importance is a model inspection technique that measures the\ncontribution of each feature to a :term:`fitted` model's statistical performance\non a given tabular dataset. This technique is particularly useful for non-linear\nor opaque :term:`estimators`, and involves randomly shuffling the values of a\nsingle feature and observing the resulting degradation of the model's score\n[1]_. By breaking the relationship between the feature and the target, we\ndetermine how much the model relies on such particular feature.\n\nIn the following figures, we observe the effect of permuting features on the correlation\nbetween the feature and the target and consequently on the model's statistical\nperformance.\n\n.. image:: ../images/permuted_predictive_feature.png\n   :align: center\n\n.. image:: ../images/permuted_non_predictive_feature.png\n   :align: center\n\nOn the top figure, we observe that permuting a predictive feature breaks the\ncorrelation between the feature and the target, and consequently the model's\nstatistical performance decreases. On the bottom figure, we observe that permuting\na non-predictive feature does not significantly degrade the model's statistical\nperformance.\n\nOne key advantage of permutation feature importance is that it is\nmodel-agnostic, i.e. it can be applied to any fitted estimator. Moreover, it can\nbe calculated multiple times with different permutations of the feature, further\nproviding a measure of the variance in the estimated feature importances for the\nspecific trained model.\n\nThe figure below shows the permutation feature importance of a\n:class:`~sklearn.ensemble.RandomForestClassifier` trained on an augmented\nversion of the titanic dataset that contains a `random_cat` and a `random_num`\nfeatures, i.e. a categorical and a numerical feature that are not correlated in\nany way with the target variable:\n\n.. figure:: ../auto_examples/inspection/images/sphx_glr_plot_permutation_importance_002.png\n   :target: ../auto_examples/inspection/plot_permutation_importance.html\n   :align: center\n   :scale: 70\n\n.. warning::\n\n  Features that are deemed of **low importance for a bad model** (low\n  cross-validation score) could be **very important for a good model**.\n  Therefore it is always important to evaluate the predictive power of a model\n  using a held-out set (or better with cross-validation) prior to computing\n  importances. Permutation importance does not reflect the intrinsic\n  predictive value of a feature by itself but **how important this feature is\n  for a particular model**.\n\nThe :func:`permutation_importance` function calculates the feature importance\nof :term:`estimators` for a given dataset. The ``n_repeats`` parameter sets the\nnumber of times a feature is randomly shuffled and returns a sample of feature\nimportances.\n\nLet's consider the following trained regression model::\n\n  >>> from sklearn.datasets import load_diabetes\n  >>> from sklearn.model_selection import train_test_split\n  >>> from sklearn.linear_model import Ridge\n  >>> diabetes = load_diabetes()\n  >>> X_train, X_val, y_train, y_val = train_test_split(\n  ...     diabetes.data, diabetes.target, random_state=0)\n  ...\n  >>> model = Ridge(alpha=1e-2).fit(X_train, y_train)\n  >>> model.score(X_val, y_val)\n  0.356...\n\nIts validation performance, measured via the :math:`R^2` score, is\nsignificantly larger than the chance level. This makes it possible to use the\n:func:`permutation_importance` function to probe which features are most\npredictive::\n\n  >>> from sklearn.inspection import permutation_importance\n  >>> r = permutation_importance(model, X_val, y_val,\n  ...                            n_repeats=30,\n  ...                            random_state=0)\n  ...\n  >>> for i in r.importances_mean.argsort()[::-1]:\n  ...     if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n  ...         print(f\"{diabetes.feature_names[i]:<8}\"\n  ...               f\"{r.importances_mean[i]:.3f}\"\n  ...               f\" +/- {r.importances_std[i]:.3f}\")\n  ...\n  s5      0.204 +/- 0.050\n  bmi     0.176 +/- 0.048\n  bp      0.088 +/- 0.033\n  sex     0.056 +/- 0.023\n\nNote that the importance values for the top features represent a large\nfraction of the reference score of 0.356.\n\nPermutation importances can be computed either on the training set or on a\nheld-out testing or validation set. Using a held-out set makes it possible to\nhighlight which features contribute the most to the generalization power of the\ninspected model. Features that are important on the training set but not on the\nheld-out set might cause the model to overfit.\n\nThe permutation feature importance depends on the score function that is\nspecified with the `scoring` argument. This argument accepts multiple scorers,\nwhich is more computationally efficient than sequentially calling\n:func:`permutation_importance` several times with a different scorer, as it\nreuses model predictions.\n\n.. dropdown:: Example of permutation feature importance using multiple scorers\n\n  In the example below we use a list of metrics, but more input formats are\n  possible, as documented in :ref:`multimetric_scoring`.\n\n    >>> scoring = ['r2', 'neg_mean_absolute_percentage_error', 'neg_mean_squared_error']\n    >>> r_multi = permutation_importance(\n    ...     model, X_val, y_val, n_repeats=30, random_state=0, scoring=scoring)\n    ...\n    >>> for metric in r_multi:\n    ...     print(f\"{metric}\")\n    ...     r = r_multi[metric]\n    ...     for i in r.importances_mean.argsort()[::-1]:\n    ...         if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n    ...             print(f\"    {diabetes.feature_names[i]:<8}\"\n    ...                   f\"{r.importances_mean[i]:.3f}\"\n    ...                   f\" +/- {r.importances_std[i]:.3f}\")\n    ...\n    r2\n        s5      0.204 +/- 0.050\n        bmi     0.176 +/- 0.048\n        bp      0.088 +/- 0.033\n        sex     0.056 +/- 0.023\n    neg_mean_absolute_percentage_error\n        s5      0.081 +/- 0.020\n        bmi     0.064 +/- 0.015\n        bp      0.029 +/- 0.010\n    neg_mean_squared_error\n        s5      1013.866 +/- 246.445\n        bmi     872.726 +/- 240.298\n        bp      438.663 +/- 163.022\n        sex     277.376 +/- 115.123\n\n  The ranking of the features is approximately the same for different metrics even\n  if the scales of the importance values are very different. However, this is not\n  guaranteed and different metrics might lead to significantly different feature\n  importances, in particular for models trained for imbalanced classification problems,\n  for which **the choice of the classification metric can be critical**."
  },
  {
    "filename": "permutation_importance.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\permutation_importance.rst.txt",
    "id": "permutation_importance.rst.txt_chunk_2",
    "header": "Outline of the permutation importance algorithm",
    "text": "Outline of the permutation importance algorithm\n-----------------------------------------------\n\n- Inputs: fitted predictive model :math:`m`, tabular dataset (training or\n  validation) :math:`D`.\n- Compute the reference score :math:`s` of the model :math:`m` on data\n  :math:`D` (for instance the accuracy for a classifier or the :math:`R^2` for\n  a regressor).\n- For each feature :math:`j` (column of :math:`D`):\n\n  - For each repetition :math:`k` in :math:`{1, ..., K}`:\n\n    - Randomly shuffle column :math:`j` of dataset :math:`D` to generate a\n      corrupted version of the data named :math:`\\tilde{D}_{k,j}`.\n    - Compute the score :math:`s_{k,j}` of model :math:`m` on corrupted data\n      :math:`\\tilde{D}_{k,j}`.\n\n  - Compute importance :math:`i_j` for feature :math:`f_j` defined as:\n\n    .. math:: i_j = s - \\frac{1}{K} \\sum_{k=1}^{K} s_{k,j}"
  },
  {
    "filename": "permutation_importance.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\permutation_importance.rst.txt",
    "id": "permutation_importance.rst.txt_chunk_3",
    "header": "Relation to impurity-based importance in trees",
    "text": "Relation to impurity-based importance in trees\n----------------------------------------------\n\nTree-based models provide an alternative measure of :ref:`feature importances\nbased on the mean decrease in impurity <random_forest_feature_importance>`\n(MDI). Impurity is quantified by the splitting criterion of the decision trees\n(Gini, Log Loss or Mean Squared Error). However, this method can give high\nimportance to features that may not be predictive on unseen data when the model\nis overfitting. Permutation-based feature importance, on the other hand, avoids\nthis issue, since it can be computed on unseen data.\n\nFurthermore, impurity-based feature importance for trees is **strongly\nbiased** and **favor high cardinality features** (typically numerical features)\nover low cardinality features such as binary features or categorical variables\nwith a small number of possible categories.\n\nPermutation-based feature importances do not exhibit such a bias. Additionally,\nthe permutation feature importance may be computed with any performance metric\non the model predictions and can be used to analyze any model class (not just\ntree-based models).\n\nThe following example highlights the limitations of impurity-based feature\nimportance in contrast to permutation-based feature importance:\n:ref:`sphx_glr_auto_examples_inspection_plot_permutation_importance.py`."
  },
  {
    "filename": "permutation_importance.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\permutation_importance.rst.txt",
    "id": "permutation_importance.rst.txt_chunk_4",
    "header": "Misleading values on strongly correlated features",
    "text": "Misleading values on strongly correlated features\n-------------------------------------------------\n\nWhen two features are correlated and one of the features is permuted, the model\nstill has access to the latter through its correlated feature. This results in a\nlower reported importance value for both features, though they might *actually*\nbe important.\n\nThe figure below shows the permutation feature importance of a\n:class:`~sklearn.ensemble.RandomForestClassifier` trained using the\n:ref:`breast_cancer_dataset`, which contains strongly correlated features. A\nnaive interpretation would suggest that all features are unimportant:\n\n.. figure:: ../auto_examples/inspection/images/sphx_glr_plot_permutation_importance_multicollinear_002.png\n   :target: ../auto_examples/inspection/plot_permutation_importance_multicollinear.html\n   :align: center\n   :scale: 70\n\nOne way to handle the issue is to cluster features that are correlated and only\nkeep one feature from each cluster.\n\n.. figure:: ../auto_examples/inspection/images/sphx_glr_plot_permutation_importance_multicollinear_004.png\n   :target: ../auto_examples/inspection/plot_permutation_importance_multicollinear.html\n   :align: center\n   :scale: 70\n\nFor more details on such strategy, see the example\n:ref:`sphx_glr_auto_examples_inspection_plot_permutation_importance_multicollinear.py`.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_inspection_plot_permutation_importance.py`\n* :ref:`sphx_glr_auto_examples_inspection_plot_permutation_importance_multicollinear.py`\n\n.. rubric:: References\n\n.. [1] L. Breiman, :doi:`\"Random Forests\" <10.1023/A:1010933404324>`,\n  Machine Learning, 45(1), 5-32, 2001."
  },
  {
    "filename": "pipeline.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\pipeline.rst.txt",
    "id": "pipeline.rst.txt_chunk_0",
    "header": ":orphan:",
    "text": ":orphan:\n\n.. raw:: html\n\n    <meta http-equiv=\"refresh\" content=\"1; url=./compose.html\" />\n    <script>\n      window.location.href = \"./compose.html\";\n    </script>\n\nThis content is now at :ref:`combining_estimators`."
  },
  {
    "filename": "preprocessing.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\preprocessing.rst.txt",
    "id": "preprocessing.rst.txt_chunk_0",
    "header": ".. _preprocessing:",
    "text": ".. _preprocessing:\n\n=================="
  },
  {
    "filename": "preprocessing.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\preprocessing.rst.txt",
    "id": "preprocessing.rst.txt_chunk_1",
    "header": "Preprocessing data",
    "text": "Preprocessing data\n==================\n\n.. currentmodule:: sklearn.preprocessing\n\nThe ``sklearn.preprocessing`` package provides several common\nutility functions and transformer classes to change raw feature vectors\ninto a representation that is more suitable for the downstream estimators.\n\nIn general, many learning algorithms such as linear models benefit from standardization of the data set\n(see :ref:`sphx_glr_auto_examples_preprocessing_plot_scaling_importance.py`).\nIf some outliers are present in the set, robust scalers or other transformers can\nbe more appropriate. The behaviors of the different scalers, transformers, and\nnormalizers on a dataset containing marginal outliers are highlighted in\n:ref:`sphx_glr_auto_examples_preprocessing_plot_all_scaling.py`.\n\n\n.. _preprocessing_scaler:"
  },
  {
    "filename": "preprocessing.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\preprocessing.rst.txt",
    "id": "preprocessing.rst.txt_chunk_2",
    "header": "Standardization, or mean removal and variance scaling",
    "text": "Standardization, or mean removal and variance scaling\n=====================================================\n\n**Standardization** of datasets is a **common requirement for many\nmachine learning estimators** implemented in scikit-learn; they might behave\nbadly if the individual features do not more or less look like standard\nnormally distributed data: Gaussian with **zero mean and unit variance**.\n\nIn practice we often ignore the shape of the distribution and just\ntransform the data to center it by removing the mean value of each\nfeature, then scale it by dividing non-constant features by their\nstandard deviation.\n\nFor instance, many elements used in the objective function of\na learning algorithm (such as the RBF kernel of Support Vector\nMachines or the l1 and l2 regularizers of linear models) may assume that\nall features are centered around zero or have variance in the same\norder. If a feature has a variance that is orders of magnitude larger\nthan others, it might dominate the objective function and make the\nestimator unable to learn from other features correctly as expected.\n\n\nThe :mod:`~sklearn.preprocessing` module provides the\n:class:`StandardScaler` utility class, which is a quick and\neasy way to perform the following operation on an array-like\ndataset::\n\n  >>> from sklearn import preprocessing\n  >>> import numpy as np\n  >>> X_train = np.array([[ 1., -1.,  2.],\n  ...                     [ 2.,  0.,  0.],\n  ...                     [ 0.,  1., -1.]])\n  >>> scaler = preprocessing.StandardScaler().fit(X_train)\n  >>> scaler\n  StandardScaler()\n\n  >>> scaler.mean_\n  array([1., 0., 0.33])\n\n  >>> scaler.scale_\n  array([0.81, 0.81, 1.24])\n\n  >>> X_scaled = scaler.transform(X_train)\n  >>> X_scaled\n  array([[ 0.  , -1.22,  1.33 ],\n         [ 1.22,  0.  , -0.267],\n         [-1.22,  1.22, -1.06 ]])\n\n..\n        >>> import numpy as np\n        >>> print_options = np.get_printoptions()\n        >>> np.set_printoptions(suppress=True)\n\nScaled data has zero mean and unit variance::\n\n  >>> X_scaled.mean(axis=0)\n  array([0., 0., 0.])\n\n  >>> X_scaled.std(axis=0)\n  array([1., 1., 1.])\n\n..    >>> print_options = np.set_printoptions(print_options)\n\nThis class implements the ``Transformer`` API to compute the mean and\nstandard deviation on a training set so as to be able to later re-apply the\nsame transformation on the testing set. This class is hence suitable for\nuse in the early steps of a :class:`~sklearn.pipeline.Pipeline`::\n\n  >>> from sklearn.datasets import make_classification\n  >>> from sklearn.linear_model import LogisticRegression\n  >>> from sklearn.model_selection import train_test_split\n  >>> from sklearn.pipeline import make_pipeline\n  >>> from sklearn.preprocessing import StandardScaler\n\n  >>> X, y = make_classification(random_state=42)\n  >>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n  >>> pipe = make_pipeline(StandardScaler(), LogisticRegression())\n  >>> pipe.fit(X_train, y_train)  # apply scaling on training data\n  Pipeline(steps=[('standardscaler', StandardScaler()),\n                  ('logisticregression', LogisticRegression())])\n\n  >>> pipe.score(X_test, y_test)  # apply scaling on testing data, without leaking training data.\n  0.96\n\nIt is possible to disable either centering or scaling by either\npassing ``with_mean=False`` or ``with_std=False`` to the constructor\nof :class:`StandardScaler`."
  },
  {
    "filename": "preprocessing.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\preprocessing.rst.txt",
    "id": "preprocessing.rst.txt_chunk_3",
    "header": "Scaling features to a range",
    "text": "Scaling features to a range\n---------------------------\n\nAn alternative standardization is scaling features to\nlie between a given minimum and maximum value, often between zero and one,\nor so that the maximum absolute value of each feature is scaled to unit size.\nThis can be achieved using :class:`MinMaxScaler` or :class:`MaxAbsScaler`,\nrespectively.\n\nThe motivation to use this scaling includes robustness to very small\nstandard deviations of features and preserving zero entries in sparse data.\n\nHere is an example to scale a toy data matrix to the ``[0, 1]`` range::\n\n  >>> X_train = np.array([[ 1., -1.,  2.],\n  ...                     [ 2.,  0.,  0.],\n  ...                     [ 0.,  1., -1.]])\n  ...\n  >>> min_max_scaler = preprocessing.MinMaxScaler()\n  >>> X_train_minmax = min_max_scaler.fit_transform(X_train)\n  >>> X_train_minmax\n  array([[0.5       , 0.        , 1.        ],\n         [1.        , 0.5       , 0.33333333],\n         [0.        , 1.        , 0.        ]])\n\nThe same instance of the transformer can then be applied to some new test data\nunseen during the fit call: the same scaling and shifting operations will be\napplied to be consistent with the transformation performed on the train data::\n\n  >>> X_test = np.array([[-3., -1.,  4.]])\n  >>> X_test_minmax = min_max_scaler.transform(X_test)\n  >>> X_test_minmax\n  array([[-1.5       ,  0.        ,  1.66666667]])\n\nIt is possible to introspect the scaler attributes to find about the exact\nnature of the transformation learned on the training data::\n\n  >>> min_max_scaler.scale_\n  array([0.5       , 0.5       , 0.33])\n\n  >>> min_max_scaler.min_\n  array([0.        , 0.5       , 0.33])\n\nIf :class:`MinMaxScaler` is given an explicit ``feature_range=(min, max)`` the\nfull formula is::\n\n    X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n\n    X_scaled = X_std * (max - min) + min\n\n:class:`MaxAbsScaler` works in a very similar fashion, but scales in a way\nthat the training data lies within the range ``[-1, 1]`` by dividing through\nthe largest maximum value in each feature. It is meant for data\nthat is already centered at zero or sparse data.\n\nHere is how to use the toy data from the previous example with this scaler::\n\n  >>> X_train = np.array([[ 1., -1.,  2.],\n  ...                     [ 2.,  0.,  0.],\n  ...                     [ 0.,  1., -1.]])\n  ...\n  >>> max_abs_scaler = preprocessing.MaxAbsScaler()\n  >>> X_train_maxabs = max_abs_scaler.fit_transform(X_train)\n  >>> X_train_maxabs\n  array([[ 0.5, -1. ,  1. ],\n         [ 1. ,  0. ,  0. ],\n         [ 0. ,  1. , -0.5]])\n  >>> X_test = np.array([[ -3., -1.,  4.]])\n  >>> X_test_maxabs = max_abs_scaler.transform(X_test)\n  >>> X_test_maxabs\n  array([[-1.5, -1. ,  2. ]])\n  >>> max_abs_scaler.scale_\n  array([2.,  1.,  2.])"
  },
  {
    "filename": "preprocessing.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\preprocessing.rst.txt",
    "id": "preprocessing.rst.txt_chunk_4",
    "header": "Scaling sparse data",
    "text": "Scaling sparse data\n-------------------\nCentering sparse data would destroy the sparseness structure in the data, and\nthus rarely is a sensible thing to do. However, it can make sense to scale\nsparse inputs, especially if features are on different scales.\n\n:class:`MaxAbsScaler` was specifically designed for scaling\nsparse data, and is the recommended way to go about this.\nHowever, :class:`StandardScaler` can accept ``scipy.sparse``\nmatrices  as input, as long as ``with_mean=False`` is explicitly passed\nto the constructor. Otherwise a ``ValueError`` will be raised as\nsilently centering would break the sparsity and would often crash the\nexecution by allocating excessive amounts of memory unintentionally.\n:class:`RobustScaler` cannot be fitted to sparse inputs, but you can use\nthe ``transform`` method on sparse inputs.\n\nNote that the scalers accept both Compressed Sparse Rows and Compressed\nSparse Columns format (see ``scipy.sparse.csr_matrix`` and\n``scipy.sparse.csc_matrix``). Any other sparse input will be **converted to\nthe Compressed Sparse Rows representation**.  To avoid unnecessary memory\ncopies, it is recommended to choose the CSR or CSC representation upstream.\n\nFinally, if the centered data is expected to be small enough, explicitly\nconverting the input to an array using the ``toarray`` method of sparse matrices\nis another option."
  },
  {
    "filename": "preprocessing.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\preprocessing.rst.txt",
    "id": "preprocessing.rst.txt_chunk_5",
    "header": "Scaling data with outliers",
    "text": "Scaling data with outliers\n--------------------------\n\nIf your data contains many outliers, scaling using the mean and variance\nof the data is likely to not work very well. In these cases, you can use\n:class:`RobustScaler` as a drop-in replacement instead. It uses\nmore robust estimates for the center and range of your data.\n\n\n.. dropdown:: References\n\n  Further discussion on the importance of centering and scaling data is\n  available on this FAQ: `Should I normalize/standardize/rescale the data?\n  <http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html>`_\n\n.. dropdown:: Scaling vs Whitening\n\n  It is sometimes not enough to center and scale the features\n  independently, since a downstream model can further make some assumption\n  on the linear independence of the features.\n\n  To address this issue you can use :class:`~sklearn.decomposition.PCA` with\n  ``whiten=True`` to further remove the linear correlation across features.\n\n\n.. _kernel_centering:"
  },
  {
    "filename": "preprocessing.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\preprocessing.rst.txt",
    "id": "preprocessing.rst.txt_chunk_6",
    "header": "Centering kernel matrices",
    "text": "Centering kernel matrices\n-------------------------\n\nIf you have a kernel matrix of a kernel :math:`K` that computes a dot product\nin a feature space (possibly implicitly) defined by a function\n:math:`\\phi(\\cdot)`, a :class:`KernelCenterer` can transform the kernel matrix\nso that it contains inner products in the feature space defined by :math:`\\phi`\nfollowed by the removal of the mean in that space. In other words,\n:class:`KernelCenterer` computes the centered Gram matrix associated to a\npositive semidefinite kernel :math:`K`.\n\n.. dropdown:: Mathematical formulation\n\n  We can have a look at the mathematical formulation now that we have the\n  intuition. Let :math:`K` be a kernel matrix of shape `(n_samples, n_samples)`\n  computed from :math:`X`, a data matrix of shape `(n_samples, n_features)`,\n  during the `fit` step. :math:`K` is defined by\n\n  .. math::\n    K(X, X) = \\phi(X) . \\phi(X)^{T}\n\n  :math:`\\phi(X)` is a function mapping of :math:`X` to a Hilbert space. A\n  centered kernel :math:`\\tilde{K}` is defined as:\n\n  .. math::\n    \\tilde{K}(X, X) = \\tilde{\\phi}(X) . \\tilde{\\phi}(X)^{T}\n\n  where :math:`\\tilde{\\phi}(X)` results from centering :math:`\\phi(X)` in the\n  Hilbert space.\n\n  Thus, one could compute :math:`\\tilde{K}` by mapping :math:`X` using the\n  function :math:`\\phi(\\cdot)` and center the data in this new space. However,\n  kernels are often used because they allow some algebra calculations that\n  avoid computing explicitly this mapping using :math:`\\phi(\\cdot)`. Indeed, one\n  can implicitly center as shown in Appendix B in [Scholkopf1998]_:\n\n  .. math::\n    \\tilde{K} = K - 1_{\\text{n}_{samples}} K - K 1_{\\text{n}_{samples}} + 1_{\\text{n}_{samples}} K 1_{\\text{n}_{samples}}\n\n  :math:`1_{\\text{n}_{samples}}` is a matrix of `(n_samples, n_samples)` where\n  all entries are equal to :math:`\\frac{1}{\\text{n}_{samples}}`. In the\n  `transform` step, the kernel becomes :math:`K_{test}(X, Y)` defined as:\n\n  .. math::\n    K_{test}(X, Y) = \\phi(Y) . \\phi(X)^{T}\n\n  :math:`Y` is the test dataset of shape `(n_samples_test, n_features)` and thus\n  :math:`K_{test}` is of shape `(n_samples_test, n_samples)`. In this case,\n  centering :math:`K_{test}` is done as:\n\n  .. math::\n    \\tilde{K}_{test}(X, Y) = K_{test} - 1'_{\\text{n}_{samples}} K - K_{test} 1_{\\text{n}_{samples}} + 1'_{\\text{n}_{samples}} K 1_{\\text{n}_{samples}}\n\n  :math:`1'_{\\text{n}_{samples}}` is a matrix of shape\n  `(n_samples_test, n_samples)` where all entries are equal to\n  :math:`\\frac{1}{\\text{n}_{samples}}`.\n\n  .. rubric:: References\n\n  .. [Scholkopf1998] B. Sch\u00f6lkopf, A. Smola, and K.R. M\u00fcller,\n    `\"Nonlinear component analysis as a kernel eigenvalue problem.\"\n    <https://www.mlpack.org/papers/kpca.pdf>`_\n    Neural computation 10.5 (1998): 1299-1319.\n\n.. _preprocessing_transformer:"
  },
  {
    "filename": "preprocessing.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\preprocessing.rst.txt",
    "id": "preprocessing.rst.txt_chunk_7",
    "header": "Non-linear transformation",
    "text": "Non-linear transformation\n=========================\n\nTwo types of transformations are available: quantile transforms and power\ntransforms. Both quantile and power transforms are based on monotonic\ntransformations of the features and thus preserve the rank of the values\nalong each feature.\n\nQuantile transforms put all features into the same desired distribution based\non the formula :math:`G^{-1}(F(X))` where :math:`F` is the cumulative\ndistribution function of the feature and :math:`G^{-1}` the\n`quantile function <https://en.wikipedia.org/wiki/Quantile_function>`_ of the\ndesired output distribution :math:`G`. This formula is using the two following\nfacts: (i) if :math:`X` is a random variable with a continuous cumulative\ndistribution function :math:`F` then :math:`F(X)` is uniformly distributed on\n:math:`[0,1]`; (ii) if :math:`U` is a random variable with uniform distribution\non :math:`[0,1]` then :math:`G^{-1}(U)` has distribution :math:`G`. By performing\na rank transformation, a quantile transform smooths out unusual distributions\nand is less influenced by outliers than scaling methods. It does, however,\ndistort correlations and distances within and across features.\n\nPower transforms are a family of parametric transformations that aim to map\ndata from any distribution to as close to a Gaussian distribution."
  },
  {
    "filename": "preprocessing.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\preprocessing.rst.txt",
    "id": "preprocessing.rst.txt_chunk_8",
    "header": "Mapping to a Uniform distribution",
    "text": "Mapping to a Uniform distribution\n---------------------------------\n\n:class:`QuantileTransformer` provides a non-parametric\ntransformation to map the data to a uniform distribution\nwith values between 0 and 1::\n\n  >>> from sklearn.datasets import load_iris\n  >>> from sklearn.model_selection import train_test_split\n  >>> X, y = load_iris(return_X_y=True)\n  >>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n  >>> quantile_transformer = preprocessing.QuantileTransformer(random_state=0)\n  >>> X_train_trans = quantile_transformer.fit_transform(X_train)\n  >>> X_test_trans = quantile_transformer.transform(X_test)\n  >>> np.percentile(X_train[:, 0], [0, 25, 50, 75, 100]) # doctest: +SKIP\n  array([ 4.3,  5.1,  5.8,  6.5,  7.9])\n\nThis feature corresponds to the sepal length in cm. Once the quantile\ntransformation is applied, those landmarks approach closely the percentiles\npreviously defined::\n\n  >>> np.percentile(X_train_trans[:, 0], [0, 25, 50, 75, 100])\n  ... # doctest: +SKIP\n  array([ 0.00 ,  0.24,  0.49,  0.73,  0.99 ])\n\nThis can be confirmed on an independent testing set with similar remarks::\n\n  >>> np.percentile(X_test[:, 0], [0, 25, 50, 75, 100])\n  ... # doctest: +SKIP\n  array([ 4.4  ,  5.125,  5.75 ,  6.175,  7.3  ])\n  >>> np.percentile(X_test_trans[:, 0], [0, 25, 50, 75, 100])\n  ... # doctest: +SKIP\n  array([ 0.01,  0.25,  0.46,  0.60 ,  0.94])"
  },
  {
    "filename": "preprocessing.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\preprocessing.rst.txt",
    "id": "preprocessing.rst.txt_chunk_9",
    "header": "Mapping to a Gaussian distribution",
    "text": "Mapping to a Gaussian distribution\n----------------------------------\n\nIn many modeling scenarios, normality of the features in a dataset is desirable.\nPower transforms are a family of parametric, monotonic transformations that aim\nto map data from any distribution to as close to a Gaussian distribution as\npossible in order to stabilize variance and minimize skewness.\n\n:class:`PowerTransformer` currently provides two such power transformations,\nthe Yeo-Johnson transform and the Box-Cox transform.\n\n.. dropdown:: Yeo-Johnson transform\n\n  .. math::\n      x_i^{(\\lambda)} =\n      \\begin{cases}\n      [(x_i + 1)^\\lambda - 1] / \\lambda & \\text{if } \\lambda \\neq 0, x_i \\geq 0, \\\\[8pt]\n      \\ln{(x_i + 1)} & \\text{if } \\lambda = 0, x_i \\geq 0 \\\\[8pt]\n      -[(-x_i + 1)^{2 - \\lambda} - 1] / (2 - \\lambda) & \\text{if } \\lambda \\neq 2, x_i < 0, \\\\[8pt]\n      - \\ln (- x_i + 1) & \\text{if } \\lambda = 2, x_i < 0\n      \\end{cases}\n\n.. dropdown:: Box-Cox transform\n\n  .. math::\n      x_i^{(\\lambda)} =\n      \\begin{cases}\n      \\dfrac{x_i^\\lambda - 1}{\\lambda} & \\text{if } \\lambda \\neq 0, \\\\[8pt]\n      \\ln{(x_i)} & \\text{if } \\lambda = 0,\n      \\end{cases}\n\n  Box-Cox can only be applied to strictly positive data. In both methods, the\n  transformation is parameterized by :math:`\\lambda`, which is determined through\n  maximum likelihood estimation. Here is an example of using Box-Cox to map\n  samples drawn from a lognormal distribution to a normal distribution::\n\n    >>> pt = preprocessing.PowerTransformer(method='box-cox', standardize=False)\n    >>> X_lognormal = np.random.RandomState(616).lognormal(size=(3, 3))\n    >>> X_lognormal\n    array([[1.28, 1.18 , 0.84 ],\n           [0.94, 1.60 , 0.388],\n           [1.35, 0.217, 1.09 ]])\n    >>> pt.fit_transform(X_lognormal)\n    array([[ 0.49 ,  0.179, -0.156],\n           [-0.051,  0.589, -0.576],\n           [ 0.69 , -0.849,  0.101]])\n\n  While the above example sets the `standardize` option to `False`,\n  :class:`PowerTransformer` will apply zero-mean, unit-variance normalization\n  to the transformed output by default.\n\n\nBelow are examples of Box-Cox and Yeo-Johnson applied to various probability\ndistributions.  Note that when applied to certain distributions, the power\ntransforms achieve very Gaussian-like results, but with others, they are\nineffective. This highlights the importance of visualizing the data before and\nafter transformation.\n\n.. figure:: ../auto_examples/preprocessing/images/sphx_glr_plot_map_data_to_normal_001.png\n   :target: ../auto_examples/preprocessing/plot_map_data_to_normal.html\n   :align: center\n   :scale: 100\n\nIt is also possible to map data to a normal distribution using\n:class:`QuantileTransformer` by setting ``output_distribution='normal'``.\nUsing the earlier example with the iris dataset::\n\n  >>> quantile_transformer = preprocessing.QuantileTransformer(\n  ...     output_distribution='normal', random_state=0)\n  >>> X_trans = quantile_transformer.fit_transform(X)\n  >>> quantile_transformer.quantiles_\n  array([[4.3, 2. , 1. , 0.1],\n         [4.4, 2.2, 1.1, 0.1],\n         [4.4, 2.2, 1.2, 0.1],\n         ...,\n         [7.7, 4.1, 6.7, 2.5],\n         [7.7, 4.2, 6.7, 2.5],\n         [7.9, 4.4, 6.9, 2.5]])\n\nThus the median of the input becomes the mean of the output, centered at 0. The\nnormal output is clipped so that the input's minimum and maximum ---\ncorresponding to the 1e-7 and 1 - 1e-7 quantiles respectively --- do not\nbecome infinite under the transformation.\n\n.. _preprocessing_normalization:"
  },
  {
    "filename": "preprocessing.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\preprocessing.rst.txt",
    "id": "preprocessing.rst.txt_chunk_10",
    "header": "Normalization",
    "text": "Normalization\n=============\n\n**Normalization** is the process of **scaling individual samples to have\nunit norm**. This process can be useful if you plan to use a quadratic form\nsuch as the dot-product or any other kernel to quantify the similarity\nof any pair of samples.\n\nThis assumption is the base of the `Vector Space Model\n<https://en.wikipedia.org/wiki/Vector_Space_Model>`_ often used in text\nclassification and clustering contexts.\n\nThe function :func:`normalize` provides a quick and easy way to perform this\noperation on a single array-like dataset, either using the ``l1``, ``l2``, or\n``max`` norms::\n\n  >>> X = [[ 1., -1.,  2.],\n  ...      [ 2.,  0.,  0.],\n  ...      [ 0.,  1., -1.]]\n  >>> X_normalized = preprocessing.normalize(X, norm='l2')\n\n  >>> X_normalized\n  array([[ 0.408, -0.408,  0.812],\n         [ 1.   ,  0.   ,  0.   ],\n         [ 0.   ,  0.707, -0.707]])\n\nThe ``preprocessing`` module further provides a utility class\n:class:`Normalizer` that implements the same operation using the\n``Transformer`` API (even though the ``fit`` method is useless in this case:\nthe class is stateless as this operation treats samples independently).\n\nThis class is hence suitable for use in the early steps of a\n:class:`~sklearn.pipeline.Pipeline`::\n\n  >>> normalizer = preprocessing.Normalizer().fit(X)  # fit does nothing\n  >>> normalizer\n  Normalizer()\n\n\nThe normalizer instance can then be used on sample vectors as any transformer::\n\n  >>> normalizer.transform(X)\n  array([[ 0.408, -0.408,  0.812],\n         [ 1.   ,  0.   ,  0.   ],\n         [ 0.   ,  0.707, -0.707]])\n\n  >>> normalizer.transform([[-1.,  1., 0.]])\n  array([[-0.707,  0.707,  0.]])\n\n\nNote: L2 normalization is also known as spatial sign preprocessing.\n\n.. dropdown:: Sparse input\n\n  :func:`normalize` and :class:`Normalizer` accept **both dense array-like\n  and sparse matrices from scipy.sparse as input**.\n\n  For sparse input the data is **converted to the Compressed Sparse Rows\n  representation** (see ``scipy.sparse.csr_matrix``) before being fed to\n  efficient Cython routines. To avoid unnecessary memory copies, it is\n  recommended to choose the CSR representation upstream.\n\n.. _preprocessing_categorical_features:"
  },
  {
    "filename": "preprocessing.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\preprocessing.rst.txt",
    "id": "preprocessing.rst.txt_chunk_11",
    "header": "Encoding categorical features",
    "text": "Encoding categorical features\n=============================\n\nOften features are not given as continuous values but categorical.\nFor example a person could have features ``[\"male\", \"female\"]``,\n``[\"from Europe\", \"from US\", \"from Asia\"]``,\n``[\"uses Firefox\", \"uses Chrome\", \"uses Safari\", \"uses Internet Explorer\"]``.\nSuch features can be efficiently coded as integers, for instance\n``[\"male\", \"from US\", \"uses Internet Explorer\"]`` could be expressed as\n``[0, 1, 3]`` while ``[\"female\", \"from Asia\", \"uses Chrome\"]`` would be\n``[1, 2, 1]``.\n\nTo convert categorical features to such integer codes, we can use the\n:class:`OrdinalEncoder`. This estimator transforms each categorical feature to one\nnew feature of integers (0 to n_categories - 1)::\n\n    >>> enc = preprocessing.OrdinalEncoder()\n    >>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n    >>> enc.fit(X)\n    OrdinalEncoder()\n    >>> enc.transform([['female', 'from US', 'uses Safari']])\n    array([[0., 1., 1.]])\n\nSuch integer representation can, however, not be used directly with all\nscikit-learn estimators, as these expect continuous input, and would interpret\nthe categories as being ordered, which is often not desired (i.e. the set of\nbrowsers was ordered arbitrarily).\n\nBy default, :class:`OrdinalEncoder` will also passthrough missing values that\nare indicated by `np.nan`.\n\n    >>> enc = preprocessing.OrdinalEncoder()\n    >>> X = [['male'], ['female'], [np.nan], ['female']]\n    >>> enc.fit_transform(X)\n    array([[ 1.],\n           [ 0.],\n           [nan],\n           [ 0.]])\n\n:class:`OrdinalEncoder` provides a parameter `encoded_missing_value` to encode\nthe missing values without the need to create a pipeline and using\n:class:`~sklearn.impute.SimpleImputer`.\n\n    >>> enc = preprocessing.OrdinalEncoder(encoded_missing_value=-1)\n    >>> X = [['male'], ['female'], [np.nan], ['female']]\n    >>> enc.fit_transform(X)\n    array([[ 1.],\n           [ 0.],\n           [-1.],\n           [ 0.]])\n\nThe above processing is equivalent to the following pipeline::\n\n    >>> from sklearn.pipeline import Pipeline\n    >>> from sklearn.impute import SimpleImputer\n    >>> enc = Pipeline(steps=[\n    ...     (\"encoder\", preprocessing.OrdinalEncoder()),\n    ...     (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=-1)),\n    ... ])\n    >>> enc.fit_transform(X)\n    array([[ 1.],\n           [ 0.],\n           [-1.],\n           [ 0.]])\n\nAnother possibility to convert categorical features to features that can be used\nwith scikit-learn estimators is to use a one-of-K, also known as one-hot or\ndummy encoding.\nThis type of encoding can be obtained with the :class:`OneHotEncoder`,\nwhich transforms each categorical feature with\n``n_categories`` possible values into ``n_categories`` binary features, with\none of them 1, and all others 0.\n\nContinuing the example above::\n\n  >>> enc = preprocessing.OneHotEncoder()\n  >>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n  >>> enc.fit(X)\n  OneHotEncoder()\n  >>> enc.transform([['female', 'from US', 'uses Safari'],\n  ...                ['male', 'from Europe', 'uses Safari']]).toarray()\n  array([[1., 0., 0., 1., 0., 1.],\n         [0., 1., 1., 0., 0., 1.]])\n\nBy default, the values each feature can take is inferred automatically\nfrom the dataset and can be found in the ``categories_`` attribute::\n\n    >>> enc.categories_\n    [array(['female', 'male'], dtype=object), array(['from Europe', 'from US'], dtype=object), array(['uses Firefox', 'uses Safari'], dtype=object)]\n\nIt is possible to specify this explicitly using the parameter ``categories``.\nThere are two genders, four possible continents and four web browsers in our\ndataset::\n\n    >>> genders = ['female', 'male']\n    >>> locations = ['from Africa', 'from Asia', 'from Europe', 'from US']\n    >>> browsers = ['uses Chrome', 'uses Firefox', 'uses IE', 'uses Safari']\n    >>> enc = preprocessing.OneHotEncoder(categories=[genders, locations, browsers])\n    >>> # Note that for there are missing categorical values for the 2nd and 3rd\n    >>> # feature\n    >>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n    >>> enc.fit(X)\n    OneHotEncoder(categories=[['female', 'male'],\n                              ['from Africa', 'from Asia', 'from Europe',\n                               'from US'],\n                              ['uses Chrome', 'uses Firefox', 'uses IE',\n                               'uses Safari']])\n    >>> enc.transform([['female', 'from Asia', 'uses Chrome']]).toarray()\n    array([[1., 0., 0., 1., 0., 0., 1., 0., 0., 0.]])\n\nIf there is a possibility that the training data might have missing categorical\nfeatures, it can often be better to specify\n`handle_unknown='infrequent_if_exist'` instead of setting the `categories`\nmanually as above. When `handle_unknown='infrequent_if_exist'` is specified\nand unknown categories are encountered during transform, no error will be\nraised but the resulting one-hot encoded columns for this feature will be all\nzeros or considered as an infrequent category if enabled.\n(`handle_unknown='infrequent_if_exist'` is only supported for one-hot\nencoding)::\n\n    >>> enc = preprocessing.OneHotEncoder(handle_unknown='infrequent_if_exist')\n    >>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n    >>> enc.fit(X)\n    OneHotEncoder(handle_unknown='infrequent_if_exist')\n    >>> enc.transform([['female', 'from Asia', 'uses Chrome']]).toarray()\n    array([[1., 0., 0., 0., 0., 0.]])\n\n\nIt is also possible to encode each column into ``n_categories - 1`` columns\ninstead of ``n_categories`` columns by using the ``drop`` parameter. This\nparameter allows the user to specify a category for each feature to be dropped.\nThis is useful to avoid co-linearity in the input matrix in some classifiers.\nSuch functionality is useful, for example, when using non-regularized\nregression (:class:`LinearRegression <sklearn.linear_model.LinearRegression>`),\nsince co-linearity would cause the covariance matrix to be non-invertible::\n\n    >>> X = [['male', 'from US', 'uses Safari'],\n    ...      ['female', 'from Europe', 'uses Firefox']]\n    >>> drop_enc = preprocessing.OneHotEncoder(drop='first').fit(X)\n    >>> drop_enc.categories_\n    [array(['female', 'male'], dtype=object), array(['from Europe', 'from US'], dtype=object),\n     array(['uses Firefox', 'uses Safari'], dtype=object)]\n    >>> drop_enc.transform(X).toarray()\n    array([[1., 1., 1.],\n           [0., 0., 0.]])\n\nOne might want to drop one of the two columns only for features with 2\ncategories. In this case, you can set the parameter `drop='if_binary'`.\n\n    >>> X = [['male', 'US', 'Safari'],\n    ...      ['female', 'Europe', 'Firefox'],\n    ...      ['female', 'Asia', 'Chrome']]\n    >>> drop_enc = preprocessing.OneHotEncoder(drop='if_binary').fit(X)\n    >>> drop_enc.categories_\n    [array(['female', 'male'], dtype=object), array(['Asia', 'Europe', 'US'], dtype=object),\n     array(['Chrome', 'Firefox', 'Safari'], dtype=object)]\n    >>> drop_enc.transform(X).toarray()\n    array([[1., 0., 0., 1., 0., 0., 1.],\n           [0., 0., 1., 0., 0., 1., 0.],\n           [0., 1., 0., 0., 1., 0., 0.]])\n\nIn the transformed `X`, the first column is the encoding of the feature with\ncategories \"male\"/\"female\", while the remaining 6 columns are the encoding of\nthe 2 features with respectively 3 categories each.\n\nWhen `handle_unknown='ignore'` and `drop` is not None, unknown categories will\nbe encoded as all zeros::\n\n    >>> drop_enc = preprocessing.OneHotEncoder(drop='first',\n    ...                                        handle_unknown='ignore').fit(X)\n    >>> X_test = [['unknown', 'America', 'IE']]\n    >>> drop_enc.transform(X_test).toarray()\n    array([[0., 0., 0., 0., 0.]])\n\nAll the categories in `X_test` are unknown during transform and will be mapped\nto all zeros. This means that unknown categories will have the same mapping as\nthe dropped category. :meth:`OneHotEncoder.inverse_transform` will map all zeros\nto the dropped category if a category is dropped and `None` if a category is\nnot dropped::\n\n    >>> drop_enc = preprocessing.OneHotEncoder(drop='if_binary', sparse_output=False,\n    ...                                        handle_unknown='ignore').fit(X)\n    >>> X_test = [['unknown', 'America', 'IE']]\n    >>> X_trans = drop_enc.transform(X_test)\n    >>> X_trans\n    array([[0., 0., 0., 0., 0., 0., 0.]])\n    >>> drop_enc.inverse_transform(X_trans)\n    array([['female', None, None]], dtype=object)\n\n.. dropdown:: Support of categorical features with missing values\n\n  :class:`OneHotEncoder` supports categorical features with missing values by\n  considering the missing values as an additional category::\n\n      >>> X = [['male', 'Safari'],\n      ...      ['female', None],\n      ...      [np.nan, 'Firefox']]\n      >>> enc = preprocessing.OneHotEncoder(handle_unknown='error').fit(X)\n      >>> enc.categories_\n      [array(['female', 'male', nan], dtype=object),\n      array(['Firefox', 'Safari', None], dtype=object)]\n      >>> enc.transform(X).toarray()\n      array([[0., 1., 0., 0., 1., 0.],\n            [1., 0., 0., 0., 0., 1.],\n            [0., 0., 1., 1., 0., 0.]])\n\n  If a feature contains both `np.nan` and `None`, they will be considered\n  separate categories::\n\n      >>> X = [['Safari'], [None], [np.nan], ['Firefox']]\n      >>> enc = preprocessing.OneHotEncoder(handle_unknown='error').fit(X)\n      >>> enc.categories_\n      [array(['Firefox', 'Safari', None, nan], dtype=object)]\n      >>> enc.transform(X).toarray()\n      array([[0., 1., 0., 0.],\n            [0., 0., 1., 0.],\n            [0., 0., 0., 1.],\n            [1., 0., 0., 0.]])\n\n  See :ref:`dict_feature_extraction` for categorical features that are\n  represented as a dict, not as scalars.\n\n\n.. _encoder_infrequent_categories:"
  },
  {
    "filename": "preprocessing.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\preprocessing.rst.txt",
    "id": "preprocessing.rst.txt_chunk_12",
    "header": "Infrequent categories",
    "text": "Infrequent categories\n---------------------\n\n:class:`OneHotEncoder` and :class:`OrdinalEncoder` support aggregating\ninfrequent categories into a single output for each feature. The parameters to\nenable the gathering of infrequent categories are `min_frequency` and\n`max_categories`.\n\n1. `min_frequency` is either an  integer greater or equal to 1, or a float in\n   the interval `(0.0, 1.0)`. If `min_frequency` is an integer, categories with\n   a cardinality smaller than `min_frequency`  will be considered infrequent.\n   If `min_frequency` is a float, categories with a cardinality smaller than\n   this fraction of the total number of samples will be considered infrequent.\n   The default value is 1, which means every category is encoded separately.\n\n2. `max_categories` is either `None` or any integer greater than 1. This\n   parameter sets an upper limit to the number of output features for each\n   input feature. `max_categories` includes the feature that combines\n   infrequent categories.\n\nIn the following example with :class:`OrdinalEncoder`, the categories `'dog'`\nand `'snake'` are considered infrequent::\n\n   >>> X = np.array([['dog'] * 5 + ['cat'] * 20 + ['rabbit'] * 10 +\n   ...               ['snake'] * 3], dtype=object).T\n   >>> enc = preprocessing.OrdinalEncoder(min_frequency=6).fit(X)\n   >>> enc.infrequent_categories_\n   [array(['dog', 'snake'], dtype=object)]\n   >>> enc.transform(np.array([['dog'], ['cat'], ['rabbit'], ['snake']]))\n   array([[2.],\n          [0.],\n          [1.],\n          [2.]])\n\n:class:`OrdinalEncoder`'s `max_categories` do **not** take into account missing\nor unknown categories. Setting `unknown_value` or `encoded_missing_value` to an\ninteger will increase the number of unique integer codes by one each. This can\nresult in up to `max_categories + 2` integer codes. In the following example,\n\"a\" and \"d\" are considered infrequent and grouped together into a single\ncategory, \"b\" and \"c\" are their own categories, unknown values are encoded as 3\nand missing values are encoded as 4.\n\n  >>> X_train = np.array(\n  ...     [[\"a\"] * 5 + [\"b\"] * 20 + [\"c\"] * 10 + [\"d\"] * 3 + [np.nan]],\n  ...     dtype=object).T\n  >>> enc = preprocessing.OrdinalEncoder(\n  ...     handle_unknown=\"use_encoded_value\", unknown_value=3,\n  ...     max_categories=3, encoded_missing_value=4)\n  >>> _ = enc.fit(X_train)\n  >>> X_test = np.array([[\"a\"], [\"b\"], [\"c\"], [\"d\"], [\"e\"], [np.nan]], dtype=object)\n  >>> enc.transform(X_test)\n  array([[2.],\n         [0.],\n         [1.],\n         [2.],\n         [3.],\n         [4.]])\n\nSimilarly, :class:`OneHotEncoder` can be configured to group together infrequent\ncategories::\n\n   >>> enc = preprocessing.OneHotEncoder(min_frequency=6, sparse_output=False).fit(X)\n   >>> enc.infrequent_categories_\n   [array(['dog', 'snake'], dtype=object)]\n   >>> enc.transform(np.array([['dog'], ['cat'], ['rabbit'], ['snake']]))\n   array([[0., 0., 1.],\n          [1., 0., 0.],\n          [0., 1., 0.],\n          [0., 0., 1.]])\n\nBy setting handle_unknown to `'infrequent_if_exist'`, unknown categories will\nbe considered infrequent::\n\n   >>> enc = preprocessing.OneHotEncoder(\n   ...    handle_unknown='infrequent_if_exist', sparse_output=False, min_frequency=6)\n   >>> enc = enc.fit(X)\n   >>> enc.transform(np.array([['dragon']]))\n   array([[0., 0., 1.]])\n\n:meth:`OneHotEncoder.get_feature_names_out` uses 'infrequent' as the infrequent\nfeature name::\n\n   >>> enc.get_feature_names_out()\n   array(['x0_cat', 'x0_rabbit', 'x0_infrequent_sklearn'], dtype=object)\n\nWhen `'handle_unknown'` is set to `'infrequent_if_exist'` and an unknown\ncategory is encountered in transform:\n\n1. If infrequent category support was not configured or there was no\n   infrequent category during training, the resulting one-hot encoded columns\n   for this feature will be all zeros. In the inverse transform, an unknown\n   category will be denoted as `None`.\n\n2. If there is an infrequent category during training, the unknown category\n   will be considered infrequent. In the inverse transform, 'infrequent_sklearn'\n   will be used to represent the infrequent category.\n\nInfrequent categories can also be configured using `max_categories`. In the\nfollowing example, we set `max_categories=2` to limit the number of features in\nthe output. This will result in all but the `'cat'` category to be considered\ninfrequent, leading to two features, one for `'cat'` and one for infrequent\ncategories - which are all the others::\n\n   >>> enc = preprocessing.OneHotEncoder(max_categories=2, sparse_output=False)\n   >>> enc = enc.fit(X)\n   >>> enc.transform([['dog'], ['cat'], ['rabbit'], ['snake']])\n   array([[0., 1.],\n          [1., 0.],\n          [0., 1.],\n          [0., 1.]])\n\nIf both `max_categories` and `min_frequency` are non-default values, then\ncategories are selected based on `min_frequency` first and `max_categories`\ncategories are kept. In the following example, `min_frequency=4` considers\nonly `snake` to be infrequent, but `max_categories=3`, forces `dog` to also be\ninfrequent::\n\n   >>> enc = preprocessing.OneHotEncoder(min_frequency=4, max_categories=3, sparse_output=False)\n   >>> enc = enc.fit(X)\n   >>> enc.transform([['dog'], ['cat'], ['rabbit'], ['snake']])\n   array([[0., 0., 1.],\n          [1., 0., 0.],\n          [0., 1., 0.],\n          [0., 0., 1.]])\n\nIf there are infrequent categories with the same cardinality at the cutoff of\n`max_categories`, then the first `max_categories` are taken based on lexicon\nordering. In the following example, \"b\", \"c\", and \"d\", have the same cardinality\nand with `max_categories=2`, \"b\" and \"c\" are infrequent because they have a higher\nlexicon order.\n\n   >>> X = np.asarray([[\"a\"] * 20 + [\"b\"] * 10 + [\"c\"] * 10 + [\"d\"] * 10], dtype=object).T\n   >>> enc = preprocessing.OneHotEncoder(max_categories=3).fit(X)\n   >>> enc.infrequent_categories_\n   [array(['b', 'c'], dtype=object)]\n\n.. _target_encoder:"
  },
  {
    "filename": "preprocessing.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\preprocessing.rst.txt",
    "id": "preprocessing.rst.txt_chunk_13",
    "header": "Target Encoder",
    "text": "Target Encoder\n--------------\n\n.. currentmodule:: sklearn.preprocessing\n\nThe :class:`TargetEncoder` uses the target mean conditioned on the categorical\nfeature for encoding unordered categories, i.e. nominal categories [PAR]_\n[MIC]_. This encoding scheme is useful with categorical features with high\ncardinality, where one-hot encoding would inflate the feature space making it\nmore expensive for a downstream model to process. A classical example of high\ncardinality categories are location based such as zip code or region.\n\n.. dropdown:: Binary classification targets\n\n  For the binary classification target, the target encoding is given by:\n\n  .. math::\n      S_i = \\lambda_i\\frac{n_{iY}}{n_i} + (1 - \\lambda_i)\\frac{n_Y}{n}\n\n  where :math:`S_i` is the encoding for category :math:`i`, :math:`n_{iY}` is the\n  number of observations with :math:`Y=1` and category :math:`i`, :math:`n_i` is\n  the number of observations with category :math:`i`, :math:`n_Y` is the number of\n  observations with :math:`Y=1`, :math:`n` is the number of observations, and\n  :math:`\\lambda_i` is a shrinkage factor for category :math:`i`. The shrinkage\n  factor is given by:\n\n  .. math::\n      \\lambda_i = \\frac{n_i}{m + n_i}\n\n  where :math:`m` is a smoothing factor, which is controlled with the `smooth`\n  parameter in :class:`TargetEncoder`. Large smoothing factors will put more\n  weight on the global mean. When `smooth=\"auto\"`, the smoothing factor is\n  computed as an empirical Bayes estimate: :math:`m=\\sigma_i^2/\\tau^2`, where\n  :math:`\\sigma_i^2` is the variance of `y` with category :math:`i` and\n  :math:`\\tau^2` is the global variance of `y`.\n\n.. dropdown:: Multiclass classification targets\n\n  For multiclass classification targets, the formulation is similar to binary\n  classification:\n\n  .. math::\n      S_{ij} = \\lambda_i\\frac{n_{iY_j}}{n_i} + (1 - \\lambda_i)\\frac{n_{Y_j}}{n}\n\n  where :math:`S_{ij}` is the encoding for category :math:`i` and class :math:`j`,\n  :math:`n_{iY_j}` is the number of observations with :math:`Y=j` and category\n  :math:`i`, :math:`n_i` is the number of observations with category :math:`i`,\n  :math:`n_{Y_j}` is the number of observations with :math:`Y=j`, :math:`n` is the\n  number of observations, and :math:`\\lambda_i` is a shrinkage factor for category\n  :math:`i`.\n\n.. dropdown:: Continuous targets\n\n  For continuous targets, the formulation is similar to binary classification:\n\n  .. math::\n      S_i = \\lambda_i\\frac{\\sum_{k\\in L_i}Y_k}{n_i} + (1 - \\lambda_i)\\frac{\\sum_{k=1}^{n}Y_k}{n}\n\n  where :math:`L_i` is the set of observations with category :math:`i` and\n  :math:`n_i` is the number of observations with category :math:`i`.\n\n\n:meth:`~TargetEncoder.fit_transform` internally relies on a :term:`cross fitting`\nscheme to prevent target information from leaking into the train-time\nrepresentation, especially for non-informative high-cardinality categorical\nvariables, and help prevent the downstream model from overfitting spurious\ncorrelations. Note that as a result, `fit(X, y).transform(X)` does not equal\n`fit_transform(X, y)`. In :meth:`~TargetEncoder.fit_transform`, the training\ndata is split into *k* folds (determined by the `cv` parameter) and each fold is\nencoded using the encodings learnt using the other *k-1* folds. The following\ndiagram shows the :term:`cross fitting` scheme in\n:meth:`~TargetEncoder.fit_transform` with the default `cv=5`:\n\n.. image:: ../images/target_encoder_cross_validation.svg\n   :width: 600\n   :align: center\n\n:meth:`~TargetEncoder.fit_transform` also learns a 'full data' encoding using\nthe whole training set. This is never used in\n:meth:`~TargetEncoder.fit_transform` but is saved to the attribute `encodings_`,\nfor use when :meth:`~TargetEncoder.transform` is called. Note that the encodings\nlearned for each fold during the :term:`cross fitting` scheme are not saved to\nan attribute.\n\nThe :meth:`~TargetEncoder.fit` method does **not** use any :term:`cross fitting`\nschemes and learns one encoding on the entire training set, which is used to\nencode categories in :meth:`~TargetEncoder.transform`.\nThis encoding is the same as the 'full data'\nencoding learned in :meth:`~TargetEncoder.fit_transform`.\n\n.. note::\n  :class:`TargetEncoder` considers missing values, such as `np.nan` or `None`,\n  as another category and encodes them like any other category. Categories\n  that are not seen during `fit` are encoded with the target mean, i.e.\n  `target_mean_`.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_preprocessing_plot_target_encoder.py`\n* :ref:`sphx_glr_auto_examples_preprocessing_plot_target_encoder_cross_val.py`\n\n.. rubric:: References\n\n.. [MIC] :doi:`Micci-Barreca, Daniele. \"A preprocessing scheme for high-cardinality\n    categorical attributes in classification and prediction problems\"\n    SIGKDD Explor. Newsl. 3, 1 (July 2001), 27-32. <10.1145/507533.507538>`\n\n.. [PAR] :doi:`Pargent, F., Pfisterer, F., Thomas, J. et al. \"Regularized target\n    encoding outperforms traditional methods in supervised machine learning with\n    high cardinality features\" Comput Stat 37, 2671-2692 (2022)\n    <10.1007/s00180-022-01207-6>`\n\n.. _preprocessing_discretization:"
  },
  {
    "filename": "preprocessing.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\preprocessing.rst.txt",
    "id": "preprocessing.rst.txt_chunk_14",
    "header": "Discretization",
    "text": "Discretization\n==============\n\n`Discretization <https://en.wikipedia.org/wiki/Discretization_of_continuous_features>`_\n(otherwise known as quantization or binning) provides a way to partition continuous\nfeatures into discrete values. Certain datasets with continuous features\nmay benefit from discretization, because discretization can transform the dataset\nof continuous attributes to one with only nominal attributes.\n\nOne-hot encoded discretized features can make a model more expressive, while\nmaintaining interpretability. For instance, pre-processing with a discretizer\ncan introduce nonlinearity to linear models. For more advanced possibilities,\nin particular smooth ones, see :ref:`generating_polynomial_features` further\nbelow."
  },
  {
    "filename": "preprocessing.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\preprocessing.rst.txt",
    "id": "preprocessing.rst.txt_chunk_15",
    "header": "K-bins discretization",
    "text": "K-bins discretization\n---------------------\n\n:class:`KBinsDiscretizer` discretizes features into ``k`` bins::\n\n  >>> X = np.array([[ -3., 5., 15 ],\n  ...               [  0., 6., 14 ],\n  ...               [  6., 3., 11 ]])\n  >>> est = preprocessing.KBinsDiscretizer(n_bins=[3, 2, 2], encode='ordinal').fit(X)\n\nBy default the output is one-hot encoded into a sparse matrix\n(See :ref:`preprocessing_categorical_features`)\nand this can be configured with the ``encode`` parameter.\nFor each feature, the bin edges are computed during ``fit`` and together with\nthe number of bins, they will define the intervals. Therefore, for the current\nexample, these intervals are defined as:\n\n- feature 1: :math:`{[-\\infty, -1), [-1, 2), [2, \\infty)}`\n- feature 2: :math:`{[-\\infty, 5), [5, \\infty)}`\n- feature 3: :math:`{[-\\infty, 14), [14, \\infty)}`\n\nBased on these bin intervals, ``X`` is transformed as follows::\n\n  >>> est.transform(X)                      # doctest: +SKIP\n  array([[ 0., 1., 1.],\n         [ 1., 1., 1.],\n         [ 2., 0., 0.]])\n\nThe resulting dataset contains ordinal attributes which can be further used\nin a :class:`~sklearn.pipeline.Pipeline`.\n\nDiscretization is similar to constructing histograms for continuous data.\nHowever, histograms focus on counting features which fall into particular\nbins, whereas discretization focuses on assigning feature values to these bins.\n\n:class:`KBinsDiscretizer` implements different binning strategies, which can be\nselected with the ``strategy`` parameter. The 'uniform' strategy uses\nconstant-width bins. The 'quantile' strategy uses the quantiles values to have\nequally populated bins in each feature. The 'kmeans' strategy defines bins based\non a k-means clustering procedure performed on each feature independently.\n\nBe aware that one can specify custom bins by passing a callable defining the\ndiscretization strategy to :class:`~sklearn.preprocessing.FunctionTransformer`.\nFor instance, we can use the Pandas function :func:`pandas.cut`::\n\n  >>> import pandas as pd\n  >>> import numpy as np\n  >>> from sklearn import preprocessing\n  >>>\n  >>> bins = [0, 1, 13, 20, 60, np.inf]\n  >>> labels = ['infant', 'kid', 'teen', 'adult', 'senior citizen']\n  >>> transformer = preprocessing.FunctionTransformer(\n  ...     pd.cut, kw_args={'bins': bins, 'labels': labels, 'retbins': False}\n  ... )\n  >>> X = np.array([0.2, 2, 15, 25, 97])\n  >>> transformer.fit_transform(X)\n  ['infant', 'kid', 'teen', 'adult', 'senior citizen']\n  Categories (5, object): ['infant' < 'kid' < 'teen' < 'adult' < 'senior citizen']\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_preprocessing_plot_discretization.py`\n* :ref:`sphx_glr_auto_examples_preprocessing_plot_discretization_classification.py`\n* :ref:`sphx_glr_auto_examples_preprocessing_plot_discretization_strategies.py`\n\n.. _preprocessing_binarization:"
  },
  {
    "filename": "preprocessing.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\preprocessing.rst.txt",
    "id": "preprocessing.rst.txt_chunk_16",
    "header": "Feature binarization",
    "text": "Feature binarization\n--------------------\n\n**Feature binarization** is the process of **thresholding numerical\nfeatures to get boolean values**. This can be useful for downstream\nprobabilistic estimators that make assumption that the input data\nis distributed according to a multi-variate `Bernoulli distribution\n<https://en.wikipedia.org/wiki/Bernoulli_distribution>`_. For instance,\nthis is the case for the :class:`~sklearn.neural_network.BernoulliRBM`.\n\nIt is also common among the text processing community to use binary\nfeature values (probably to simplify the probabilistic reasoning) even\nif normalized counts (a.k.a. term frequencies) or TF-IDF valued features\noften perform slightly better in practice.\n\nAs for the :class:`Normalizer`, the utility class\n:class:`Binarizer` is meant to be used in the early stages of\n:class:`~sklearn.pipeline.Pipeline`. The ``fit`` method does nothing\nas each sample is treated independently of others::\n\n  >>> X = [[ 1., -1.,  2.],\n  ...      [ 2.,  0.,  0.],\n  ...      [ 0.,  1., -1.]]\n\n  >>> binarizer = preprocessing.Binarizer().fit(X)  # fit does nothing\n  >>> binarizer\n  Binarizer()\n\n  >>> binarizer.transform(X)\n  array([[1., 0., 1.],\n         [1., 0., 0.],\n         [0., 1., 0.]])\n\nIt is possible to adjust the threshold of the binarizer::\n\n  >>> binarizer = preprocessing.Binarizer(threshold=1.1)\n  >>> binarizer.transform(X)\n  array([[0., 0., 1.],\n         [1., 0., 0.],\n         [0., 0., 0.]])\n\nAs for the :class:`Normalizer` class, the preprocessing module\nprovides a companion function :func:`binarize`\nto be used when the transformer API is not necessary.\n\nNote that the :class:`Binarizer` is similar to the :class:`KBinsDiscretizer`\nwhen ``k = 2``, and when the bin edge is at the value ``threshold``.\n\n.. topic:: Sparse input\n\n  :func:`binarize` and :class:`Binarizer` accept **both dense array-like\n  and sparse matrices from scipy.sparse as input**.\n\n  For sparse input the data is **converted to the Compressed Sparse Rows\n  representation** (see ``scipy.sparse.csr_matrix``).\n  To avoid unnecessary memory copies, it is recommended to choose the CSR\n  representation upstream.\n\n.. _imputation:"
  },
  {
    "filename": "preprocessing.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\preprocessing.rst.txt",
    "id": "preprocessing.rst.txt_chunk_17",
    "header": "Imputation of missing values",
    "text": "Imputation of missing values\n============================\n\nTools for imputing missing values are discussed at :ref:`impute`.\n\n.. _generating_polynomial_features:"
  },
  {
    "filename": "preprocessing.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\preprocessing.rst.txt",
    "id": "preprocessing.rst.txt_chunk_18",
    "header": "Generating polynomial features",
    "text": "Generating polynomial features\n==============================\n\nOften it's useful to add complexity to a model by considering nonlinear\nfeatures of the input data. We show two possibilities that are both based on\npolynomials: The first one uses pure polynomials, the second one uses splines,\ni.e. piecewise polynomials.\n\n.. _polynomial_features:"
  },
  {
    "filename": "preprocessing.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\preprocessing.rst.txt",
    "id": "preprocessing.rst.txt_chunk_19",
    "header": "Polynomial features",
    "text": "Polynomial features\n-------------------\n\nA simple and common method to use is polynomial features, which can get\nfeatures' high-order and interaction terms. It is implemented in\n:class:`PolynomialFeatures`::\n\n    >>> import numpy as np\n    >>> from sklearn.preprocessing import PolynomialFeatures\n    >>> X = np.arange(6).reshape(3, 2)\n    >>> X\n    array([[0, 1],\n           [2, 3],\n           [4, 5]])\n    >>> poly = PolynomialFeatures(2)\n    >>> poly.fit_transform(X)\n    array([[ 1.,  0.,  1.,  0.,  0.,  1.],\n           [ 1.,  2.,  3.,  4.,  6.,  9.],\n           [ 1.,  4.,  5., 16., 20., 25.]])\n\nThe features of X have been transformed from :math:`(X_1, X_2)` to\n:math:`(1, X_1, X_2, X_1^2, X_1X_2, X_2^2)`.\n\nIn some cases, only interaction terms among features are required, and it can\nbe gotten with the setting ``interaction_only=True``::\n\n    >>> X = np.arange(9).reshape(3, 3)\n    >>> X\n    array([[0, 1, 2],\n           [3, 4, 5],\n           [6, 7, 8]])\n    >>> poly = PolynomialFeatures(degree=3, interaction_only=True)\n    >>> poly.fit_transform(X)\n    array([[  1.,   0.,   1.,   2.,   0.,   0.,   2.,   0.],\n           [  1.,   3.,   4.,   5.,  12.,  15.,  20.,  60.],\n           [  1.,   6.,   7.,   8.,  42.,  48.,  56., 336.]])\n\nThe features of X have been transformed from :math:`(X_1, X_2, X_3)` to\n:math:`(1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3)`.\n\nNote that polynomial features are used implicitly in `kernel methods\n<https://en.wikipedia.org/wiki/Kernel_method>`_ (e.g., :class:`~sklearn.svm.SVC`,\n:class:`~sklearn.decomposition.KernelPCA`) when using polynomial :ref:`svm_kernels`.\n\nSee :ref:`sphx_glr_auto_examples_linear_model_plot_polynomial_interpolation.py`\nfor Ridge regression using created polynomial features.\n\n.. _spline_transformer:"
  },
  {
    "filename": "preprocessing.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\preprocessing.rst.txt",
    "id": "preprocessing.rst.txt_chunk_20",
    "header": "Spline transformer",
    "text": "Spline transformer\n------------------\n\nAnother way to add nonlinear terms instead of pure polynomials of features is\nto generate spline basis functions for each feature with the\n:class:`SplineTransformer`. Splines are piecewise polynomials, parametrized by\ntheir polynomial degree and the positions of the knots. The\n:class:`SplineTransformer` implements a B-spline basis, cf. the references\nbelow.\n\n.. note::\n\n    The :class:`SplineTransformer` treats each feature separately, i.e. it\n    won't give you interaction terms.\n\nSome of the advantages of splines over polynomials are:\n\n- B-splines are very flexible and robust if you keep a fixed low degree,\n  usually 3, and parsimoniously adapt the number of knots. Polynomials\n  would need a higher degree, which leads to the next point.\n- B-splines do not have oscillatory behaviour at the boundaries as have\n  polynomials (the higher the degree, the worse). This is known as `Runge's\n  phenomenon <https://en.wikipedia.org/wiki/Runge%27s_phenomenon>`_.\n- B-splines provide good options for extrapolation beyond the boundaries,\n  i.e. beyond the range of fitted values. Have a look at the option\n  ``extrapolation``.\n- B-splines generate a feature matrix with a banded structure. For a single\n  feature, every row contains only ``degree + 1`` non-zero elements, which\n  occur consecutively and are even positive. This results in a matrix with\n  good numerical properties, e.g. a low condition number, in sharp contrast\n  to a matrix of polynomials, which goes under the name\n  `Vandermonde matrix <https://en.wikipedia.org/wiki/Vandermonde_matrix>`_.\n  A low condition number is important for stable algorithms of linear\n  models.\n\nThe following code snippet shows splines in action::\n\n    >>> import numpy as np\n    >>> from sklearn.preprocessing import SplineTransformer\n    >>> X = np.arange(5).reshape(5, 1)\n    >>> X\n    array([[0],\n           [1],\n           [2],\n           [3],\n           [4]])\n    >>> spline = SplineTransformer(degree=2, n_knots=3)\n    >>> spline.fit_transform(X)\n    array([[0.5  , 0.5  , 0.   , 0.   ],\n           [0.125, 0.75 , 0.125, 0.   ],\n           [0.   , 0.5  , 0.5  , 0.   ],\n           [0.   , 0.125, 0.75 , 0.125],\n           [0.   , 0.   , 0.5  , 0.5  ]])\n\nAs the ``X`` is sorted, one can easily see the banded matrix output. Only the\nthree middle diagonals are non-zero for ``degree=2``. The higher the degree,\nthe more overlapping of the splines.\n\nInterestingly, a :class:`SplineTransformer` of ``degree=0`` is the same as\n:class:`~sklearn.preprocessing.KBinsDiscretizer` with\n``encode='onehot-dense'`` and ``n_bins = n_knots - 1`` if\n``knots = strategy``.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_linear_model_plot_polynomial_interpolation.py`\n* :ref:`sphx_glr_auto_examples_applications_plot_cyclical_feature_engineering.py`\n\n.. dropdown:: References\n\n  * Eilers, P., & Marx, B. (1996). :doi:`Flexible Smoothing with B-splines and\n    Penalties <10.1214/ss/1038425655>`. Statist. Sci. 11 (1996), no. 2, 89--121.\n\n  * Perperoglou, A., Sauerbrei, W., Abrahamowicz, M. et al. :doi:`A review of\n    spline function procedures in R <10.1186/s12874-019-0666-3>`.\n    BMC Med Res Methodol 19, 46 (2019).\n\n\n.. _function_transformer:"
  },
  {
    "filename": "preprocessing.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\preprocessing.rst.txt",
    "id": "preprocessing.rst.txt_chunk_21",
    "header": "Custom transformers",
    "text": "Custom transformers\n===================\n\nOften, you will want to convert an existing Python function into a transformer\nto assist in data cleaning or processing. You can implement a transformer from\nan arbitrary function with :class:`FunctionTransformer`. For example, to build\na transformer that applies a log transformation in a pipeline, do::\n\n    >>> import numpy as np\n    >>> from sklearn.preprocessing import FunctionTransformer\n    >>> transformer = FunctionTransformer(np.log1p, validate=True)\n    >>> X = np.array([[0, 1], [2, 3]])\n    >>> # Since FunctionTransformer is no-op during fit, we can call transform directly\n    >>> transformer.transform(X)\n    array([[0.        , 0.69314718],\n           [1.09861229, 1.38629436]])\n\nYou can ensure that ``func`` and ``inverse_func`` are the inverse of each other\nby setting ``check_inverse=True`` and calling ``fit`` before\n``transform``. Please note that a warning is raised and can be turned into an\nerror with a ``filterwarnings``::\n\n  >>> import warnings\n  >>> warnings.filterwarnings(\"error\", message=\".*check_inverse*.\",\n  ...                         category=UserWarning, append=False)\n\nFor a full code example that demonstrates using a :class:`FunctionTransformer`\nto extract features from text data see\n:ref:`sphx_glr_auto_examples_compose_plot_column_transformer.py` and\n:ref:`sphx_glr_auto_examples_applications_plot_cyclical_feature_engineering.py`."
  },
  {
    "filename": "preprocessing_targets.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\preprocessing_targets.rst.txt",
    "id": "preprocessing_targets.rst.txt_chunk_0",
    "header": ".. currentmodule:: sklearn.preprocessing",
    "text": ".. currentmodule:: sklearn.preprocessing\n\n.. _preprocessing_targets:\n\n=========================================="
  },
  {
    "filename": "preprocessing_targets.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\preprocessing_targets.rst.txt",
    "id": "preprocessing_targets.rst.txt_chunk_1",
    "header": "Transforming the prediction target (``y``)",
    "text": "Transforming the prediction target (``y``)\n==========================================\n\nThese are transformers that are not intended to be used on features, only on\nsupervised learning targets. See also :ref:`transformed_target_regressor` if\nyou want to transform the prediction target for learning, but evaluate the\nmodel in the original (untransformed) space."
  },
  {
    "filename": "preprocessing_targets.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\preprocessing_targets.rst.txt",
    "id": "preprocessing_targets.rst.txt_chunk_2",
    "header": "Label binarization",
    "text": "Label binarization\n=================="
  },
  {
    "filename": "preprocessing_targets.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\preprocessing_targets.rst.txt",
    "id": "preprocessing_targets.rst.txt_chunk_3",
    "header": "LabelBinarizer",
    "text": "LabelBinarizer\n--------------\n\n:class:`LabelBinarizer` is a utility class to help create a :term:`label\nindicator matrix` from a list of :term:`multiclass` labels::\n\n    >>> from sklearn import preprocessing\n    >>> lb = preprocessing.LabelBinarizer()\n    >>> lb.fit([1, 2, 6, 4, 2])\n    LabelBinarizer()\n    >>> lb.classes_\n    array([1, 2, 4, 6])\n    >>> lb.transform([1, 6])\n    array([[1, 0, 0, 0],\n           [0, 0, 0, 1]])\n\nUsing this format can enable multiclass classification in estimators\nthat support the label indicator matrix format.\n\n.. warning::\n\n    LabelBinarizer is not needed if you are using an estimator that\n    already supports :term:`multiclass` data.\n\nFor more information about multiclass classification, refer to\n:ref:`multiclass_classification`."
  },
  {
    "filename": "preprocessing_targets.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\preprocessing_targets.rst.txt",
    "id": "preprocessing_targets.rst.txt_chunk_4",
    "header": "MultiLabelBinarizer",
    "text": "MultiLabelBinarizer\n-------------------\n\nIn :term:`multilabel` learning, the joint set of binary classification tasks is\nexpressed with a label binary indicator array: each sample is one row of a 2d\narray of shape (n_samples, n_classes) with binary values where the one, i.e. the\nnon zero elements, corresponds to the subset of labels for that sample. An array\nsuch as ``np.array([[1, 0, 0], [0, 1, 1], [0, 0, 0]])`` represents label 0 in the\nfirst sample, labels 1 and 2 in the second sample, and no labels in the third\nsample.\n\nProducing multilabel data as a list of sets of labels may be more intuitive.\nThe :class:`MultiLabelBinarizer <sklearn.preprocessing.MultiLabelBinarizer>`\ntransformer can be used to convert between a collection of collections of\nlabels and the indicator format::\n\n    >>> from sklearn.preprocessing import MultiLabelBinarizer\n    >>> y = [[2, 3, 4], [2], [0, 1, 3], [0, 1, 2, 3, 4], [0, 1, 2]]\n    >>> MultiLabelBinarizer().fit_transform(y)\n    array([[0, 0, 1, 1, 1],\n           [0, 0, 1, 0, 0],\n           [1, 1, 0, 1, 0],\n           [1, 1, 1, 1, 1],\n           [1, 1, 1, 0, 0]])\n\nFor more information about multilabel classification, refer to\n:ref:`multilabel_classification`."
  },
  {
    "filename": "preprocessing_targets.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\preprocessing_targets.rst.txt",
    "id": "preprocessing_targets.rst.txt_chunk_5",
    "header": "Label encoding",
    "text": "Label encoding\n==============\n\n:class:`LabelEncoder` is a utility class to help normalize labels such that\nthey contain only values between 0 and n_classes-1. This is sometimes useful\nfor writing efficient Cython routines. :class:`LabelEncoder` can be used as\nfollows::\n\n    >>> from sklearn import preprocessing\n    >>> le = preprocessing.LabelEncoder()\n    >>> le.fit([1, 2, 2, 6])\n    LabelEncoder()\n    >>> le.classes_\n    array([1, 2, 6])\n    >>> le.transform([1, 1, 2, 6])\n    array([0, 0, 1, 2])\n    >>> le.inverse_transform([0, 0, 1, 2])\n    array([1, 1, 2, 6])\n\nIt can also be used to transform non-numerical labels (as long as they are\nhashable and comparable) to numerical labels::\n\n    >>> le = preprocessing.LabelEncoder()\n    >>> le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n    LabelEncoder()\n    >>> list(le.classes_)\n    [np.str_('amsterdam'), np.str_('paris'), np.str_('tokyo')]\n    >>> le.transform([\"tokyo\", \"tokyo\", \"paris\"])\n    array([2, 2, 1])\n    >>> list(le.inverse_transform([2, 2, 1]))\n    [np.str_('tokyo'), np.str_('tokyo'), np.str_('paris')]"
  },
  {
    "filename": "random_projection.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\random_projection.rst.txt",
    "id": "random_projection.rst.txt_chunk_0",
    "header": ".. _random_projection:",
    "text": ".. _random_projection:\n\n=================="
  },
  {
    "filename": "random_projection.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\random_projection.rst.txt",
    "id": "random_projection.rst.txt_chunk_1",
    "header": "Random Projection",
    "text": "Random Projection\n==================\n.. currentmodule:: sklearn.random_projection\n\nThe :mod:`sklearn.random_projection` module implements a simple and\ncomputationally efficient way to reduce the dimensionality of the data by\ntrading a controlled amount of accuracy (as additional variance) for faster\nprocessing times and smaller model sizes. This module implements two types of\nunstructured random matrix:\n:ref:`Gaussian random matrix <gaussian_random_matrix>` and\n:ref:`sparse random matrix <sparse_random_matrix>`.\n\nThe dimensions and distribution of random projections matrices are\ncontrolled so as to preserve the pairwise distances between any two\nsamples of the dataset. Thus random projection is a suitable approximation\ntechnique for distance based method.\n\n\n.. rubric:: References\n\n* Sanjoy Dasgupta. 2000.\n  `Experiments with random projection. <https://cseweb.ucsd.edu/~dasgupta/papers/randomf.pdf>`_\n  In Proceedings of the Sixteenth conference on Uncertainty in artificial\n  intelligence (UAI'00), Craig Boutilier and Mois\u00e9s Goldszmidt (Eds.). Morgan\n  Kaufmann Publishers Inc., San Francisco, CA, USA, 143-151.\n\n* Ella Bingham and Heikki Mannila. 2001.\n  `Random projection in dimensionality reduction: applications to image and text data. <https://cs-people.bu.edu/evimaria/cs565/kdd-rp.pdf>`_\n  In Proceedings of the seventh ACM SIGKDD international conference on\n  Knowledge discovery and data mining (KDD '01). ACM, New York, NY, USA,\n  245-250.\n\n\n.. _johnson_lindenstrauss:"
  },
  {
    "filename": "random_projection.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\random_projection.rst.txt",
    "id": "random_projection.rst.txt_chunk_2",
    "header": "The Johnson-Lindenstrauss lemma",
    "text": "The Johnson-Lindenstrauss lemma\n===============================\n\nThe main theoretical result behind the efficiency of random projection is the\n`Johnson-Lindenstrauss lemma (quoting Wikipedia)\n<https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma>`_:\n\n  In mathematics, the Johnson-Lindenstrauss lemma is a result\n  concerning low-distortion embeddings of points from high-dimensional\n  into low-dimensional Euclidean space. The lemma states that a small set\n  of points in a high-dimensional space can be embedded into a space of\n  much lower dimension in such a way that distances between the points are\n  nearly preserved. The map used for the embedding is at least Lipschitz,\n  and can even be taken to be an orthogonal projection.\n\nKnowing only the number of samples, the\n:func:`johnson_lindenstrauss_min_dim` estimates\nconservatively the minimal size of the random subspace to guarantee a\nbounded distortion introduced by the random projection::\n\n  >>> from sklearn.random_projection import johnson_lindenstrauss_min_dim\n  >>> johnson_lindenstrauss_min_dim(n_samples=1e6, eps=0.5)\n  np.int64(663)\n  >>> johnson_lindenstrauss_min_dim(n_samples=1e6, eps=[0.5, 0.1, 0.01])\n  array([    663,   11841, 1112658])\n  >>> johnson_lindenstrauss_min_dim(n_samples=[1e4, 1e5, 1e6], eps=0.1)\n  array([ 7894,  9868, 11841])\n\n.. figure:: ../auto_examples/miscellaneous/images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png\n   :target: ../auto_examples/miscellaneous/plot_johnson_lindenstrauss_bound.html\n   :scale: 75\n   :align: center\n\n.. figure:: ../auto_examples/miscellaneous/images/sphx_glr_plot_johnson_lindenstrauss_bound_002.png\n   :target: ../auto_examples/miscellaneous/plot_johnson_lindenstrauss_bound.html\n   :scale: 75\n   :align: center\n\n.. rubric:: Examples\n\n* See :ref:`sphx_glr_auto_examples_miscellaneous_plot_johnson_lindenstrauss_bound.py`\n  for a theoretical explication on the Johnson-Lindenstrauss lemma and an\n  empirical validation using sparse random matrices.\n\n.. rubric:: References\n\n* Sanjoy Dasgupta and Anupam Gupta, 1999.\n  `An elementary proof of the Johnson-Lindenstrauss Lemma.\n  <https://cseweb.ucsd.edu/~dasgupta/papers/jl.pdf>`_\n\n.. _gaussian_random_matrix:"
  },
  {
    "filename": "random_projection.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\random_projection.rst.txt",
    "id": "random_projection.rst.txt_chunk_3",
    "header": "Gaussian random projection",
    "text": "Gaussian random projection\n==========================\nThe :class:`GaussianRandomProjection` reduces the\ndimensionality by projecting the original input space on a randomly generated\nmatrix where components are drawn from the following distribution\n:math:`N(0, \\frac{1}{n_{components}})`.\n\nHere is a small excerpt which illustrates how to use the Gaussian random\nprojection transformer::\n\n  >>> import numpy as np\n  >>> from sklearn import random_projection\n  >>> X = np.random.rand(100, 10000)\n  >>> transformer = random_projection.GaussianRandomProjection()\n  >>> X_new = transformer.fit_transform(X)\n  >>> X_new.shape\n  (100, 3947)\n\n\n.. _sparse_random_matrix:"
  },
  {
    "filename": "random_projection.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\random_projection.rst.txt",
    "id": "random_projection.rst.txt_chunk_4",
    "header": "Sparse random projection",
    "text": "Sparse random projection\n========================\nThe :class:`SparseRandomProjection` reduces the\ndimensionality by projecting the original input space using a sparse\nrandom matrix.\n\nSparse random matrices are an alternative to dense Gaussian random\nprojection matrix that guarantees similar embedding quality while being much\nmore memory efficient and allowing faster computation of the projected data.\n\nIf we define ``s = 1 / density``, the elements of the random matrix\nare drawn from\n\n.. math::\n\n  \\left\\{\n  \\begin{array}{c c l}\n  -\\sqrt{\\frac{s}{n_{\\text{components}}}} & & 1 / 2s\\\\\n  0 &\\text{with probability}  & 1 - 1 / s \\\\\n  +\\sqrt{\\frac{s}{n_{\\text{components}}}} & & 1 / 2s\\\\\n  \\end{array}\n  \\right.\n\nwhere :math:`n_{\\text{components}}` is the size of the projected subspace.\nBy default the density of non zero elements is set to the minimum density as\nrecommended by Ping Li et al.: :math:`1 / \\sqrt{n_{\\text{features}}}`.\n\nHere is a small excerpt which illustrates how to use the sparse random\nprojection transformer::\n\n  >>> import numpy as np\n  >>> from sklearn import random_projection\n  >>> X = np.random.rand(100, 10000)\n  >>> transformer = random_projection.SparseRandomProjection()\n  >>> X_new = transformer.fit_transform(X)\n  >>> X_new.shape\n  (100, 3947)\n\n\n.. rubric:: References\n\n* D. Achlioptas. 2003.\n  `Database-friendly random projections: Johnson-Lindenstrauss  with binary\n  coins <https://www.sciencedirect.com/science/article/pii/S0022000003000254>`_.\n  Journal of Computer and System Sciences 66 (2003) 671-687.\n\n* Ping Li, Trevor J. Hastie, and Kenneth W. Church. 2006.\n  `Very sparse random projections. <https://web.stanford.edu/~hastie/Papers/Ping/KDD06_rp.pdf>`_\n  In Proceedings of the 12th ACM SIGKDD international conference on\n  Knowledge discovery and data mining (KDD '06). ACM, New York, NY, USA, 287-296.\n\n\n.. _random_projection_inverse_transform:"
  },
  {
    "filename": "random_projection.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\random_projection.rst.txt",
    "id": "random_projection.rst.txt_chunk_5",
    "header": "Inverse Transform",
    "text": "Inverse Transform\n=================\nThe random projection transformers have ``compute_inverse_components`` parameter. When\nset to True, after creating the random ``components_`` matrix during fitting,\nthe transformer computes the pseudo-inverse of this matrix and stores it as\n``inverse_components_``. The ``inverse_components_`` matrix has shape\n:math:`n_{features} \\times n_{components}`, and it is always a dense matrix,\nregardless of whether the components matrix is sparse or dense. So depending on\nthe number of features and components, it may use a lot of memory.\n\nWhen the ``inverse_transform`` method is called, it computes the product of the\ninput ``X`` and the transpose of the inverse components. If the inverse components have\nbeen computed during fit, they are reused at each call to ``inverse_transform``.\nOtherwise they are recomputed each time, which can be costly. The result is always\ndense, even if ``X`` is sparse.\n\nHere is a small code example which illustrates how to use the inverse transform\nfeature::\n\n  >>> import numpy as np\n  >>> from sklearn.random_projection import SparseRandomProjection\n  >>> X = np.random.rand(100, 10000)\n  >>> transformer = SparseRandomProjection(\n  ...   compute_inverse_components=True\n  ... )\n  ...\n  >>> X_new = transformer.fit_transform(X)\n  >>> X_new.shape\n  (100, 3947)\n  >>> X_new_inversed = transformer.inverse_transform(X_new)\n  >>> X_new_inversed.shape\n  (100, 10000)\n  >>> X_new_again = transformer.transform(X_new_inversed)\n  >>> np.allclose(X_new, X_new_again)\n  True"
  },
  {
    "filename": "semi_supervised.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\semi_supervised.rst.txt",
    "id": "semi_supervised.rst.txt_chunk_0",
    "header": ".. _semi_supervised:",
    "text": ".. _semi_supervised:\n\n==================================================="
  },
  {
    "filename": "semi_supervised.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\semi_supervised.rst.txt",
    "id": "semi_supervised.rst.txt_chunk_1",
    "header": "Semi-supervised learning",
    "text": "Semi-supervised learning\n===================================================\n\n.. currentmodule:: sklearn.semi_supervised\n\n`Semi-supervised learning\n<https://en.wikipedia.org/wiki/Semi-supervised_learning>`_ is a situation\nin which in your training data some of the samples are not labeled. The\nsemi-supervised estimators in :mod:`sklearn.semi_supervised` are able to\nmake use of this additional unlabeled data to better capture the shape of\nthe underlying data distribution and generalize better to new samples.\nThese algorithms can perform well when we have a very small amount of\nlabeled points and a large amount of unlabeled points.\n\n.. topic:: Unlabeled entries in `y`\n\n   It is important to assign an identifier to unlabeled points along with the\n   labeled data when training the model with the ``fit`` method. The\n   identifier that this implementation uses is the integer value :math:`-1`.\n   Note that for string labels, the dtype of `y` should be object so that it\n   can contain both strings and integers.\n\n.. note::\n\n   Semi-supervised algorithms need to make assumptions about the distribution\n   of the dataset in order to achieve performance gains. See `here\n   <https://en.wikipedia.org/wiki/Semi-supervised_learning#Assumptions>`_\n   for more details.\n\n.. _self_training:"
  },
  {
    "filename": "semi_supervised.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\semi_supervised.rst.txt",
    "id": "semi_supervised.rst.txt_chunk_2",
    "header": "Self Training",
    "text": "Self Training\n=============\n\nThis self-training implementation is based on Yarowsky's [1]_ algorithm. Using\nthis algorithm, a given supervised classifier can function as a semi-supervised\nclassifier, allowing it to learn from unlabeled data.\n\n:class:`SelfTrainingClassifier` can be called with any classifier that\nimplements `predict_proba`, passed as the parameter `estimator`. In\neach iteration, the `estimator` predicts labels for the unlabeled\nsamples and adds a subset of these labels to the labeled dataset.\n\nThe choice of this subset is determined by the selection criterion. This\nselection can be done using a `threshold` on the prediction probabilities, or\nby choosing the `k_best` samples according to the prediction probabilities.\n\nThe labels used for the final fit as well as the iteration in which each sample\nwas labeled are available as attributes. The optional `max_iter` parameter\nspecifies how many times the loop is executed at most.\n\nThe `max_iter` parameter may be set to `None`, causing the algorithm to iterate\nuntil all samples have labels or no new samples are selected in that iteration.\n\n.. note::\n\n   When using the self-training classifier, the\n   :ref:`calibration <calibration>` of the classifier is important.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_semi_supervised_plot_self_training_varying_threshold.py`\n* :ref:`sphx_glr_auto_examples_semi_supervised_plot_semi_supervised_versus_svm_iris.py`\n\n.. rubric:: References\n\n.. [1] :doi:`\"Unsupervised word sense disambiguation rivaling supervised methods\"\n    <10.3115/981658.981684>`\n    David Yarowsky, Proceedings of the 33rd annual meeting on Association for\n    Computational Linguistics (ACL '95). Association for Computational Linguistics,\n    Stroudsburg, PA, USA, 189-196.\n\n.. _label_propagation:"
  },
  {
    "filename": "semi_supervised.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\semi_supervised.rst.txt",
    "id": "semi_supervised.rst.txt_chunk_3",
    "header": "Label Propagation",
    "text": "Label Propagation\n=================\n\nLabel propagation denotes a few variations of semi-supervised graph\ninference algorithms.\n\nA few features available in this model:\n  * Used for classification tasks\n  * Kernel methods to project data into alternate dimensional spaces\n\n`scikit-learn` provides two label propagation models:\n:class:`LabelPropagation` and :class:`LabelSpreading`. Both work by\nconstructing a similarity graph over all items in the input dataset.\n\n.. figure:: ../auto_examples/semi_supervised/images/sphx_glr_plot_label_propagation_structure_001.png\n    :target: ../auto_examples/semi_supervised/plot_label_propagation_structure.html\n    :align: center\n    :scale: 60%\n\n    **An illustration of label-propagation:** *the structure of unlabeled\n    observations is consistent with the class structure, and thus the\n    class label can be propagated to the unlabeled observations of the\n    training set.*\n\n:class:`LabelPropagation` and :class:`LabelSpreading`\ndiffer in modifications to the similarity matrix that graph and the\nclamping effect on the label distributions.\nClamping allows the algorithm to change the weight of the true ground labeled\ndata to some degree. The :class:`LabelPropagation` algorithm performs hard\nclamping of input labels, which means :math:`\\alpha=0`. This clamping factor\ncan be relaxed, to say :math:`\\alpha=0.2`, which means that we will always\nretain 80 percent of our original label distribution, but the algorithm gets to\nchange its confidence of the distribution within 20 percent.\n\n:class:`LabelPropagation` uses the raw similarity matrix constructed from\nthe data with no modifications. In contrast, :class:`LabelSpreading`\nminimizes a loss function that has regularization properties, as such it\nis often more robust to noise. The algorithm iterates on a modified\nversion of the original graph and normalizes the edge weights by\ncomputing the normalized graph Laplacian matrix. This procedure is also\nused in :ref:`spectral_clustering`.\n\nLabel propagation models have two built-in kernel methods. Choice of kernel\naffects both scalability and performance of the algorithms. The following are\navailable:\n\n* rbf (:math:`\\exp(-\\gamma |x-y|^2), \\gamma > 0`). :math:`\\gamma` is\n  specified by keyword gamma.\n\n* knn (:math:`1[x' \\in kNN(x)]`). :math:`k` is specified by keyword\n  n_neighbors.\n\nThe RBF kernel will produce a fully connected graph which is represented in memory\nby a dense matrix. This matrix may be very large and combined with the cost of\nperforming a full matrix multiplication calculation for each iteration of the\nalgorithm can lead to prohibitively long running times. On the other hand,\nthe KNN kernel will produce a much more memory-friendly sparse matrix\nwhich can drastically reduce running times.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_semi_supervised_plot_semi_supervised_versus_svm_iris.py`\n* :ref:`sphx_glr_auto_examples_semi_supervised_plot_label_propagation_structure.py`\n* :ref:`sphx_glr_auto_examples_semi_supervised_plot_label_propagation_digits.py`\n* :ref:`sphx_glr_auto_examples_semi_supervised_plot_label_propagation_digits_active_learning.py`\n\n.. rubric:: References\n\n[2] Yoshua Bengio, Olivier Delalleau, Nicolas Le Roux. In Semi-Supervised\nLearning (2006), pp. 193-216\n\n[3] Olivier Delalleau, Yoshua Bengio, Nicolas Le Roux. Efficient\nNon-Parametric Function Induction in Semi-Supervised Learning. AISTAT 2005\nhttps://www.gatsby.ucl.ac.uk/aistats/fullpapers/204.pdf"
  },
  {
    "filename": "sgd.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\sgd.rst.txt",
    "id": "sgd.rst.txt_chunk_0",
    "header": ".. _sgd:",
    "text": ".. _sgd:\n\n==========================="
  },
  {
    "filename": "sgd.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\sgd.rst.txt",
    "id": "sgd.rst.txt_chunk_1",
    "header": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\n===========================\n\n.. currentmodule:: sklearn.linear_model\n\n**Stochastic Gradient Descent (SGD)** is a simple yet very efficient\napproach to fitting linear classifiers and regressors under\nconvex loss functions such as (linear) `Support Vector Machines\n<https://en.wikipedia.org/wiki/Support_vector_machine>`_ and `Logistic\nRegression <https://en.wikipedia.org/wiki/Logistic_regression>`_.\nEven though SGD has been around in the machine learning community for\na long time, it has received a considerable amount of attention just\nrecently in the context of large-scale learning.\n\nSGD has been successfully applied to large-scale and sparse machine\nlearning problems often encountered in text classification and natural\nlanguage processing.  Given that the data is sparse, the classifiers\nin this module easily scale to problems with more than :math:`10^5` training\nexamples and more than :math:`10^5` features.\n\nStrictly speaking, SGD is merely an optimization technique and does not\ncorrespond to a specific family of machine learning models. It is only a\n*way* to train a model. Often, an instance of :class:`SGDClassifier` or\n:class:`SGDRegressor` will have an equivalent estimator in\nthe scikit-learn API, potentially using a different optimization technique.\nFor example, using `SGDClassifier(loss='log_loss')` results in logistic regression,\ni.e. a model equivalent to :class:`~sklearn.linear_model.LogisticRegression`\nwhich is fitted via SGD instead of being fitted by one of the other solvers\nin :class:`~sklearn.linear_model.LogisticRegression`. Similarly,\n`SGDRegressor(loss='squared_error', penalty='l2')` and\n:class:`~sklearn.linear_model.Ridge` solve the same optimization problem, via\ndifferent means.\n\nThe advantages of Stochastic Gradient Descent are:\n\n+ Efficiency.\n\n+ Ease of implementation (lots of opportunities for code tuning).\n\nThe disadvantages of Stochastic Gradient Descent include:\n\n+ SGD requires a number of hyperparameters such as the regularization\n  parameter and the number of iterations.\n\n+ SGD is sensitive to feature scaling.\n\n.. warning::\n\n  Make sure you permute (shuffle) your training data before fitting the model\n  or use ``shuffle=True`` to shuffle after each iteration (used by default).\n  Also, ideally, features should be standardized using e.g.\n  `make_pipeline(StandardScaler(), SGDClassifier())` (see :ref:`Pipelines\n  <combining_estimators>`)."
  },
  {
    "filename": "sgd.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\sgd.rst.txt",
    "id": "sgd.rst.txt_chunk_2",
    "header": "Classification",
    "text": "Classification\n==============\n\n\nThe class :class:`SGDClassifier` implements a plain stochastic gradient\ndescent learning routine which supports different loss functions and\npenalties for classification. Below is the decision boundary of a\n:class:`SGDClassifier` trained with the hinge loss, equivalent to a linear SVM.\n\n.. figure:: ../auto_examples/linear_model/images/sphx_glr_plot_sgd_separating_hyperplane_001.png\n   :target: ../auto_examples/linear_model/plot_sgd_separating_hyperplane.html\n   :align: center\n   :scale: 75\n\nAs other classifiers, SGD has to be fitted with two arrays: an array `X`\nof shape (n_samples, n_features) holding the training samples, and an\narray `y` of shape (n_samples,) holding the target values (class labels)\nfor the training samples::\n\n    >>> from sklearn.linear_model import SGDClassifier\n    >>> X = [[0., 0.], [1., 1.]]\n    >>> y = [0, 1]\n    >>> clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5)\n    >>> clf.fit(X, y)\n    SGDClassifier(max_iter=5)\n\n\nAfter being fitted, the model can then be used to predict new values::\n\n    >>> clf.predict([[2., 2.]])\n    array([1])\n\nSGD fits a linear model to the training data. The ``coef_`` attribute holds\nthe model parameters::\n\n    >>> clf.coef_\n    array([[9.9, 9.9]])\n\nThe ``intercept_`` attribute holds the intercept (aka offset or bias)::\n\n    >>> clf.intercept_\n    array([-9.9])\n\nWhether or not the model should use an intercept, i.e. a biased\nhyperplane, is controlled by the parameter ``fit_intercept``.\n\nThe signed distance to the hyperplane (computed as the dot product between\nthe coefficients and the input sample, plus the intercept) is given by\n:meth:`SGDClassifier.decision_function`::\n\n    >>> clf.decision_function([[2., 2.]])\n    array([29.6])\n\nThe concrete loss function can be set via the ``loss``\nparameter. :class:`SGDClassifier` supports the following loss functions:\n\n* ``loss=\"hinge\"``: (soft-margin) linear Support Vector Machine,\n* ``loss=\"modified_huber\"``: smoothed hinge loss,\n* ``loss=\"log_loss\"``: logistic regression,\n* and all regression losses below. In this case the target is encoded as :math:`-1`\n  or :math:`1`, and the problem is treated as a regression problem. The predicted\n  class then corresponds to the sign of the predicted target.\n\nPlease refer to the :ref:`mathematical section below\n<sgd_mathematical_formulation>` for formulas.\nThe first two loss functions are lazy, they only update the model\nparameters if an example violates the margin constraint, which makes\ntraining very efficient and may result in sparser models (i.e. with more zero\ncoefficients), even when :math:`L_2` penalty is used.\n\nUsing ``loss=\"log_loss\"`` or ``loss=\"modified_huber\"`` enables the\n``predict_proba`` method, which gives a vector of probability estimates\n:math:`P(y|x)` per sample :math:`x`::\n\n    >>> clf = SGDClassifier(loss=\"log_loss\", max_iter=5).fit(X, y)\n    >>> clf.predict_proba([[1., 1.]]) # doctest: +SKIP\n    array([[0.00, 0.99]])\n\nThe concrete penalty can be set via the ``penalty`` parameter.\nSGD supports the following penalties:\n\n* ``penalty=\"l2\"``: :math:`L_2` norm penalty on ``coef_``.\n* ``penalty=\"l1\"``: :math:`L_1` norm penalty on ``coef_``.\n* ``penalty=\"elasticnet\"``: Convex combination of :math:`L_2` and :math:`L_1`;\n  ``(1 - l1_ratio) * L2 + l1_ratio * L1``.\n\nThe default setting is ``penalty=\"l2\"``. The :math:`L_1` penalty leads to sparse\nsolutions, driving most coefficients to zero. The Elastic Net [#5]_ solves\nsome deficiencies of the :math:`L_1` penalty in the presence of highly correlated\nattributes. The parameter ``l1_ratio`` controls the convex combination\nof :math:`L_1` and :math:`L_2` penalty.\n\n:class:`SGDClassifier` supports multi-class classification by combining\nmultiple binary classifiers in a \"one versus all\" (OVA) scheme. For each\nof the :math:`K` classes, a binary classifier is learned that discriminates\nbetween that and all other :math:`K-1` classes. At testing time, we compute the\nconfidence score (i.e. the signed distances to the hyperplane) for each\nclassifier and choose the class with the highest confidence. The Figure\nbelow illustrates the OVA approach on the iris dataset.  The dashed\nlines represent the three OVA classifiers; the background colors show\nthe decision surface induced by the three classifiers.\n\n.. figure:: ../auto_examples/linear_model/images/sphx_glr_plot_sgd_iris_001.png\n   :target: ../auto_examples/linear_model/plot_sgd_iris.html\n   :align: center\n   :scale: 75\n\nIn the case of multi-class classification ``coef_`` is a two-dimensional\narray of shape (n_classes, n_features) and ``intercept_`` is a\none-dimensional array of shape (n_classes,). The :math:`i`-th row of ``coef_`` holds\nthe weight vector of the OVA classifier for the :math:`i`-th class; classes are\nindexed in ascending order (see attribute ``classes_``).\nNote that, in principle, since they allow to create a probability model,\n``loss=\"log_loss\"`` and ``loss=\"modified_huber\"`` are more suitable for\none-vs-all classification.\n\n:class:`SGDClassifier` supports both weighted classes and weighted\ninstances via the fit parameters ``class_weight`` and ``sample_weight``. See\nthe examples below and the docstring of :meth:`SGDClassifier.fit` for\nfurther information.\n\n:class:`SGDClassifier` supports averaged SGD (ASGD) [#4]_. Averaging can be\nenabled by setting `average=True`. ASGD performs the same updates as the\nregular SGD (see :ref:`sgd_mathematical_formulation`), but instead of using\nthe last value of the coefficients as the `coef_` attribute (i.e. the values\nof the last update), `coef_` is set instead to the **average** value of the\ncoefficients across all updates. The same is done for the `intercept_`\nattribute. When using ASGD the learning rate can be larger and even constant,\nleading on some datasets to a speed up in training time.\n\nFor classification with a logistic loss, another variant of SGD with an\naveraging strategy is available with Stochastic Average Gradient (SAG)\nalgorithm, available as a solver in :class:`LogisticRegression`.\n\n.. rubric:: Examples\n\n- :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_separating_hyperplane.py`\n- :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_iris.py`\n- :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_weighted_samples.py`\n- :ref:`sphx_glr_auto_examples_svm_plot_separating_hyperplane_unbalanced.py`\n  (See the Note in the example)"
  },
  {
    "filename": "sgd.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\sgd.rst.txt",
    "id": "sgd.rst.txt_chunk_3",
    "header": "Regression",
    "text": "Regression\n==========\n\nThe class :class:`SGDRegressor` implements a plain stochastic gradient\ndescent learning routine which supports different loss functions and\npenalties to fit linear regression models. :class:`SGDRegressor` is\nwell suited for regression problems with a large number of training\nsamples (> 10.000), for other problems we recommend :class:`Ridge`,\n:class:`Lasso`, or :class:`ElasticNet`.\n\nThe concrete loss function can be set via the ``loss``\nparameter. :class:`SGDRegressor` supports the following loss functions:\n\n* ``loss=\"squared_error\"``: Ordinary least squares,\n* ``loss=\"huber\"``: Huber loss for robust regression,\n* ``loss=\"epsilon_insensitive\"``: linear Support Vector Regression.\n\nPlease refer to the :ref:`mathematical section below\n<sgd_mathematical_formulation>` for formulas.\nThe Huber and epsilon-insensitive loss functions can be used for\nrobust regression. The width of the insensitive region has to be\nspecified via the parameter ``epsilon``. This parameter depends on the\nscale of the target variables.\n\nThe `penalty` parameter determines the regularization to be used (see\ndescription above in the classification section).\n\n:class:`SGDRegressor` also supports averaged SGD [#4]_ (here again, see\ndescription above in the classification section).\n\nFor regression with a squared loss and a :math:`L_2` penalty, another variant of\nSGD with an averaging strategy is available with Stochastic Average\nGradient (SAG) algorithm, available as a solver in :class:`Ridge`.\n\n.. rubric:: Examples\n\n- :ref:`sphx_glr_auto_examples_applications_plot_prediction_latency.py`\n\n.. _sgd_online_one_class_svm:"
  },
  {
    "filename": "sgd.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\sgd.rst.txt",
    "id": "sgd.rst.txt_chunk_4",
    "header": "Online One-Class SVM",
    "text": "Online One-Class SVM\n====================\n\nThe class :class:`sklearn.linear_model.SGDOneClassSVM` implements an online\nlinear version of the One-Class SVM using a stochastic gradient descent.\nCombined with kernel approximation techniques,\n:class:`sklearn.linear_model.SGDOneClassSVM` can be used to approximate the\nsolution of a kernelized One-Class SVM, implemented in\n:class:`sklearn.svm.OneClassSVM`, with a linear complexity in the number of\nsamples. Note that the complexity of a kernelized One-Class SVM is at best\nquadratic in the number of samples.\n:class:`sklearn.linear_model.SGDOneClassSVM` is thus well suited for datasets\nwith a large number of training samples (over 10,000) for which the SGD\nvariant can be several orders of magnitude faster.\n\n.. dropdown:: Mathematical details\n\n  Its implementation is based on the implementation of the stochastic\n  gradient descent. Indeed, the original optimization problem of the One-Class\n  SVM is given by\n\n  .. math::\n\n    \\begin{aligned}\n    \\min_{w, \\rho, \\xi} & \\quad \\frac{1}{2}\\Vert w \\Vert^2 - \\rho + \\frac{1}{\\nu n} \\sum_{i=1}^n \\xi_i \\\\\n    \\text{s.t.} & \\quad \\langle w, x_i \\rangle \\geq \\rho - \\xi_i \\quad 1 \\leq i \\leq n \\\\\n    & \\quad \\xi_i \\geq 0 \\quad 1 \\leq i \\leq n\n    \\end{aligned}\n\n  where :math:`\\nu \\in (0, 1]` is the user-specified parameter controlling the\n  proportion of outliers and the proportion of support vectors. Getting rid of\n  the slack variables :math:`\\xi_i` this problem is equivalent to\n\n  .. math::\n\n    \\min_{w, \\rho} \\frac{1}{2}\\Vert w \\Vert^2 - \\rho + \\frac{1}{\\nu n} \\sum_{i=1}^n \\max(0, \\rho - \\langle w, x_i \\rangle) \\, .\n\n  Multiplying by the constant :math:`\\nu` and introducing the intercept\n  :math:`b = 1 - \\rho` we obtain the following equivalent optimization problem\n\n  .. math::\n\n    \\min_{w, b} \\frac{\\nu}{2}\\Vert w \\Vert^2 + b\\nu + \\frac{1}{n} \\sum_{i=1}^n \\max(0, 1 - (\\langle w, x_i \\rangle + b)) \\, .\n\n  This is similar to the optimization problems studied in section\n  :ref:`sgd_mathematical_formulation` with :math:`y_i = 1, 1 \\leq i \\leq n` and\n  :math:`\\alpha = \\nu/2`, :math:`L` being the hinge loss function and :math:`R`\n  being the :math:`L_2` norm. We just need to add the term :math:`b\\nu` in the\n  optimization loop.\n\nAs :class:`SGDClassifier` and :class:`SGDRegressor`, :class:`SGDOneClassSVM`\nsupports averaged SGD. Averaging can be enabled by setting ``average=True``.\n\n.. rubric:: Examples\n\n- :ref:`sphx_glr_auto_examples_linear_model_plot_sgdocsvm_vs_ocsvm.py`"
  },
  {
    "filename": "sgd.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\sgd.rst.txt",
    "id": "sgd.rst.txt_chunk_5",
    "header": "Stochastic Gradient Descent for sparse data",
    "text": "Stochastic Gradient Descent for sparse data\n===========================================\n\n.. note:: The sparse implementation produces slightly different results\n  from the dense implementation, due to a shrunk learning rate for the\n  intercept. See :ref:`implementation_details`.\n\nThere is built-in support for sparse data given in any matrix in a format\nsupported by `scipy.sparse\n<https://docs.scipy.org/doc/scipy/reference/sparse.html>`_. For maximum\nefficiency, however, use the CSR\nmatrix format as defined in `scipy.sparse.csr_matrix\n<https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html>`_.\n\n.. rubric:: Examples\n\n- :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`"
  },
  {
    "filename": "sgd.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\sgd.rst.txt",
    "id": "sgd.rst.txt_chunk_6",
    "header": "Complexity",
    "text": "Complexity\n==========\n\nThe major advantage of SGD is its efficiency, which is basically\nlinear in the number of training examples. If :math:`X` is a matrix of size\n:math:`n \\times p` (with :math:`n` samples and :math:`p` features),\ntraining has a cost of :math:`O(k n \\bar p)`, where :math:`k` is the number\nof iterations (epochs) and :math:`\\bar p` is the average number of\nnon-zero attributes per sample.\n\nRecent theoretical results, however, show that the runtime to get some\ndesired optimization accuracy does not increase as the training set size increases."
  },
  {
    "filename": "sgd.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\sgd.rst.txt",
    "id": "sgd.rst.txt_chunk_7",
    "header": "Stopping criterion",
    "text": "Stopping criterion\n==================\n\nThe classes :class:`SGDClassifier` and :class:`SGDRegressor` provide two\ncriteria to stop the algorithm when a given level of convergence is reached:\n\n* With ``early_stopping=True``, the input data is split into a training set\n  and a validation set. The model is then fitted on the training set, and the\n  stopping criterion is based on the prediction score (using the `score`\n  method) computed on the validation set. The size of the validation set\n  can be changed with the parameter ``validation_fraction``.\n* With ``early_stopping=False``, the model is fitted on the entire input data\n  and the stopping criterion is based on the objective function computed on\n  the training data.\n\nIn both cases, the criterion is evaluated once by epoch, and the algorithm stops\nwhen the criterion does not improve ``n_iter_no_change`` times in a row. The\nimprovement is evaluated with absolute tolerance ``tol``, and the algorithm\nstops in any case after a maximum number of iterations ``max_iter``.\n\nSee :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_early_stopping.py` for an\nexample of the effects of early stopping."
  },
  {
    "filename": "sgd.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\sgd.rst.txt",
    "id": "sgd.rst.txt_chunk_8",
    "header": "Tips on Practical Use",
    "text": "Tips on Practical Use\n=====================\n\n* Stochastic Gradient Descent is sensitive to feature scaling, so it\n  is highly recommended to scale your data. For example, scale each\n  attribute on the input vector :math:`X` to :math:`[0,1]` or :math:`[-1,1]`, or standardize\n  it to have mean :math:`0` and variance :math:`1`. Note that the *same* scaling must be\n  applied to the test vector to obtain meaningful results. This can be easily\n  done using :class:`~sklearn.preprocessing.StandardScaler`::\n\n    from sklearn.preprocessing import StandardScaler\n    scaler = StandardScaler()\n    scaler.fit(X_train)  # Don't cheat - fit only on training data\n    X_train = scaler.transform(X_train)\n    X_test = scaler.transform(X_test)  # apply same transformation to test data\n\n    # Or better yet: use a pipeline!\n    from sklearn.pipeline import make_pipeline\n    est = make_pipeline(StandardScaler(), SGDClassifier())\n    est.fit(X_train)\n    est.predict(X_test)\n\n  If your attributes have an intrinsic scale (e.g. word frequencies or\n  indicator features) scaling is not needed.\n\n* Finding a reasonable regularization term :math:`\\alpha` is\n  best done using automatic hyper-parameter search, e.g.\n  :class:`~sklearn.model_selection.GridSearchCV` or\n  :class:`~sklearn.model_selection.RandomizedSearchCV`, usually in the\n  range ``10.0**-np.arange(1,7)``.\n\n* Empirically, we found that SGD converges after observing\n  approximately :math:`10^6` training samples. Thus, a reasonable first guess\n  for the number of iterations is ``max_iter = np.ceil(10**6 / n)``,\n  where ``n`` is the size of the training set.\n\n* If you apply SGD to features extracted using PCA we found that\n  it is often wise to scale the feature values by some constant `c`\n  such that the average :math:`L_2` norm of the training data equals one.\n\n* We found that Averaged SGD works best with a larger number of features\n  and a higher `eta0`.\n\n.. rubric:: References\n\n* `\"Efficient BackProp\" <http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf>`_\n  Y. LeCun, L. Bottou, G. Orr, K. M\u00fcller - In Neural Networks: Tricks\n  of the Trade 1998.\n\n.. _sgd_mathematical_formulation:"
  },
  {
    "filename": "sgd.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\sgd.rst.txt",
    "id": "sgd.rst.txt_chunk_9",
    "header": "Mathematical formulation",
    "text": "Mathematical formulation\n========================\n\nWe describe here the mathematical details of the SGD procedure. A good\noverview with convergence rates can be found in [#6]_.\n\nGiven a set of training examples :math:`\\{(x_1, y_1), \\ldots, (x_n, y_n)\\}` where\n:math:`x_i \\in \\mathbf{R}^m` and :math:`y_i \\in \\mathbf{R}`\n(:math:`y_i \\in \\{-1, 1\\}` for classification),\nour goal is to learn a linear scoring function\n:math:`f(x) = w^T x + b` with model parameters :math:`w \\in \\mathbf{R}^m` and\nintercept :math:`b \\in \\mathbf{R}`. In order to make predictions for binary\nclassification, we simply look at the sign of :math:`f(x)`. To find the model\nparameters, we minimize the regularized training error given by\n\n.. math::\n\n    E(w,b) = \\frac{1}{n}\\sum_{i=1}^{n} L(y_i, f(x_i)) + \\alpha R(w)\n\nwhere :math:`L` is a loss function that measures model (mis)fit and\n:math:`R` is a regularization term (aka penalty) that penalizes model\ncomplexity; :math:`\\alpha > 0` is a non-negative hyperparameter that controls\nthe regularization strength.\n\n.. dropdown:: Loss functions details\n\n  Different choices for :math:`L` entail different classifiers or regressors:\n\n  - Hinge (soft-margin): equivalent to Support Vector Classification.\n    :math:`L(y_i, f(x_i)) = \\max(0, 1 - y_i f(x_i))`.\n  - Perceptron:\n    :math:`L(y_i, f(x_i)) = \\max(0, - y_i f(x_i))`.\n  - Modified Huber:\n    :math:`L(y_i, f(x_i)) = \\max(0, 1 - y_i f(x_i))^2` if :math:`y_i f(x_i) >\n    -1`, and :math:`L(y_i, f(x_i)) = -4 y_i f(x_i)` otherwise.\n  - Log Loss: equivalent to Logistic Regression.\n    :math:`L(y_i, f(x_i)) = \\log(1 + \\exp (-y_i f(x_i)))`.\n  - Squared Error: Linear regression (Ridge or Lasso depending on\n    :math:`R`).\n    :math:`L(y_i, f(x_i)) = \\frac{1}{2}(y_i - f(x_i))^2`.\n  - Huber: less sensitive to outliers than least-squares. It is equivalent to\n    least squares when :math:`|y_i - f(x_i)| \\leq \\varepsilon`, and\n    :math:`L(y_i, f(x_i)) = \\varepsilon |y_i - f(x_i)| - \\frac{1}{2}\n    \\varepsilon^2` otherwise.\n  - Epsilon-Insensitive: (soft-margin) equivalent to Support Vector Regression.\n    :math:`L(y_i, f(x_i)) = \\max(0, |y_i - f(x_i)| - \\varepsilon)`.\n\nAll of the above loss functions can be regarded as an upper bound on the\nmisclassification error (Zero-one loss) as shown in the Figure below.\n\n.. figure:: ../auto_examples/linear_model/images/sphx_glr_plot_sgd_loss_functions_001.png\n    :target: ../auto_examples/linear_model/plot_sgd_loss_functions.html\n    :align: center\n    :scale: 75\n\nPopular choices for the regularization term :math:`R` (the `penalty`\nparameter) include:\n\n- :math:`L_2` norm: :math:`R(w) := \\frac{1}{2} \\sum_{j=1}^{m} w_j^2 = ||w||_2^2`,\n- :math:`L_1` norm: :math:`R(w) := \\sum_{j=1}^{m} |w_j|`, which leads to sparse\n  solutions.\n- Elastic Net: :math:`R(w) := \\frac{\\rho}{2} \\sum_{j=1}^{n} w_j^2 +\n  (1-\\rho) \\sum_{j=1}^{m} |w_j|`, a convex combination of :math:`L_2` and :math:`L_1`, where\n  :math:`\\rho` is given by ``1 - l1_ratio``.\n\nThe Figure below shows the contours of the different regularization terms\nin a 2-dimensional parameter space (:math:`m=2`) when :math:`R(w) = 1`.\n\n.. figure:: ../auto_examples/linear_model/images/sphx_glr_plot_sgd_penalties_001.png\n    :target: ../auto_examples/linear_model/plot_sgd_penalties.html\n    :align: center\n    :scale: 75"
  },
  {
    "filename": "sgd.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\sgd.rst.txt",
    "id": "sgd.rst.txt_chunk_10",
    "header": "SGD",
    "text": "SGD\n---\n\nStochastic gradient descent is an optimization method for unconstrained\noptimization problems. In contrast to (batch) gradient descent, SGD\napproximates the true gradient of :math:`E(w,b)` by considering a\nsingle training example at a time.\n\nThe class :class:`SGDClassifier` implements a first-order SGD learning\nroutine.  The algorithm iterates over the training examples and for each\nexample updates the model parameters according to the update rule given by\n\n.. math::\n\n    w \\leftarrow w - \\eta \\left[\\alpha \\frac{\\partial R(w)}{\\partial w}\n    + \\frac{\\partial L(w^T x_i + b, y_i)}{\\partial w}\\right]\n\nwhere :math:`\\eta` is the learning rate which controls the step-size in\nthe parameter space.  The intercept :math:`b` is updated similarly but\nwithout regularization (and with additional decay for sparse matrices, as\ndetailed in :ref:`implementation_details`).\n\nThe learning rate :math:`\\eta` can be either constant or gradually decaying. For\nclassification, the default learning rate schedule (``learning_rate='optimal'``)\nis given by\n\n.. math::\n\n    \\eta^{(t)} = \\frac {1}{\\alpha  (t_0 + t)}\n\nwhere :math:`t` is the time step (there are a total of `n_samples * n_iter`\ntime steps), :math:`t_0` is determined based on a heuristic proposed by L\u00e9on Bottou\nsuch that the expected initial updates are comparable with the expected\nsize of the weights (this assumes that the norm of the training samples is\napproximately 1). The exact definition can be found in ``_init_t`` in `BaseSGD`.\n\n\nFor regression the default learning rate schedule is inverse scaling\n(``learning_rate='invscaling'``), given by\n\n.. math::\n\n    \\eta^{(t)} = \\frac{\\eta_0}{t^{power\\_t}}\n\nwhere :math:`\\eta_0` and :math:`power\\_t` are hyperparameters chosen by the\nuser via ``eta0`` and ``power_t``, respectively.\n\nFor a constant learning rate use ``learning_rate='constant'`` and use ``eta0``\nto specify the learning rate.\n\nFor an adaptively decreasing learning rate, use ``learning_rate='adaptive'``\nand use ``eta0`` to specify the starting learning rate. When the stopping\ncriterion is reached, the learning rate is divided by 5, and the algorithm\ndoes not stop. The algorithm stops when the learning rate goes below `1e-6`.\n\nThe model parameters can be accessed through the ``coef_`` and\n``intercept_`` attributes: ``coef_`` holds the weights :math:`w` and\n``intercept_`` holds :math:`b`.\n\nWhen using Averaged SGD (with the `average` parameter), `coef_` is set to the\naverage weight across all updates:\n`coef_` :math:`= \\frac{1}{T} \\sum_{t=0}^{T-1} w^{(t)}`,\nwhere :math:`T` is the total number of updates, found in the `t_` attribute.\n\n.. _implementation_details:"
  },
  {
    "filename": "sgd.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\sgd.rst.txt",
    "id": "sgd.rst.txt_chunk_11",
    "header": "Implementation details",
    "text": "Implementation details\n======================\n\nThe implementation of SGD is influenced by the `Stochastic Gradient SVM` of\n[#1]_.\nSimilar to SvmSGD,\nthe weight vector is represented as the product of a scalar and a vector\nwhich allows an efficient weight update in the case of :math:`L_2` regularization.\nIn the case of sparse input `X`, the intercept is updated with a\nsmaller learning rate (multiplied by 0.01) to account for the fact that\nit is updated more frequently. Training examples are picked up sequentially\nand the learning rate is lowered after each observed example. We adopted the\nlearning rate schedule from [#2]_.\nFor multi-class classification, a \"one versus all\" approach is used.\nWe use the truncated gradient algorithm proposed in [#3]_\nfor :math:`L_1` regularization (and the Elastic Net).\nThe code is written in Cython.\n\n.. rubric:: References\n\n.. [#1] `\"Stochastic Gradient Descent\"\n  <https://leon.bottou.org/projects/sgd>`_ L. Bottou - Website, 2010.\n\n.. [#2] :doi:`\"Pegasos: Primal estimated sub-gradient solver for svm\"\n  <10.1145/1273496.1273598>`\n  S. Shalev-Shwartz, Y. Singer, N. Srebro - In Proceedings of ICML '07.\n\n.. [#3] `\"Stochastic gradient descent training for l1-regularized\n  log-linear models with cumulative penalty\"\n  <https://www.aclweb.org/anthology/P/P09/P09-1054.pdf>`_\n  Y. Tsuruoka, J. Tsujii, S. Ananiadou - In Proceedings of the AFNLP/ACL'09.\n\n.. [#4] :arxiv:`\"Towards Optimal One Pass Large Scale Learning with\n  Averaged Stochastic Gradient Descent\"\n  <1107.2490v2>`. Xu, Wei (2011)\n\n.. [#5] :doi:`\"Regularization and variable selection via the elastic net\"\n  <10.1111/j.1467-9868.2005.00503.x>`\n  H. Zou, T. Hastie - Journal of the Royal Statistical Society Series B,\n  67 (2), 301-320.\n\n.. [#6] :doi:`\"Solving large scale linear prediction problems using stochastic\n  gradient descent algorithms\" <10.1145/1015330.1015332>`\n  T. Zhang - In Proceedings of ICML '04."
  },
  {
    "filename": "svm.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\svm.rst.txt",
    "id": "svm.rst.txt_chunk_0",
    "header": ".. _svm:",
    "text": ".. _svm:\n\n======================="
  },
  {
    "filename": "svm.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\svm.rst.txt",
    "id": "svm.rst.txt_chunk_1",
    "header": "Support Vector Machines",
    "text": "Support Vector Machines\n=======================\n\n.. TODO: Describe tol parameter\n.. TODO: Describe max_iter parameter\n\n.. currentmodule:: sklearn.svm\n\n**Support vector machines (SVMs)** are a set of supervised learning\nmethods used for :ref:`classification <svm_classification>`,\n:ref:`regression <svm_regression>` and :ref:`outliers detection\n<svm_outlier_detection>`.\n\nThe advantages of support vector machines are:\n\n- Effective in high dimensional spaces.\n\n- Still effective in cases where number of dimensions is greater\n  than the number of samples.\n\n- Uses a subset of training points in the decision function (called\n  support vectors), so it is also memory efficient.\n\n- Versatile: different :ref:`svm_kernels` can be\n  specified for the decision function. Common kernels are\n  provided, but it is also possible to specify custom kernels.\n\nThe disadvantages of support vector machines include:\n\n- If the number of features is much greater than the number of\n  samples, avoid over-fitting in choosing :ref:`svm_kernels` and regularization\n  term is crucial.\n\n- SVMs do not directly provide probability estimates, these are\n  calculated using an expensive five-fold cross-validation\n  (see :ref:`Scores and probabilities <scores_probabilities>`, below).\n\nThe support vector machines in scikit-learn support both dense\n(``numpy.ndarray`` and convertible to that by ``numpy.asarray``) and\nsparse (any ``scipy.sparse``) sample vectors as input. However, to use\nan SVM to make predictions for sparse data, it must have been fit on such\ndata. For optimal performance, use C-ordered ``numpy.ndarray`` (dense) or\n``scipy.sparse.csr_matrix`` (sparse) with ``dtype=float64``.\n\n\n.. _svm_classification:"
  },
  {
    "filename": "svm.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\svm.rst.txt",
    "id": "svm.rst.txt_chunk_2",
    "header": "Classification",
    "text": "Classification\n==============\n\n:class:`SVC`, :class:`NuSVC` and :class:`LinearSVC` are classes\ncapable of performing binary and multi-class classification on a dataset.\n\n\n.. figure:: ../auto_examples/svm/images/sphx_glr_plot_iris_svc_001.png\n   :target: ../auto_examples/svm/plot_iris_svc.html\n   :align: center\n\n\n:class:`SVC` and :class:`NuSVC` are similar methods, but accept slightly\ndifferent sets of parameters and have different mathematical formulations (see\nsection :ref:`svm_mathematical_formulation`). On the other hand,\n:class:`LinearSVC` is another (faster) implementation of Support Vector\nClassification for the case of a linear kernel. It also\nlacks some of the attributes of :class:`SVC` and :class:`NuSVC`, like\n`support_`. :class:`LinearSVC` uses `squared_hinge` loss and due to its\nimplementation in `liblinear` it also regularizes the intercept, if considered.\nThis effect can however be reduced by carefully fine tuning its\n`intercept_scaling` parameter, which allows the intercept term to have a\ndifferent regularization behavior compared to the other features. The\nclassification results and score can therefore differ from the other two\nclassifiers.\n\nAs other classifiers, :class:`SVC`, :class:`NuSVC` and\n:class:`LinearSVC` take as input two arrays: an array `X` of shape\n`(n_samples, n_features)` holding the training samples, and an array `y` of\nclass labels (strings or integers), of shape `(n_samples)`::\n\n\n    >>> from sklearn import svm\n    >>> X = [[0, 0], [1, 1]]\n    >>> y = [0, 1]\n    >>> clf = svm.SVC()\n    >>> clf.fit(X, y)\n    SVC()\n\nAfter being fitted, the model can then be used to predict new values::\n\n    >>> clf.predict([[2., 2.]])\n    array([1])\n\nSVMs decision function (detailed in the :ref:`svm_mathematical_formulation`)\ndepends on some subset of the training data, called the support vectors. Some\nproperties of these support vectors can be found in attributes\n``support_vectors_``, ``support_`` and ``n_support_``::\n\n    >>> # get support vectors\n    >>> clf.support_vectors_\n    array([[0., 0.],\n           [1., 1.]])\n    >>> # get indices of support vectors\n    >>> clf.support_\n    array([0, 1]...)\n    >>> # get number of support vectors for each class\n    >>> clf.n_support_\n    array([1, 1]...)\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_svm_plot_separating_hyperplane.py`\n* :ref:`sphx_glr_auto_examples_svm_plot_svm_anova.py`\n* :ref:`sphx_glr_auto_examples_classification_plot_classification_probability.py`\n\n.. _svm_multi_class:"
  },
  {
    "filename": "svm.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\svm.rst.txt",
    "id": "svm.rst.txt_chunk_3",
    "header": "Multi-class classification",
    "text": "Multi-class classification\n--------------------------\n\n:class:`SVC` and :class:`NuSVC` implement the \"one-versus-one\" (\"ovo\")\napproach for multi-class classification, which constructs\n``n_classes * (n_classes - 1) / 2``\nclassifiers, each trained on data from two classes. Internally, the solver\nalways uses this \"ovo\" strategy to train the models. However, by default, the\n`decision_function_shape` parameter is set to `\"ovr\"` (\"one-vs-rest\"), to have\na consistent interface with other classifiers by monotonically transforming the \"ovo\"\ndecision function into an \"ovr\" decision function of shape ``(n_samples, n_classes)``.\n\n    >>> X = [[0], [1], [2], [3]]\n    >>> Y = [0, 1, 2, 3]\n    >>> clf = svm.SVC(decision_function_shape='ovo')\n    >>> clf.fit(X, Y)\n    SVC(decision_function_shape='ovo')\n    >>> dec = clf.decision_function([[1]])\n    >>> dec.shape[1] # 6 classes: 4*3/2 = 6\n    6\n    >>> clf.decision_function_shape = \"ovr\"\n    >>> dec = clf.decision_function([[1]])\n    >>> dec.shape[1] # 4 classes\n    4\n\nOn the other hand, :class:`LinearSVC` implements a \"one-vs-rest\" (\"ovr\")\nmulti-class strategy, thus training `n_classes` models.\n\n    >>> lin_clf = svm.LinearSVC()\n    >>> lin_clf.fit(X, Y)\n    LinearSVC()\n    >>> dec = lin_clf.decision_function([[1]])\n    >>> dec.shape[1]\n    4\n\nSee :ref:`svm_mathematical_formulation` for a complete description of\nthe decision function.\n\n.. dropdown:: Details on multi-class strategies\n\n  Note that the :class:`LinearSVC` also implements an alternative multi-class\n  strategy, the so-called multi-class SVM formulated by Crammer and Singer\n  [#8]_, by using the option ``multi_class='crammer_singer'``. In practice,\n  one-vs-rest classification is usually preferred, since the results are mostly\n  similar, but the runtime is significantly less.\n\n  For \"one-vs-rest\" :class:`LinearSVC` the attributes ``coef_`` and ``intercept_``\n  have the shape ``(n_classes, n_features)`` and ``(n_classes,)`` respectively.\n  Each row of the coefficients corresponds to one of the ``n_classes``\n  \"one-vs-rest\" classifiers and similar for the intercepts, in the\n  order of the \"one\" class.\n\n  In the case of \"one-vs-one\" :class:`SVC` and :class:`NuSVC`, the layout of\n  the attributes is a little more involved. In the case of a linear\n  kernel, the attributes ``coef_`` and ``intercept_`` have the shape\n  ``(n_classes * (n_classes - 1) / 2, n_features)`` and ``(n_classes *\n  (n_classes - 1) / 2)`` respectively. This is similar to the layout for\n  :class:`LinearSVC` described above, with each row now corresponding\n  to a binary classifier. The order for classes\n  0 to n is \"0 vs 1\", \"0 vs 2\" , ... \"0 vs n\", \"1 vs 2\", \"1 vs 3\", \"1 vs n\", . .\n  . \"n-1 vs n\".\n\n  The shape of ``dual_coef_`` is ``(n_classes-1, n_SV)`` with\n  a somewhat hard to grasp layout.\n  The columns correspond to the support vectors involved in any\n  of the ``n_classes * (n_classes - 1) / 2`` \"one-vs-one\" classifiers.\n  Each support vector ``v`` has a dual coefficient in each of the\n  ``n_classes - 1`` classifiers comparing the class of ``v`` against another class.\n  Note that some, but not all, of these dual coefficients, may be zero.\n  The ``n_classes - 1`` entries in each column are these dual coefficients,\n  ordered by the opposing class.\n\n  This might be clearer with an example: consider a three class problem with\n  class 0 having three support vectors\n  :math:`v^{0}_0, v^{1}_0, v^{2}_0` and class 1 and 2 having two support vectors\n  :math:`v^{0}_1, v^{1}_1` and :math:`v^{0}_2, v^{1}_2` respectively.  For each\n  support vector :math:`v^{j}_i`, there are two dual coefficients.  Let's call\n  the coefficient of support vector :math:`v^{j}_i` in the classifier between\n  classes :math:`i` and :math:`k` :math:`\\alpha^{j}_{i,k}`.\n  Then ``dual_coef_`` looks like this:\n\n  +------------------------+------------------------+------------------------+------------------------+------------------------+------------------------+------------------------+\n  |:math:`\\alpha^{0}_{0,1}`|:math:`\\alpha^{1}_{0,1}`|:math:`\\alpha^{2}_{0,1}`|:math:`\\alpha^{0}_{1,0}`|:math:`\\alpha^{1}_{1,0}`|:math:`\\alpha^{0}_{2,0}`|:math:`\\alpha^{1}_{2,0}`|\n  +------------------------+------------------------+------------------------+------------------------+------------------------+------------------------+------------------------+\n  |:math:`\\alpha^{0}_{0,2}`|:math:`\\alpha^{1}_{0,2}`|:math:`\\alpha^{2}_{0,2}`|:math:`\\alpha^{0}_{1,2}`|:math:`\\alpha^{1}_{1,2}`|:math:`\\alpha^{0}_{2,1}`|:math:`\\alpha^{1}_{2,1}`|\n  +------------------------+------------------------+------------------------+------------------------+------------------------+------------------------+------------------------+\n  |Coefficients                                                              |Coefficients                                     |Coefficients                                     |\n  |for SVs of class 0                                                        |for SVs of class 1                               |for SVs of class 2                               |\n  +--------------------------------------------------------------------------+-------------------------------------------------+-------------------------------------------------+\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_svm_plot_iris_svc.py`\n\n.. _scores_probabilities:"
  },
  {
    "filename": "svm.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\svm.rst.txt",
    "id": "svm.rst.txt_chunk_4",
    "header": "Scores and probabilities",
    "text": "Scores and probabilities\n------------------------\n\nThe ``decision_function`` method of :class:`SVC` and :class:`NuSVC` gives\nper-class scores for each sample (or a single score per sample in the binary\ncase). When the constructor option ``probability`` is set to ``True``,\nclass membership probability estimates (from the methods ``predict_proba`` and\n``predict_log_proba``) are enabled. In the binary case, the probabilities are\ncalibrated using Platt scaling [#1]_: logistic regression on the SVM's scores,\nfit by an additional cross-validation on the training data.\nIn the multiclass case, this is extended as per [#2]_.\n\n.. note::\n\n  The same probability calibration procedure is available for all estimators\n  via the :class:`~sklearn.calibration.CalibratedClassifierCV` (see\n  :ref:`calibration`). In the case of :class:`SVC` and :class:`NuSVC`, this\n  procedure is builtin to `libsvm`_ which is used under the hood, so it does\n  not rely on scikit-learn's\n  :class:`~sklearn.calibration.CalibratedClassifierCV`.\n\nThe cross-validation involved in Platt scaling\nis an expensive operation for large datasets.\nIn addition, the probability estimates may be inconsistent with the scores:\n\n- the \"argmax\" of the scores may not be the argmax of the probabilities\n- in binary classification, a sample may be labeled by ``predict`` as\n  belonging to the positive class even if the output of `predict_proba` is\n  less than 0.5; and similarly, it could be labeled as negative even if the\n  output of `predict_proba` is more than 0.5.\n\nPlatt's method is also known to have theoretical issues.\nIf confidence scores are required, but these do not have to be probabilities,\nthen it is advisable to set ``probability=False``\nand use ``decision_function`` instead of ``predict_proba``.\n\nPlease note that when ``decision_function_shape='ovr'`` and ``n_classes > 2``,\nunlike ``decision_function``, the ``predict`` method does not try to break ties\nby default. You can set ``break_ties=True`` for the output of ``predict`` to be\nthe same as ``np.argmax(clf.decision_function(...), axis=1)``, otherwise the\nfirst class among the tied classes will always be returned; but have in mind\nthat it comes with a computational cost. See\n:ref:`sphx_glr_auto_examples_svm_plot_svm_tie_breaking.py` for an example on\ntie breaking."
  },
  {
    "filename": "svm.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\svm.rst.txt",
    "id": "svm.rst.txt_chunk_5",
    "header": "Unbalanced problems",
    "text": "Unbalanced problems\n--------------------\n\nIn problems where it is desired to give more importance to certain\nclasses or certain individual samples, the parameters ``class_weight`` and\n``sample_weight`` can be used.\n\n:class:`SVC` (but not :class:`NuSVC`) implements the parameter\n``class_weight`` in the ``fit`` method. It's a dictionary of the form\n``{class_label : value}``, where value is a floating point number > 0\nthat sets the parameter ``C`` of class ``class_label`` to ``C * value``.\nThe figure below illustrates the decision boundary of an unbalanced problem,\nwith and without weight correction.\n\n.. figure:: ../auto_examples/svm/images/sphx_glr_plot_separating_hyperplane_unbalanced_001.png\n   :target: ../auto_examples/svm/plot_separating_hyperplane_unbalanced.html\n   :align: center\n   :scale: 75\n\n\n:class:`SVC`, :class:`NuSVC`, :class:`SVR`, :class:`NuSVR`, :class:`LinearSVC`,\n:class:`LinearSVR` and :class:`OneClassSVM` implement also weights for\nindividual samples in the `fit` method through the ``sample_weight`` parameter.\nSimilar to ``class_weight``, this sets the parameter ``C`` for the i-th\nexample to ``C * sample_weight[i]``, which will encourage the classifier to\nget these samples right. The figure below illustrates the effect of sample\nweighting on the decision boundary. The size of the circles is proportional\nto the sample weights:\n\n.. figure:: ../auto_examples/svm/images/sphx_glr_plot_weighted_samples_001.png\n   :target: ../auto_examples/svm/plot_weighted_samples.html\n   :align: center\n   :scale: 75\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_svm_plot_separating_hyperplane_unbalanced.py`\n* :ref:`sphx_glr_auto_examples_svm_plot_weighted_samples.py`\n\n\n.. _svm_regression:"
  },
  {
    "filename": "svm.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\svm.rst.txt",
    "id": "svm.rst.txt_chunk_6",
    "header": "Regression",
    "text": "Regression\n==========\n\nThe method of Support Vector Classification can be extended to solve\nregression problems. This method is called Support Vector Regression.\n\nThe model produced by support vector classification (as described\nabove) depends only on a subset of the training data, because the cost\nfunction for building the model does not care about training points\nthat lie beyond the margin. Analogously, the model produced by Support\nVector Regression depends only on a subset of the training data,\nbecause the cost function ignores samples whose prediction is close to their\ntarget.\n\nThere are three different implementations of Support Vector Regression:\n:class:`SVR`, :class:`NuSVR` and :class:`LinearSVR`. :class:`LinearSVR`\nprovides a faster implementation than :class:`SVR` but only considers the\nlinear kernel, while :class:`NuSVR` implements a slightly different formulation\nthan :class:`SVR` and :class:`LinearSVR`. Due to its implementation in\n`liblinear` :class:`LinearSVR` also regularizes the intercept, if considered.\nThis effect can however be reduced by carefully fine tuning its\n`intercept_scaling` parameter, which allows the intercept term to have a\ndifferent regularization behavior compared to the other features. The\nclassification results and score can therefore differ from the other two\nclassifiers. See :ref:`svm_implementation_details` for further details.\n\nAs with classification classes, the fit method will take as\nargument vectors X, y, only that in this case y is expected to have\nfloating point values instead of integer values::\n\n    >>> from sklearn import svm\n    >>> X = [[0, 0], [2, 2]]\n    >>> y = [0.5, 2.5]\n    >>> regr = svm.SVR()\n    >>> regr.fit(X, y)\n    SVR()\n    >>> regr.predict([[1, 1]])\n    array([1.5])\n\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_svm_plot_svm_regression.py`\n\n.. _svm_outlier_detection:"
  },
  {
    "filename": "svm.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\svm.rst.txt",
    "id": "svm.rst.txt_chunk_7",
    "header": "Density estimation, novelty detection",
    "text": "Density estimation, novelty detection\n=======================================\n\nThe class :class:`OneClassSVM` implements a One-Class SVM which is used in\noutlier detection.\n\nSee :ref:`outlier_detection` for the description and usage of OneClassSVM."
  },
  {
    "filename": "svm.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\svm.rst.txt",
    "id": "svm.rst.txt_chunk_8",
    "header": "Complexity",
    "text": "Complexity\n==========\n\nSupport Vector Machines are powerful tools, but their compute and\nstorage requirements increase rapidly with the number of training\nvectors. The core of an SVM is a quadratic programming problem (QP),\nseparating support vectors from the rest of the training data. The QP\nsolver used by the `libsvm`_-based implementation scales between\n:math:`O(n_{features} \\times n_{samples}^2)` and\n:math:`O(n_{features} \\times n_{samples}^3)` depending on how efficiently\nthe `libsvm`_ cache is used in practice (dataset dependent). If the data\nis very sparse :math:`n_{features}` should be replaced by the average number\nof non-zero features in a sample vector.\n\nFor the linear case, the algorithm used in\n:class:`LinearSVC` by the `liblinear`_ implementation is much more\nefficient than its `libsvm`_-based :class:`SVC` counterpart and can\nscale almost linearly to millions of samples and/or features."
  },
  {
    "filename": "svm.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\svm.rst.txt",
    "id": "svm.rst.txt_chunk_9",
    "header": "Tips on Practical Use",
    "text": "Tips on Practical Use\n=====================\n\n\n* **Avoiding data copy**: For :class:`SVC`, :class:`SVR`, :class:`NuSVC` and\n  :class:`NuSVR`, if the data passed to certain methods is not C-ordered\n  contiguous and double precision, it will be copied before calling the\n  underlying C implementation. You can check whether a given numpy array is\n  C-contiguous by inspecting its ``flags`` attribute.\n\n  For :class:`LinearSVC` (and :class:`LogisticRegression\n  <sklearn.linear_model.LogisticRegression>`) any input passed as a numpy\n  array will be copied and converted to the `liblinear`_ internal sparse data\n  representation (double precision floats and int32 indices of non-zero\n  components). If you want to fit a large-scale linear classifier without\n  copying a dense numpy C-contiguous double precision array as input, we\n  suggest to use the :class:`SGDClassifier\n  <sklearn.linear_model.SGDClassifier>` class instead.  The objective\n  function can be configured to be almost the same as the :class:`LinearSVC`\n  model.\n\n* **Kernel cache size**: For :class:`SVC`, :class:`SVR`, :class:`NuSVC` and\n  :class:`NuSVR`, the size of the kernel cache has a strong impact on run\n  times for larger problems.  If you have enough RAM available, it is\n  recommended to set ``cache_size`` to a higher value than the default of\n  200(MB), such as 500(MB) or 1000(MB).\n\n\n* **Setting C**: ``C`` is ``1`` by default and it's a reasonable default\n  choice.  If you have a lot of noisy observations you should decrease it:\n  decreasing C corresponds to more regularization.\n\n  :class:`LinearSVC` and :class:`LinearSVR` are less sensitive to ``C`` when\n  it becomes large, and prediction results stop improving after a certain\n  threshold. Meanwhile, larger ``C`` values will take more time to train,\n  sometimes up to 10 times longer, as shown in [#3]_.\n\n* Support Vector Machine algorithms are not scale invariant, so **it\n  is highly recommended to scale your data**. For example, scale each\n  attribute on the input vector X to [0,1] or [-1,+1], or standardize it\n  to have mean 0 and variance 1. Note that the *same* scaling must be\n  applied to the test vector to obtain meaningful results. This can be done\n  easily by using a :class:`~sklearn.pipeline.Pipeline`::\n\n      >>> from sklearn.pipeline import make_pipeline\n      >>> from sklearn.preprocessing import StandardScaler\n      >>> from sklearn.svm import SVC\n\n      >>> clf = make_pipeline(StandardScaler(), SVC())\n\n  See section :ref:`preprocessing` for more details on scaling and\n  normalization.\n\n.. _shrinking_svm:\n\n* Regarding the `shrinking` parameter, quoting [#4]_: *We found that if the\n  number of iterations is large, then shrinking can shorten the training\n  time. However, if we loosely solve the optimization problem (e.g., by\n  using a large stopping tolerance), the code without using shrinking may\n  be much faster*\n\n* Parameter ``nu`` in :class:`NuSVC`/:class:`OneClassSVM`/:class:`NuSVR`\n  approximates the fraction of training errors and support vectors.\n\n* In :class:`SVC`, if the data is unbalanced (e.g. many\n  positive and few negative), set ``class_weight='balanced'`` and/or try\n  different penalty parameters ``C``.\n\n* **Randomness of the underlying implementations**: The underlying\n  implementations of :class:`SVC` and :class:`NuSVC` use a random number\n  generator only to shuffle the data for probability estimation (when\n  ``probability`` is set to ``True``). This randomness can be controlled\n  with the ``random_state`` parameter. If ``probability`` is set to ``False``\n  these estimators are not random and ``random_state`` has no effect on the\n  results. The underlying :class:`OneClassSVM` implementation is similar to\n  the ones of :class:`SVC` and :class:`NuSVC`. As no probability estimation\n  is provided for :class:`OneClassSVM`, it is not random.\n\n  The underlying :class:`LinearSVC` implementation uses a random number\n  generator to select features when fitting the model with a dual coordinate\n  descent (i.e. when ``dual`` is set to ``True``). It is thus not uncommon\n  to have slightly different results for the same input data. If that\n  happens, try with a smaller `tol` parameter. This randomness can also be\n  controlled with the ``random_state`` parameter. When ``dual`` is\n  set to ``False`` the underlying implementation of :class:`LinearSVC` is\n  not random and ``random_state`` has no effect on the results.\n\n* Using L1 penalization as provided by ``LinearSVC(penalty='l1',\n  dual=False)`` yields a sparse solution, i.e. only a subset of feature\n  weights is different from zero and contribute to the decision function.\n  Increasing ``C`` yields a more complex model (more features are selected).\n  The ``C`` value that yields a \"null\" model (all weights equal to zero) can\n  be calculated using :func:`l1_min_c`.\n\n\n.. _svm_kernels:"
  },
  {
    "filename": "svm.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\svm.rst.txt",
    "id": "svm.rst.txt_chunk_10",
    "header": "Kernel functions",
    "text": "Kernel functions\n================\n\nThe *kernel function* can be any of the following:\n\n* linear: :math:`\\langle x, x'\\rangle`.\n\n* polynomial: :math:`(\\gamma \\langle x, x'\\rangle + r)^d`, where\n  :math:`d` is specified by parameter ``degree``, :math:`r` by ``coef0``.\n\n* rbf: :math:`\\exp(-\\gamma \\|x-x'\\|^2)`, where :math:`\\gamma` is\n  specified by parameter ``gamma``, must be greater than 0.\n\n* sigmoid :math:`\\tanh(\\gamma \\langle x,x'\\rangle + r)`,\n  where :math:`r` is specified by ``coef0``.\n\nDifferent kernels are specified by the `kernel` parameter::\n\n    >>> linear_svc = svm.SVC(kernel='linear')\n    >>> linear_svc.kernel\n    'linear'\n    >>> rbf_svc = svm.SVC(kernel='rbf')\n    >>> rbf_svc.kernel\n    'rbf'\n\nSee also :ref:`kernel_approximation` for a solution to use RBF kernels that is much faster and more scalable."
  },
  {
    "filename": "svm.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\svm.rst.txt",
    "id": "svm.rst.txt_chunk_11",
    "header": "Parameters of the RBF Kernel",
    "text": "Parameters of the RBF Kernel\n----------------------------\n\nWhen training an SVM with the *Radial Basis Function* (RBF) kernel, two\nparameters must be considered: ``C`` and ``gamma``.  The parameter ``C``,\ncommon to all SVM kernels, trades off misclassification of training examples\nagainst simplicity of the decision surface. A low ``C`` makes the decision\nsurface smooth, while a high ``C`` aims at classifying all training examples\ncorrectly.  ``gamma`` defines how much influence a single training example has.\nThe larger ``gamma`` is, the closer other examples must be to be affected.\n\nProper choice of ``C`` and ``gamma`` is critical to the SVM's performance.  One\nis advised to use :class:`~sklearn.model_selection.GridSearchCV` with\n``C`` and ``gamma`` spaced exponentially far apart to choose good values.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_svm_plot_rbf_parameters.py`\n* :ref:`sphx_glr_auto_examples_svm_plot_svm_scale_c.py`"
  },
  {
    "filename": "svm.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\svm.rst.txt",
    "id": "svm.rst.txt_chunk_12",
    "header": "Custom Kernels",
    "text": "Custom Kernels\n--------------\n\nYou can define your own kernels by either giving the kernel as a\npython function or by precomputing the Gram matrix.\n\nClassifiers with custom kernels behave the same way as any other\nclassifiers, except that:\n\n* Field ``support_vectors_`` is now empty, only indices of support\n  vectors are stored in ``support_``\n\n* A reference (and not a copy) of the first argument in the ``fit()``\n  method is stored for future reference. If that array changes between the\n  use of ``fit()`` and ``predict()`` you will have unexpected results.\n\n\n.. dropdown:: Using Python functions as kernels\n\n  You can use your own defined kernels by passing a function to the\n  ``kernel`` parameter.\n\n  Your kernel must take as arguments two matrices of shape\n  ``(n_samples_1, n_features)``, ``(n_samples_2, n_features)``\n  and return a kernel matrix of shape ``(n_samples_1, n_samples_2)``.\n\n  The following code defines a linear kernel and creates a classifier\n  instance that will use that kernel::\n\n      >>> import numpy as np\n      >>> from sklearn import svm\n      >>> def my_kernel(X, Y):\n      ...     return np.dot(X, Y.T)\n      ...\n      >>> clf = svm.SVC(kernel=my_kernel)\n\n\n.. dropdown:: Using the Gram matrix\n\n  You can pass pre-computed kernels by using the ``kernel='precomputed'``\n  option. You should then pass Gram matrix instead of X to the `fit` and\n  `predict` methods. The kernel values between *all* training vectors and the\n  test vectors must be provided:\n\n      >>> import numpy as np\n      >>> from sklearn.datasets import make_classification\n      >>> from sklearn.model_selection import train_test_split\n      >>> from sklearn import svm\n      >>> X, y = make_classification(n_samples=10, random_state=0)\n      >>> X_train , X_test , y_train, y_test = train_test_split(X, y, random_state=0)\n      >>> clf = svm.SVC(kernel='precomputed')\n      >>> # linear kernel computation\n      >>> gram_train = np.dot(X_train, X_train.T)\n      >>> clf.fit(gram_train, y_train)\n      SVC(kernel='precomputed')\n      >>> # predict on training examples\n      >>> gram_test = np.dot(X_test, X_train.T)\n      >>> clf.predict(gram_test)\n      array([0, 1, 0])\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_svm_plot_custom_kernel.py`\n\n.. _svm_mathematical_formulation:"
  },
  {
    "filename": "svm.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\svm.rst.txt",
    "id": "svm.rst.txt_chunk_13",
    "header": "Mathematical formulation",
    "text": "Mathematical formulation\n========================\n\nA support vector machine constructs a hyper-plane or set of hyper-planes in a\nhigh or infinite dimensional space, which can be used for\nclassification, regression or other tasks. Intuitively, a good\nseparation is achieved by the hyper-plane that has the largest distance\nto the nearest training data points of any class (so-called functional\nmargin), since in general the larger the margin the lower the\ngeneralization error of the classifier. The figure below shows the decision\nfunction for a linearly separable problem, with three samples on the\nmargin boundaries, called \"support vectors\":\n\n.. figure:: ../auto_examples/svm/images/sphx_glr_plot_separating_hyperplane_001.png\n   :align: center\n   :scale: 75\n\nIn general, when the problem isn't linearly separable, the support vectors\nare the samples *within* the margin boundaries.\n\nWe recommend [#5]_ and [#6]_ as good references for the theory and\npracticalities of SVMs."
  },
  {
    "filename": "svm.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\svm.rst.txt",
    "id": "svm.rst.txt_chunk_14",
    "header": "SVC",
    "text": "SVC\n---\n\nGiven training vectors :math:`x_i \\in \\mathbb{R}^p`, i=1,..., n, in two classes, and a\nvector :math:`y \\in \\{1, -1\\}^n`, our goal is to find :math:`w \\in\n\\mathbb{R}^p` and :math:`b \\in \\mathbb{R}` such that the prediction given by\n:math:`\\text{sign} (w^T\\phi(x) + b)` is correct for most samples.\n\nSVC solves the following primal problem:\n\n.. math::\n\n    \\min_ {w, b, \\zeta} \\frac{1}{2} w^T w + C \\sum_{i=1}^{n} \\zeta_i\n\n    \\textrm {subject to } & y_i (w^T \\phi (x_i) + b) \\geq 1 - \\zeta_i,\\\\\n    & \\zeta_i \\geq 0, i=1, ..., n\n\nIntuitively, we're trying to maximize the margin (by minimizing\n:math:`||w||^2 = w^Tw`), while incurring a penalty when a sample is\nmisclassified or within the margin boundary. Ideally, the value :math:`y_i\n(w^T \\phi (x_i) + b)` would be :math:`\\geq 1` for all samples, which\nindicates a perfect prediction. But problems are usually not always perfectly\nseparable with a hyperplane, so we allow some samples to be at a distance :math:`\\zeta_i` from\ntheir correct margin boundary. The penalty term `C` controls the strength of\nthis penalty, and as a result, acts as an inverse regularization parameter\n(see note below).\n\nThe dual problem to the primal is\n\n.. math::\n\n   \\min_{\\alpha} \\frac{1}{2} \\alpha^T Q \\alpha - e^T \\alpha\n\n\n   \\textrm {subject to } & y^T \\alpha = 0\\\\\n   & 0 \\leq \\alpha_i \\leq C, i=1, ..., n\n\nwhere :math:`e` is the vector of all ones,\nand :math:`Q` is an :math:`n` by :math:`n` positive semidefinite matrix,\n:math:`Q_{ij} \\equiv y_i y_j K(x_i, x_j)`, where :math:`K(x_i, x_j) = \\phi (x_i)^T \\phi (x_j)`\nis the kernel. The terms :math:`\\alpha_i` are called the dual coefficients,\nand they are upper-bounded by :math:`C`.\nThis dual representation highlights the fact that training vectors are\nimplicitly mapped into a higher (maybe infinite)\ndimensional space by the function :math:`\\phi`: see `kernel trick\n<https://en.wikipedia.org/wiki/Kernel_method>`_.\n\nOnce the optimization problem is solved, the output of\n:term:`decision_function` for a given sample :math:`x` becomes:\n\n.. math:: \\sum_{i\\in SV} y_i \\alpha_i K(x_i, x) + b,\n\nand the predicted class corresponds to its sign. We only need to sum over the\nsupport vectors (i.e. the samples that lie within the margin) because the\ndual coefficients :math:`\\alpha_i` are zero for the other samples.\n\nThese parameters can be accessed through the attributes ``dual_coef_``\nwhich holds the product :math:`y_i \\alpha_i`, ``support_vectors_`` which\nholds the support vectors, and ``intercept_`` which holds the independent\nterm :math:`b`.\n\n.. note::\n\n    While SVM models derived from `libsvm`_ and `liblinear`_ use ``C`` as\n    regularization parameter, most other estimators use ``alpha``. The exact\n    equivalence between the amount of regularization of two models depends on\n    the exact objective function optimized by the model. For example, when the\n    estimator used is :class:`~sklearn.linear_model.Ridge` regression,\n    the relation between them is given as :math:`C = \\frac{1}{\\alpha}`.\n\n.. dropdown:: LinearSVC\n\n  The primal problem can be equivalently formulated as\n\n  .. math::\n\n      \\min_ {w, b} \\frac{1}{2} w^T w + C \\sum_{i=1}^{n}\\max(0, 1 - y_i (w^T \\phi(x_i) + b)),\n\n  where we make use of the `hinge loss\n  <https://en.wikipedia.org/wiki/Hinge_loss>`_. This is the form that is\n  directly optimized by :class:`LinearSVC`, but unlike the dual form, this one\n  does not involve inner products between samples, so the famous kernel trick\n  cannot be applied. This is why only the linear kernel is supported by\n  :class:`LinearSVC` (:math:`\\phi` is the identity function).\n\n.. _nu_svc:\n\n.. dropdown:: NuSVC\n\n  The :math:`\\nu`-SVC formulation [#7]_ is a reparameterization of the\n  :math:`C`-SVC and therefore mathematically equivalent.\n\n  We introduce a new parameter :math:`\\nu` (instead of :math:`C`) which\n  controls the number of support vectors and *margin errors*:\n  :math:`\\nu \\in (0, 1]` is an upper bound on the fraction of margin errors and\n  a lower bound of the fraction of support vectors. A margin error corresponds\n  to a sample that lies on the wrong side of its margin boundary: it is either\n  misclassified, or it is correctly classified but does not lie beyond the\n  margin."
  },
  {
    "filename": "svm.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\svm.rst.txt",
    "id": "svm.rst.txt_chunk_15",
    "header": "SVR",
    "text": "SVR\n---\n\nGiven training vectors :math:`x_i \\in \\mathbb{R}^p`, i=1,..., n, and a\nvector :math:`y \\in \\mathbb{R}^n` :math:`\\varepsilon`-SVR solves the following primal problem:\n\n\n.. math::\n\n    \\min_ {w, b, \\zeta, \\zeta^*} \\frac{1}{2} w^T w + C \\sum_{i=1}^{n} (\\zeta_i + \\zeta_i^*)\n\n\n\n    \\textrm {subject to } & y_i - w^T \\phi (x_i) - b \\leq \\varepsilon + \\zeta_i,\\\\\n                          & w^T \\phi (x_i) + b - y_i \\leq \\varepsilon + \\zeta_i^*,\\\\\n                          & \\zeta_i, \\zeta_i^* \\geq 0, i=1, ..., n\n\nHere, we are penalizing samples whose prediction is at least :math:`\\varepsilon`\naway from their true target. These samples penalize the objective by\n:math:`\\zeta_i` or :math:`\\zeta_i^*`, depending on whether their predictions\nlie above or below the :math:`\\varepsilon` tube.\n\nThe dual problem is\n\n.. math::\n\n   \\min_{\\alpha, \\alpha^*} \\frac{1}{2} (\\alpha - \\alpha^*)^T Q (\\alpha - \\alpha^*) + \\varepsilon e^T (\\alpha + \\alpha^*) - y^T (\\alpha - \\alpha^*)\n\n\n   \\textrm {subject to } & e^T (\\alpha - \\alpha^*) = 0\\\\\n   & 0 \\leq \\alpha_i, \\alpha_i^* \\leq C, i=1, ..., n\n\nwhere :math:`e` is the vector of all ones,\n:math:`Q` is an :math:`n` by :math:`n` positive semidefinite matrix,\n:math:`Q_{ij} \\equiv K(x_i, x_j) = \\phi (x_i)^T \\phi (x_j)`\nis the kernel. Here training vectors are implicitly mapped into a higher\n(maybe infinite) dimensional space by the function :math:`\\phi`.\n\nThe prediction is:\n\n.. math:: \\sum_{i \\in SV}(\\alpha_i - \\alpha_i^*) K(x_i, x) + b\n\nThese parameters can be accessed through the attributes ``dual_coef_``\nwhich holds the difference :math:`\\alpha_i - \\alpha_i^*`, ``support_vectors_`` which\nholds the support vectors, and ``intercept_`` which holds the independent\nterm :math:`b`\n\n.. dropdown:: LinearSVR\n\n  The primal problem can be equivalently formulated as\n\n  .. math::\n\n      \\min_ {w, b} \\frac{1}{2} w^T w + C \\sum_{i=1}^{n}\\max(0, |y_i - (w^T \\phi(x_i) + b)| - \\varepsilon),\n\n  where we make use of the epsilon-insensitive loss, i.e. errors of less than\n  :math:`\\varepsilon` are ignored. This is the form that is directly optimized\n  by :class:`LinearSVR`.\n\n.. _svm_implementation_details:"
  },
  {
    "filename": "svm.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\svm.rst.txt",
    "id": "svm.rst.txt_chunk_16",
    "header": "Implementation details",
    "text": "Implementation details\n======================\n\nInternally, we use `libsvm`_ [#4]_ and `liblinear`_ [#3]_ to handle all\ncomputations. These libraries are wrapped using C and Cython.\nFor a description of the implementation and details of the algorithms\nused, please refer to their respective papers.\n\n\n.. _`libsvm`: https://www.csie.ntu.edu.tw/~cjlin/libsvm/\n.. _`liblinear`: https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n\n.. rubric:: References\n\n.. [#1] Platt `\"Probabilistic outputs for SVMs and comparisons to\n  regularized likelihood methods\"\n  <https://www.cs.colorado.edu/~mozer/Teaching/syllabi/6622/papers/Platt1999.pdf>`_.\n\n.. [#2] Wu, Lin and Weng, `\"Probability estimates for multi-class\n  classification by pairwise coupling\"\n  <https://www.csie.ntu.edu.tw/~cjlin/papers/svmprob/svmprob.pdf>`_,\n  JMLR 5:975-1005, 2004.\n\n.. [#3] Fan, Rong-En, et al.,\n  `\"LIBLINEAR: A library for large linear classification.\"\n  <https://www.csie.ntu.edu.tw/~cjlin/papers/liblinear.pdf>`_,\n  Journal of machine learning research 9.Aug (2008): 1871-1874.\n\n.. [#4] Chang and Lin, `LIBSVM: A Library for Support Vector Machines\n  <https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_.\n\n.. [#5] Bishop, `Pattern recognition and machine learning\n  <https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf>`_,\n  chapter 7 Sparse Kernel Machines.\n\n.. [#6] :doi:`\"A Tutorial on Support Vector Regression\"\n  <10.1023/B:STCO.0000035301.49549.88>`\n  Alex J. Smola, Bernhard Sch\u00f6lkopf - Statistics and Computing archive\n  Volume 14 Issue 3, August 2004, p. 199-222.\n\n.. [#7] Sch\u00f6lkopf et. al `New Support Vector Algorithms\n  <https://www.stat.purdue.edu/~yuzhu/stat598m3/Papers/NewSVM.pdf>`_,\n  Neural Computation 12, 1207-1245 (2000).\n\n.. [#8] Crammer and Singer `On the Algorithmic Implementation of Multiclass\n  Kernel-based Vector Machines\n  <http://jmlr.csail.mit.edu/papers/volume2/crammer01a/crammer01a.pdf>`_, JMLR 2001."
  },
  {
    "filename": "tree.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\tree.rst.txt",
    "id": "tree.rst.txt_chunk_0",
    "header": ".. _tree:",
    "text": ".. _tree:\n\n=============="
  },
  {
    "filename": "tree.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\tree.rst.txt",
    "id": "tree.rst.txt_chunk_1",
    "header": "Decision Trees",
    "text": "Decision Trees\n==============\n\n.. currentmodule:: sklearn.tree\n\n**Decision Trees (DTs)** are a non-parametric supervised learning method used\nfor :ref:`classification <tree_classification>` and :ref:`regression\n<tree_regression>`. The goal is to create a model that predicts the value of a\ntarget variable by learning simple decision rules inferred from the data\nfeatures. A tree can be seen as a piecewise constant approximation.\n\nFor instance, in the example below, decision trees learn from data to\napproximate a sine curve with a set of if-then-else decision rules. The deeper\nthe tree, the more complex the decision rules and the fitter the model.\n\n.. figure:: ../auto_examples/tree/images/sphx_glr_plot_tree_regression_001.png\n   :target: ../auto_examples/tree/plot_tree_regression.html\n   :scale: 75\n   :align: center\n\nSome advantages of decision trees are:\n\n- Simple to understand and to interpret. Trees can be visualized.\n\n- Requires little data preparation. Other techniques often require data\n  normalization, dummy variables need to be created and blank values to\n  be removed. Some tree and algorithm combinations support\n  :ref:`missing values <tree_missing_value_support>`.\n\n- The cost of using the tree (i.e., predicting data) is logarithmic in the\n  number of data points used to train the tree.\n\n- Able to handle both numerical and categorical data. However, the scikit-learn\n  implementation does not support categorical variables for now. Other\n  techniques are usually specialized in analyzing datasets that have only one type\n  of variable. See :ref:`algorithms <tree_algorithms>` for more\n  information.\n\n- Able to handle multi-output problems.\n\n- Uses a white box model. If a given situation is observable in a model,\n  the explanation for the condition is easily explained by boolean logic.\n  By contrast, in a black box model (e.g., in an artificial neural\n  network), results may be more difficult to interpret.\n\n- Possible to validate a model using statistical tests. That makes it\n  possible to account for the reliability of the model.\n\n- Performs well even if its assumptions are somewhat violated by\n  the true model from which the data were generated.\n\n\nThe disadvantages of decision trees include:\n\n- Decision-tree learners can create over-complex trees that do not\n  generalize the data well. This is called overfitting. Mechanisms\n  such as pruning, setting the minimum number of samples required\n  at a leaf node or setting the maximum depth of the tree are\n  necessary to avoid this problem.\n\n- Decision trees can be unstable because small variations in the\n  data might result in a completely different tree being generated.\n  This problem is mitigated by using decision trees within an\n  ensemble.\n\n- Predictions of decision trees are neither smooth nor continuous, but\n  piecewise constant approximations as seen in the above figure. Therefore,\n  they are not good at extrapolation.\n\n- The problem of learning an optimal decision tree is known to be\n  NP-complete under several aspects of optimality and even for simple\n  concepts. Consequently, practical decision-tree learning algorithms\n  are based on heuristic algorithms such as the greedy algorithm where\n  locally optimal decisions are made at each node. Such algorithms\n  cannot guarantee to return the globally optimal decision tree.  This\n  can be mitigated by training multiple trees in an ensemble learner,\n  where the features and samples are randomly sampled with replacement.\n\n- There are concepts that are hard to learn because decision trees\n  do not express them easily, such as XOR, parity or multiplexer problems.\n\n- Decision tree learners create biased trees if some classes dominate.\n  It is therefore recommended to balance the dataset prior to fitting\n  with the decision tree.\n\n\n.. _tree_classification:"
  },
  {
    "filename": "tree.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\tree.rst.txt",
    "id": "tree.rst.txt_chunk_2",
    "header": "Classification",
    "text": "Classification\n==============\n\n:class:`DecisionTreeClassifier` is a class capable of performing multi-class\nclassification on a dataset.\n\nAs with other classifiers, :class:`DecisionTreeClassifier` takes as input two arrays:\nan array X, sparse or dense, of shape ``(n_samples, n_features)`` holding the\ntraining samples, and an array Y of integer values, shape ``(n_samples,)``,\nholding the class labels for the training samples::\n\n    >>> from sklearn import tree\n    >>> X = [[0, 0], [1, 1]]\n    >>> Y = [0, 1]\n    >>> clf = tree.DecisionTreeClassifier()\n    >>> clf = clf.fit(X, Y)\n\nAfter being fitted, the model can then be used to predict the class of samples::\n\n    >>> clf.predict([[2., 2.]])\n    array([1])\n\nIn case that there are multiple classes with the same and highest\nprobability, the classifier will predict the class with the lowest index\namongst those classes.\n\nAs an alternative to outputting a specific class, the probability of each class\ncan be predicted, which is the fraction of training samples of the class in a\nleaf::\n\n    >>> clf.predict_proba([[2., 2.]])\n    array([[0., 1.]])\n\n:class:`DecisionTreeClassifier` is capable of both binary (where the\nlabels are [-1, 1]) classification and multiclass (where the labels are\n[0, ..., K-1]) classification.\n\nUsing the Iris dataset, we can construct a tree as follows::\n\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn import tree\n    >>> iris = load_iris()\n    >>> X, y = iris.data, iris.target\n    >>> clf = tree.DecisionTreeClassifier()\n    >>> clf = clf.fit(X, y)\n\nOnce trained, you can plot the tree with the :func:`plot_tree` function::\n\n\n    >>> tree.plot_tree(clf)\n    [...]\n\n.. figure:: ../auto_examples/tree/images/sphx_glr_plot_iris_dtc_002.png\n   :target: ../auto_examples/tree/plot_iris_dtc.html\n   :scale: 75\n   :align: center\n\n.. dropdown:: Alternative ways to export trees\n\n  We can also export the tree in `Graphviz\n  <https://www.graphviz.org/>`_ format using the :func:`export_graphviz`\n  exporter. If you use the `conda <https://conda.io>`_ package manager, the graphviz binaries\n  and the python package can be installed with `conda install python-graphviz`.\n\n  Alternatively binaries for graphviz can be downloaded from the graphviz project homepage,\n  and the Python wrapper installed from pypi with `pip install graphviz`.\n\n  Below is an example graphviz export of the above tree trained on the entire\n  iris dataset; the results are saved in an output file `iris.pdf`::\n\n\n      >>> import graphviz # doctest: +SKIP\n      >>> dot_data = tree.export_graphviz(clf, out_file=None) # doctest: +SKIP\n      >>> graph = graphviz.Source(dot_data) # doctest: +SKIP\n      >>> graph.render(\"iris\") # doctest: +SKIP\n\n  The :func:`export_graphviz` exporter also supports a variety of aesthetic\n  options, including coloring nodes by their class (or value for regression) and\n  using explicit variable and class names if desired. Jupyter notebooks also\n  render these plots inline automatically::\n\n      >>> dot_data = tree.export_graphviz(clf, out_file=None, # doctest: +SKIP\n      ...                      feature_names=iris.feature_names,  # doctest: +SKIP\n      ...                      class_names=iris.target_names,  # doctest: +SKIP\n      ...                      filled=True, rounded=True,  # doctest: +SKIP\n      ...                      special_characters=True)  # doctest: +SKIP\n      >>> graph = graphviz.Source(dot_data)  # doctest: +SKIP\n      >>> graph # doctest: +SKIP\n\n  .. only:: html\n\n      .. figure:: ../images/iris.svg\n        :align: center\n\n  .. only:: latex\n\n      .. figure:: ../images/iris.pdf\n        :align: center\n\n  .. figure:: ../auto_examples/tree/images/sphx_glr_plot_iris_dtc_001.png\n    :target: ../auto_examples/tree/plot_iris_dtc.html\n    :align: center\n    :scale: 75\n\n  Alternatively, the tree can also be exported in textual format with the\n  function :func:`export_text`. This method doesn't require the installation\n  of external libraries and is more compact:\n\n      >>> from sklearn.datasets import load_iris\n      >>> from sklearn.tree import DecisionTreeClassifier\n      >>> from sklearn.tree import export_text\n      >>> iris = load_iris()\n      >>> decision_tree = DecisionTreeClassifier(random_state=0, max_depth=2)\n      >>> decision_tree = decision_tree.fit(iris.data, iris.target)\n      >>> r = export_text(decision_tree, feature_names=iris['feature_names'])\n      >>> print(r)\n      |--- petal width (cm) <= 0.80\n      |   |--- class: 0\n      |--- petal width (cm) >  0.80\n      |   |--- petal width (cm) <= 1.75\n      |   |   |--- class: 1\n      |   |--- petal width (cm) >  1.75\n      |   |   |--- class: 2\n      <BLANKLINE>\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_tree_plot_iris_dtc.py`\n* :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n\n.. _tree_regression:"
  },
  {
    "filename": "tree.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\tree.rst.txt",
    "id": "tree.rst.txt_chunk_3",
    "header": "Regression",
    "text": "Regression\n==========\n\n.. figure:: ../auto_examples/tree/images/sphx_glr_plot_tree_regression_001.png\n   :target: ../auto_examples/tree/plot_tree_regression.html\n   :scale: 75\n   :align: center\n\nDecision trees can also be applied to regression problems, using the\n:class:`DecisionTreeRegressor` class.\n\nAs in the classification setting, the fit method will take as argument arrays X\nand y, only that in this case y is expected to have floating point values\ninstead of integer values::\n\n    >>> from sklearn import tree\n    >>> X = [[0, 0], [2, 2]]\n    >>> y = [0.5, 2.5]\n    >>> clf = tree.DecisionTreeRegressor()\n    >>> clf = clf.fit(X, y)\n    >>> clf.predict([[1, 1]])\n    array([0.5])\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_tree_plot_tree_regression.py`\n\n\n.. _tree_multioutput:"
  },
  {
    "filename": "tree.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\tree.rst.txt",
    "id": "tree.rst.txt_chunk_4",
    "header": "Multi-output problems",
    "text": "Multi-output problems\n=====================\n\nA multi-output problem is a supervised learning problem with several outputs\nto predict, that is when Y is a 2d array of shape ``(n_samples, n_outputs)``.\n\nWhen there is no correlation between the outputs, a very simple way to solve\nthis kind of problem is to build n independent models, i.e. one for each\noutput, and then to use those models to independently predict each one of the n\noutputs. However, because it is likely that the output values related to the\nsame input are themselves correlated, an often better way is to build a single\nmodel capable of predicting simultaneously all n outputs. First, it requires\nlower training time since only a single estimator is built. Second, the\ngeneralization accuracy of the resulting estimator may often be increased.\n\nWith regard to decision trees, this strategy can readily be used to support\nmulti-output problems. This requires the following changes:\n\n- Store n output values in leaves, instead of 1;\n- Use splitting criteria that compute the average reduction across all\n  n outputs.\n\nThis module offers support for multi-output problems by implementing this\nstrategy in both :class:`DecisionTreeClassifier` and\n:class:`DecisionTreeRegressor`. If a decision tree is fit on an output array Y\nof shape ``(n_samples, n_outputs)`` then the resulting estimator will:\n\n* Output n_output values upon ``predict``;\n\n* Output a list of n_output arrays of class probabilities upon\n  ``predict_proba``.\n\nThe use of multi-output trees for regression is demonstrated in\n:ref:`sphx_glr_auto_examples_tree_plot_tree_regression.py`. In this example, the input\nX is a single real value and the outputs Y are the sine and cosine of X.\n\n.. figure:: ../auto_examples/tree/images/sphx_glr_plot_tree_regression_002.png\n   :target: ../auto_examples/tree/plot_tree_regression.html\n   :scale: 75\n   :align: center\n\nThe use of multi-output trees for classification is demonstrated in\n:ref:`sphx_glr_auto_examples_miscellaneous_plot_multioutput_face_completion.py`. In this example, the inputs\nX are the pixels of the upper half of faces and the outputs Y are the pixels of\nthe lower half of those faces.\n\n.. figure:: ../auto_examples/miscellaneous/images/sphx_glr_plot_multioutput_face_completion_001.png\n   :target: ../auto_examples/miscellaneous/plot_multioutput_face_completion.html\n   :scale: 75\n   :align: center\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_miscellaneous_plot_multioutput_face_completion.py`\n\n.. rubric:: References\n\n* M. Dumont et al,  `Fast multi-class image annotation with random subwindows\n  and multiple output randomized trees\n  <http://www.montefiore.ulg.ac.be/services/stochastic/pubs/2009/DMWG09/dumont-visapp09-shortpaper.pdf>`_,\n  International Conference on Computer Vision Theory and Applications 2009\n\n.. _tree_complexity:"
  },
  {
    "filename": "tree.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\tree.rst.txt",
    "id": "tree.rst.txt_chunk_5",
    "header": "Complexity",
    "text": "Complexity\n==========\n\nIn general, the run time cost to construct a balanced binary tree is\n:math:`O(n_{samples}n_{features}\\log(n_{samples}))` and query time\n:math:`O(\\log(n_{samples}))`.  Although the tree construction algorithm attempts\nto generate balanced trees, they will not always be balanced.  Assuming that the\nsubtrees remain approximately balanced, the cost at each node consists of\nsearching through :math:`O(n_{features})` to find the feature that offers the\nlargest reduction in the impurity criterion, e.g. log loss (which is equivalent to an\ninformation gain). This has a cost of\n:math:`O(n_{features}n_{samples}\\log(n_{samples}))` at each node, leading to a\ntotal cost over the entire trees (by summing the cost at each node) of\n:math:`O(n_{features}n_{samples}^{2}\\log(n_{samples}))`."
  },
  {
    "filename": "tree.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\tree.rst.txt",
    "id": "tree.rst.txt_chunk_6",
    "header": "Tips on practical use",
    "text": "Tips on practical use\n=====================\n\n* Decision trees tend to overfit on data with a large number of features.\n  Getting the right ratio of samples to number of features is important, since\n  a tree with few samples in high dimensional space is very likely to overfit.\n\n* Consider performing  dimensionality reduction (:ref:`PCA <PCA>`,\n  :ref:`ICA <ICA>`, or :ref:`feature_selection`) beforehand to\n  give your tree a better chance of finding features that are discriminative.\n\n* :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py` will help\n  in gaining more insights about how the decision tree makes predictions, which is\n  important for understanding the important features in the data.\n\n* Visualize your tree as you are training by using the ``export``\n  function.  Use ``max_depth=3`` as an initial tree depth to get a feel for\n  how the tree is fitting to your data, and then increase the depth.\n\n* Remember that the number of samples required to populate the tree doubles\n  for each additional level the tree grows to.  Use ``max_depth`` to control\n  the size of the tree to prevent overfitting.\n\n* Use ``min_samples_split`` or ``min_samples_leaf`` to ensure that multiple\n  samples inform every decision in the tree, by controlling which splits will\n  be considered. A very small number will usually mean the tree will overfit,\n  whereas a large number will prevent the tree from learning the data. Try\n  ``min_samples_leaf=5`` as an initial value. If the sample size varies\n  greatly, a float number can be used as percentage in these two parameters.\n  While ``min_samples_split`` can create arbitrarily small leaves,\n  ``min_samples_leaf`` guarantees that each leaf has a minimum size, avoiding\n  low-variance, over-fit leaf nodes in regression problems.  For\n  classification with few classes, ``min_samples_leaf=1`` is often the best\n  choice.\n\n  Note that ``min_samples_split`` considers samples directly and independent of\n  ``sample_weight``, if provided (e.g. a node with m weighted samples is still\n  treated as having exactly m samples). Consider ``min_weight_fraction_leaf`` or\n  ``min_impurity_decrease`` if accounting for sample weights is required at splits.\n\n* Balance your dataset before training to prevent the tree from being biased\n  toward the classes that are dominant. Class balancing can be done by\n  sampling an equal number of samples from each class, or preferably by\n  normalizing the sum of the sample weights (``sample_weight``) for each\n  class to the same value. Also note that weight-based pre-pruning criteria,\n  such as ``min_weight_fraction_leaf``, will then be less biased toward\n  dominant classes than criteria that are not aware of the sample weights,\n  like ``min_samples_leaf``.\n\n* If the samples are weighted, it will be easier to optimize the tree\n  structure using weight-based pre-pruning criterion such as\n  ``min_weight_fraction_leaf``, which ensures that leaf nodes contain at least\n  a fraction of the overall sum of the sample weights.\n\n* All decision trees use ``np.float32`` arrays internally.\n  If training data is not in this format, a copy of the dataset will be made.\n\n* If the input matrix X is very sparse, it is recommended to convert to sparse\n  ``csc_matrix`` before calling fit and sparse ``csr_matrix`` before calling\n  predict. Training time can be orders of magnitude faster for a sparse\n  matrix input compared to a dense matrix when features have zero values in\n  most of the samples.\n\n\n.. _tree_algorithms:"
  },
  {
    "filename": "tree.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\tree.rst.txt",
    "id": "tree.rst.txt_chunk_7",
    "header": "Tree algorithms: ID3, C4.5, C5.0 and CART",
    "text": "Tree algorithms: ID3, C4.5, C5.0 and CART\n==========================================\n\nWhat are all the various decision tree algorithms and how do they differ\nfrom each other? Which one is implemented in scikit-learn?\n\n.. dropdown:: Various decision tree algorithms\n\n  ID3_ (Iterative Dichotomiser 3) was developed in 1986 by Ross Quinlan.\n  The algorithm creates a multiway tree, finding for each node (i.e. in\n  a greedy manner) the categorical feature that will yield the largest\n  information gain for categorical targets. Trees are grown to their\n  maximum size and then a pruning step is usually applied to improve the\n  ability of the tree to generalize to unseen data.\n\n  C4.5 is the successor to ID3 and removed the restriction that features\n  must be categorical by dynamically defining a discrete attribute (based\n  on numerical variables) that partitions the continuous attribute value\n  into a discrete set of intervals. C4.5 converts the trained trees\n  (i.e. the output of the ID3 algorithm) into sets of if-then rules.\n  The accuracy of each rule is then evaluated to determine the order\n  in which they should be applied. Pruning is done by removing a rule's\n  precondition if the accuracy of the rule improves without it.\n\n  C5.0 is Quinlan's latest version release under a proprietary license.\n  It uses less memory and builds smaller rulesets than C4.5 while being\n  more accurate.\n\n  CART (Classification and Regression Trees) is very similar to C4.5, but\n  it differs in that it supports numerical target variables (regression) and\n  does not compute rule sets. CART constructs binary trees using the feature\n  and threshold that yield the largest information gain at each node.\n\nscikit-learn uses an optimized version of the CART algorithm; however, the\nscikit-learn implementation does not support categorical variables for now.\n\n.. _ID3: https://en.wikipedia.org/wiki/ID3_algorithm\n\n\n.. _tree_mathematical_formulation:"
  },
  {
    "filename": "tree.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\tree.rst.txt",
    "id": "tree.rst.txt_chunk_8",
    "header": "Mathematical formulation",
    "text": "Mathematical formulation\n========================\n\nGiven training vectors :math:`x_i \\in R^n`, i=1,..., l and a label vector\n:math:`y \\in R^l`, a decision tree recursively partitions the feature space\nsuch that the samples with the same labels or similar target values are grouped\ntogether.\n\nLet the data at node :math:`m` be represented by :math:`Q_m` with :math:`n_m`\nsamples. For each candidate split :math:`\\theta = (j, t_m)` consisting of a\nfeature :math:`j` and threshold :math:`t_m`, partition the data into\n:math:`Q_m^{left}(\\theta)` and :math:`Q_m^{right}(\\theta)` subsets\n\n.. math::\n\n    Q_m^{left}(\\theta) = \\{(x, y) | x_j \\leq t_m\\}\n\n    Q_m^{right}(\\theta) = Q_m \\setminus Q_m^{left}(\\theta)\n\nThe quality of a candidate split of node :math:`m` is then computed using an\nimpurity function or loss function :math:`H()`, the choice of which depends on\nthe task being solved (classification or regression)\n\n.. math::\n\n   G(Q_m, \\theta) = \\frac{n_m^{left}}{n_m} H(Q_m^{left}(\\theta))\n   + \\frac{n_m^{right}}{n_m} H(Q_m^{right}(\\theta))\n\nSelect the parameters that minimises the impurity\n\n.. math::\n\n    \\theta^* = \\operatorname{argmin}_\\theta  G(Q_m, \\theta)\n\nRecurse for subsets :math:`Q_m^{left}(\\theta^*)` and\n:math:`Q_m^{right}(\\theta^*)` until the maximum allowable depth is reached,\n:math:`n_m < \\min_{samples}` or :math:`n_m = 1`."
  },
  {
    "filename": "tree.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\tree.rst.txt",
    "id": "tree.rst.txt_chunk_9",
    "header": "Classification criteria",
    "text": "Classification criteria\n-----------------------\n\nIf a target is a classification outcome taking on values 0,1,...,K-1,\nfor node :math:`m`, let\n\n.. math::\n\n    p_{mk} = \\frac{1}{n_m} \\sum_{y \\in Q_m} I(y = k)\n\nbe the proportion of class k observations in node :math:`m`. If :math:`m` is a\nterminal node, `predict_proba` for this region is set to :math:`p_{mk}`.\nCommon measures of impurity are the following.\n\nGini:\n\n.. math::\n\n    H(Q_m) = \\sum_k p_{mk} (1 - p_{mk})\n\nLog Loss or Entropy:\n\n.. math::\n\n    H(Q_m) = - \\sum_k p_{mk} \\log(p_{mk})\n\n.. dropdown:: Shannon entropy\n\n  The entropy criterion computes the Shannon entropy of the possible classes. It\n  takes the class frequencies of the training data points that reached a given\n  leaf :math:`m` as their probability. Using the **Shannon entropy as tree node\n  splitting criterion is equivalent to minimizing the log loss** (also known as\n  cross-entropy and multinomial deviance) between the true labels :math:`y_i`\n  and the probabilistic predictions :math:`T_k(x_i)` of the tree model :math:`T` for class :math:`k`.\n\n  To see this, first recall that the log loss of a tree model :math:`T`\n  computed on a dataset :math:`D` is defined as follows:\n\n  .. math::\n\n      \\mathrm{LL}(D, T) = -\\frac{1}{n} \\sum_{(x_i, y_i) \\in D} \\sum_k I(y_i = k) \\log(T_k(x_i))\n\n  where :math:`D` is a training dataset of :math:`n` pairs :math:`(x_i, y_i)`.\n\n  In a classification tree, the predicted class probabilities within leaf nodes\n  are constant, that is: for all :math:`(x_i, y_i) \\in Q_m`, one has:\n  :math:`T_k(x_i) = p_{mk}` for each class :math:`k`.\n\n  This property makes it possible to rewrite :math:`\\mathrm{LL}(D, T)` as the\n  sum of the Shannon entropies computed for each leaf of :math:`T` weighted by\n  the number of training data points that reached each leaf:\n\n  .. math::\n\n      \\mathrm{LL}(D, T) = \\sum_{m \\in T} \\frac{n_m}{n} H(Q_m)"
  },
  {
    "filename": "tree.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\tree.rst.txt",
    "id": "tree.rst.txt_chunk_10",
    "header": "Regression criteria",
    "text": "Regression criteria\n-------------------\n\nIf the target is a continuous value, then for node :math:`m`, common\ncriteria to minimize as for determining locations for future splits are Mean\nSquared Error (MSE or L2 error), Poisson deviance as well as Mean Absolute\nError (MAE or L1 error). MSE and Poisson deviance both set the predicted value\nof terminal nodes to the learned mean value :math:`\\bar{y}_m` of the node\nwhereas the MAE sets the predicted value of terminal nodes to the median\n:math:`median(y)_m`.\n\nMean Squared Error:\n\n.. math::\n\n    \\bar{y}_m = \\frac{1}{n_m} \\sum_{y \\in Q_m} y\n\n    H(Q_m) = \\frac{1}{n_m} \\sum_{y \\in Q_m} (y - \\bar{y}_m)^2\n\nMean Poisson deviance:\n\n.. math::\n\n    H(Q_m) = \\frac{2}{n_m} \\sum_{y \\in Q_m} (y \\log\\frac{y}{\\bar{y}_m}\n    - y + \\bar{y}_m)\n\nSetting `criterion=\"poisson\"` might be a good choice if your target is a count\nor a frequency (count per some unit). In any case, :math:`y >= 0` is a\nnecessary condition to use this criterion. Note that it fits much slower than\nthe MSE criterion. For performance reasons the actual implementation minimizes\nthe half mean poisson deviance, i.e. the mean poisson deviance divided by 2.\n\nMean Absolute Error:\n\n.. math::\n\n    median(y)_m = \\underset{y \\in Q_m}{\\mathrm{median}}(y)\n\n    H(Q_m) = \\frac{1}{n_m} \\sum_{y \\in Q_m} |y - median(y)_m|\n\nNote that it fits much slower than the MSE criterion.\n\n.. _tree_missing_value_support:"
  },
  {
    "filename": "tree.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\tree.rst.txt",
    "id": "tree.rst.txt_chunk_11",
    "header": "Missing Values Support",
    "text": "Missing Values Support\n======================\n\n:class:`DecisionTreeClassifier`, :class:`DecisionTreeRegressor`\nhave built-in support for missing values using `splitter='best'`, where\nthe splits are determined in a greedy fashion.\n:class:`ExtraTreeClassifier`, and :class:`ExtraTreeRegressor` have built-in\nsupport for missing values for `splitter='random'`, where the splits\nare determined randomly. For more details on how the splitter differs on\nnon-missing values, see the :ref:`Forest section <forest>`.\n\nThe criterion supported when there are missing values are\n`'gini'`, `'entropy'`, or `'log_loss'`, for classification or\n`'squared_error'`, `'friedman_mse'`, or `'poisson'` for regression.\n\nFirst we will describe how :class:`DecisionTreeClassifier`, :class:`DecisionTreeRegressor`\nhandle missing-values in the data.\n\nFor each potential threshold on the non-missing data, the splitter will evaluate\nthe split with all the missing values going to the left node or the right node.\n\nDecisions are made as follows:\n\n- By default when predicting, the samples with missing values are classified\n  with the class used in the split found during training::\n\n    >>> from sklearn.tree import DecisionTreeClassifier\n    >>> import numpy as np\n\n    >>> X = np.array([0, 1, 6, np.nan]).reshape(-1, 1)\n    >>> y = [0, 0, 1, 1]\n\n    >>> tree = DecisionTreeClassifier(random_state=0).fit(X, y)\n    >>> tree.predict(X)\n    array([0, 0, 1, 1])\n\n- If the criterion evaluation is the same for both nodes,\n  then the tie for missing value at predict time is broken by going to the\n  right node. The splitter also checks the split where all the missing\n  values go to one child and non-missing values go to the other::\n\n    >>> from sklearn.tree import DecisionTreeClassifier\n    >>> import numpy as np\n\n    >>> X = np.array([np.nan, -1, np.nan, 1]).reshape(-1, 1)\n    >>> y = [0, 0, 1, 1]\n\n    >>> tree = DecisionTreeClassifier(random_state=0).fit(X, y)\n\n    >>> X_test = np.array([np.nan]).reshape(-1, 1)\n    >>> tree.predict(X_test)\n    array([1])\n\n- If no missing values are seen during training for a given feature, then during\n  prediction missing values are mapped to the child with the most samples::\n\n    >>> from sklearn.tree import DecisionTreeClassifier\n    >>> import numpy as np\n\n    >>> X = np.array([0, 1, 2, 3]).reshape(-1, 1)\n    >>> y = [0, 1, 1, 1]\n\n    >>> tree = DecisionTreeClassifier(random_state=0).fit(X, y)\n\n    >>> X_test = np.array([np.nan]).reshape(-1, 1)\n    >>> tree.predict(X_test)\n    array([1])\n\n:class:`ExtraTreeClassifier`, and :class:`ExtraTreeRegressor` handle missing values\nin a slightly different way. When splitting a node, a random threshold will be chosen\nto split the non-missing values on. Then the non-missing values will be sent to the\nleft and right child based on the randomly selected threshold, while the missing\nvalues will also be randomly sent to the left or right child. This is repeated for\nevery feature considered at each split. The best split among these is chosen.\n\nDuring prediction, the treatment of missing-values is the same as that of the\ndecision tree:\n\n- By default when predicting, the samples with missing values are classified\n  with the class used in the split found during training.\n\n- If no missing values are seen during training for a given feature, then during\n  prediction missing values are mapped to the child with the most samples.\n\n.. _minimal_cost_complexity_pruning:"
  },
  {
    "filename": "tree.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\tree.rst.txt",
    "id": "tree.rst.txt_chunk_12",
    "header": "Minimal Cost-Complexity Pruning",
    "text": "Minimal Cost-Complexity Pruning\n===============================\n\nMinimal cost-complexity pruning is an algorithm used to prune a tree to avoid\nover-fitting, described in Chapter 3 of [BRE]_. This algorithm is parameterized\nby :math:`\\alpha\\ge0` known as the complexity parameter. The complexity\nparameter is used to define the cost-complexity measure, :math:`R_\\alpha(T)` of\na given tree :math:`T`:\n\n.. math::\n\n  R_\\alpha(T) = R(T) + \\alpha|\\widetilde{T}|\n\nwhere :math:`|\\widetilde{T}|` is the number of terminal nodes in :math:`T` and :math:`R(T)`\nis traditionally defined as the total misclassification rate of the terminal\nnodes. Alternatively, scikit-learn uses the total sample weighted impurity of\nthe terminal nodes for :math:`R(T)`. As shown above, the impurity of a node\ndepends on the criterion. Minimal cost-complexity pruning finds the subtree of\n:math:`T` that minimizes :math:`R_\\alpha(T)`.\n\nThe cost complexity measure of a single node is\n:math:`R_\\alpha(t)=R(t)+\\alpha`. The branch, :math:`T_t`, is defined to be a\ntree where node :math:`t` is its root. In general, the impurity of a node\nis greater than the sum of impurities of its terminal nodes,\n:math:`R(T_t)<R(t)`. However, the cost complexity measure of a node,\n:math:`t`, and its branch, :math:`T_t`, can be equal depending on\n:math:`\\alpha`. We define the effective :math:`\\alpha` of a node to be the\nvalue where they are equal, :math:`R_\\alpha(T_t)=R_\\alpha(t)` or\n:math:`\\alpha_{eff}(t)=\\frac{R(t)-R(T_t)}{|T|-1}`. A non-terminal node\nwith the smallest value of :math:`\\alpha_{eff}` is the weakest link and will\nbe pruned. This process stops when the pruned tree's minimal\n:math:`\\alpha_{eff}` is greater than the ``ccp_alpha`` parameter.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_tree_plot_cost_complexity_pruning.py`\n\n.. rubric:: References\n\n.. [BRE] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification\n  and Regression Trees. Wadsworth, Belmont, CA, 1984.\n\n* https://en.wikipedia.org/wiki/Decision_tree_learning\n\n* https://en.wikipedia.org/wiki/Predictive_analytics\n\n* J.R. Quinlan. C4. 5: programs for machine learning. Morgan\n  Kaufmann, 1993.\n\n* T. Hastie, R. Tibshirani and J. Friedman. Elements of Statistical\n  Learning, Springer, 2009."
  },
  {
    "filename": "unsupervised_reduction.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\unsupervised_reduction.rst.txt",
    "id": "unsupervised_reduction.rst.txt_chunk_0",
    "header": "",
    "text": ".. _data_reduction:\n\n====================================="
  },
  {
    "filename": "unsupervised_reduction.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\unsupervised_reduction.rst.txt",
    "id": "unsupervised_reduction.rst.txt_chunk_1",
    "header": "Unsupervised dimensionality reduction",
    "text": "Unsupervised dimensionality reduction\n=====================================\n\nIf your number of features is high, it may be useful to reduce it with an\nunsupervised step prior to supervised steps. Many of the\n:ref:`unsupervised-learning` methods implement a ``transform`` method that\ncan be used to reduce the dimensionality. Below we discuss two specific\nexamples of this pattern that are heavily used.\n\n.. topic:: **Pipelining**\n\n    The unsupervised data reduction and the supervised estimator can be\n    chained in one step. See :ref:`pipeline`.\n\n.. currentmodule:: sklearn"
  },
  {
    "filename": "unsupervised_reduction.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\unsupervised_reduction.rst.txt",
    "id": "unsupervised_reduction.rst.txt_chunk_2",
    "header": "PCA: principal component analysis",
    "text": "PCA: principal component analysis\n----------------------------------\n\n:class:`decomposition.PCA` looks for a combination of features that\ncapture well the variance of the original features. See :ref:`decompositions`.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_applications_plot_face_recognition.py`"
  },
  {
    "filename": "unsupervised_reduction.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\unsupervised_reduction.rst.txt",
    "id": "unsupervised_reduction.rst.txt_chunk_3",
    "header": "Random projections",
    "text": "Random projections\n-------------------\n\nThe module: :mod:`~sklearn.random_projection` provides several tools for data\nreduction by random projections. See the relevant section of the\ndocumentation: :ref:`random_projection`.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_miscellaneous_plot_johnson_lindenstrauss_bound.py`"
  },
  {
    "filename": "unsupervised_reduction.rst.txt",
    "source": "E:\\School\\CS_7050_Data_WH_Mining\\code\\rag_project\\scikit-learn-docs\\_sources\\modules\\unsupervised_reduction.rst.txt",
    "id": "unsupervised_reduction.rst.txt_chunk_4",
    "header": "Feature agglomeration",
    "text": "Feature agglomeration\n------------------------\n\n:class:`cluster.FeatureAgglomeration` applies\n:ref:`hierarchical_clustering` to group together features that behave\nsimilarly.\n\n.. rubric:: Examples\n\n* :ref:`sphx_glr_auto_examples_cluster_plot_feature_agglomeration_vs_univariate_selection.py`\n* :ref:`sphx_glr_auto_examples_cluster_plot_digits_agglomeration.py`\n\n.. topic:: **Feature scaling**\n\n   Note that if features have very different scaling or statistical\n   properties, :class:`cluster.FeatureAgglomeration` may not be able to\n   capture the links between related features. Using a\n   :class:`preprocessing.StandardScaler` can be useful in these settings."
  }
]